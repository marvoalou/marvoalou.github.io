<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>BadPart</title>
      <link href="/posts/58495.html"/>
      <url>/posts/58495.html</url>
      
        <content type="html"><![CDATA[<h2 id="意义和提出"><a href="#意义和提出" class="headerlink" title="意义和提出"></a>意义和提出</h2><ul><li><strong>第一个</strong>对于大分辨率图像的统一黑盒对抗性补丁，即黑盒对于逐像素回归模型生成对抗性补丁</li><li>使用了方形区域划分优化(Square-based adversarial patch optimization)的随机噪声生成，然后进行概率采样和梯度分数优化生成噪声</li></ul><hr><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="期望效果"><a href="#期望效果" class="headerlink" title="期望效果"></a>期望效果</h3><blockquote><p>显著降低黑盒逐像素模型(例如提供API的在线服务)性能</p></blockquote><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>总公式：</p><script type="math/tex; mode=display">\max_p \space Mean(\mathcal{F}(\mathcal{M}([\mathbf{x}']_n) - \mathcal{M}([\mathbf{x}_0']_n)))</script><p>其中：</p><ul><li>h为高，w为宽</li><li>$\mathbf{p}$ 是大小为3,h,h的噪声</li><li>$p_0$是纯黑噪声(基准补丁)</li><li>$[\mathbf{x}’_n] = \Lambda([\mathbf{x}’_n],p,q)$是在q处将p附加给图片x</li><li>$\mathcal{F}()$是计算两个模型修改结果的逐像素误差，并使其最大化</li></ul><p>输出：n,d,H,W的图片，The output of model M has a dimension of n × d × H × W , where d refers to the output channels for each image. For MDE models d equals 1 as the output is the estimated distance for each pixel, and for OFE models d equals 2 since the model outputs the estimated pixel-wise offset vector (two dimensions)</p><h3 id="Probabilistic-Square-Sampling"><a href="#Probabilistic-Square-Sampling" class="headerlink" title="Probabilistic Square Sampling"></a>Probabilistic Square Sampling</h3><p><img src="/posts/58495/Alg2.png" alt><br>The sampling algorithm is designed to enhance the probability of selecting locations within the patch region that are more vulnerable to adversarial perturbations.</p><p>选取由最新的补丁图片计算出来的逐像素误差图M，来展现出来的误差最大的区域，产生每个区域的采样的概率，所以称为概率采样</p><p>需要有迭代输入的原因是采样的尺寸是由关于迭代次数的预定义计划中获得的<code>SizeSche(iter)</code></p><p>采样过程：</p><ul><li>根据预定义计划来获取采样的大小，包含由粗到细的颗粒度，以及均匀采样阶段</li><li>平滑逐像素误差图M</li><li>softmax获取概率</li></ul><h3 id="Score-based-Gradient-Estimation"><a href="#Score-based-Gradient-Estimation" class="headerlink" title="Score-based Gradient Estimation"></a>Score-based Gradient Estimation</h3><p><img src="/posts/58495/Alg3.png" alt><br>该方法通过在局部区域生成并评估随机噪声，快速估计对抗性梯度，无需像传统零阶优化那样逐像素估计。这种方法有效提升了梯度估计的效率，同时能保持优化过程的稳定性。</p><ul><li><p><strong>传统方法（如零阶优化）</strong>：</p><ul><li>对每个像素分别生成扰动，计算其对损失函数的影响，逐像素估计梯度。</li><li><strong>缺点</strong>：计算量巨大，尤其在高分辨率图像上，优化效率较低。</li></ul></li><li><p>**本方法：</p><ul><li><strong>以方形区域为单位</strong>：在局部区域内生成随机噪声，整体评估其对攻击效果的影响，从而估计区域梯度。</li><li><strong>提高效率</strong>：通过利用区域噪声的整体性，避免逐像素计算，显著降低了计算复杂度。</li></ul></li></ul><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a><strong>流程</strong></h3><ol><li><p><strong>局部区域确定</strong>：</p><ul><li>确定一个正方形区域，作为梯度估计的范围。</li><li>区域位置由中心点 $(z_c, z_r)$  和边长 e 决定，确保梯度估计集中于攻击关键区域。</li></ul></li><li><p><strong>随机生成噪声</strong> $[\delta]_b$ ：</p><ul><li>在正方形区域内生成 b 组随机噪声 $\delta$，每组噪声的值限制在 ${-\varepsilon, \varepsilon}$。</li><li>每个像素的值为 $-\varepsilon $或$\varepsilon$ ，原因如下：<ul><li><strong>稀疏性假设</strong>：文献（Moon et al., 2019）指出，最优对抗噪声常位于约束空间的顶点，即取值为极端值（$\pm\varepsilon$）。</li><li>生成的 b 组噪声有助于在约束范围内探索攻击效果。</li></ul></li></ul></li><li><p><strong>计算加噪图片的评分</strong> $[\Delta]_b$：</p><ul><li>将每组噪声加到方形区域后，生成对应的加噪图片集 $[\hat{x}]_b$ 。</li><li>计算每张图片的评分（噪声对攻击效果的影响）：<script type="math/tex">\Delta_i = \text{Mean}(F_e(\hat{x}_i)) - \text{Mean}(F_e(x'))</script><ul><li>$\hat{x}_i$ ：加入第 ii 组噪声后的图片。</li><li>$x’$：当前噪声优化状态下的基准图片。</li><li>$F_e$：像素级误差函数，计算对抗噪声引起的预测误差。</li></ul></li><li>$\Delta_i$ 的正负含义：<ul><li><strong>正值</strong>：噪声提升了攻击性能。</li><li><strong>负值</strong>：噪声削弱了攻击性能。</li></ul></li></ul></li><li><p><strong>归一化评分</strong> $[\Delta]_b$：</p><ul><li>对正评分和负评分分别归一化，限制在 [0, 1] 和 [0, -1]： <script type="math/tex">[\Delta]^+ = \frac{[\Delta]^+}{\max([\Delta]^+)} \quad [\Delta]^- = \frac{[\Delta]^-}{|\min([\Delta]^-)|}</script></li><li>按正负评分数量进一步缩放： <script type="math/tex">[\Delta]^+ \leftarrow \frac{[\Delta]^+}{\#([\Delta]^+)} \quad [\Delta]^- \leftarrow \frac{[\Delta]^-}{\#([\Delta]^-)}</script></li><li><strong>目的</strong>：<br>  平衡正负评分的贡献，避免正负评分数量不对称导致优化方向偏差。</li></ul></li><li><p><strong>估计梯度 g</strong>：</p><ul><li>利用归一化评分作为权重，对随机噪声进行加权平均，计算梯度：$\sum_{i=1}^b \Delta_i \cdot \delta_i$</li><li>g 表示当前区域的梯度估计，指引噪声优化的方向。</li></ul></li><li><p><strong>L2归一化梯度</strong>：</p><ul><li>对梯度 g 进行 L2 归一化，以控制更新幅度和方向稳定性： <script type="math/tex">g_{\text{norm}} = \frac{\sqrt{3} \cdot e \cdot e \cdot g}{\|g\|_2}</script></li></ul></li><li><p><strong>更新当前噪声 s</strong>：</p><ul><li>将估计的梯度  添加到当前噪声 s 上： <script type="math/tex">g_{\text{norm}}\leftarrow s + \alpha \cdot g_{\text{norm}}</script></li><li>其中，$\alpha$  为学习率，控制噪声更新的步长。</li></ul></li></ol><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p>总框架：</p><p><img src="/posts/58495/frame.png" alt><br>算法：</p><p><img src="/posts/58495/Alg1.png" alt><br>基于竖条纹的随机噪声</p><p>迭代更新最大损失值</p><p>使用概率平方采样GETSQUAREAREA(iter, M, q, h)来获取更新的正方形区域</p><p>选取原训练图像上的方形图像区域，计算梯度来更新噪声正方形区域，直到最大损失不再增大</p><p>不断选取噪声方形区域，直到更新噪声不再使最大损失增大</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><hr><h2 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识"></a>相关知识</h2><p>“adversarial robustness” 机器学习模型在面对对抗性攻击（adversarial attacks）时，保持其预测准确性和稳定性的能力</p><p>搞清楚哪些是要花功夫去深入理解的，哪些不需要</p><p>感觉黑盒的对抗性模型在思路上也不复杂，只是需要的算力更高，因为需要迭代来寻找优化的方向，以及每一次迭代需要跑原模型进行验证</p><p>目前的黑盒攻击：</p><ul><li><p>基于替代模型的攻击:<br>  attackers construct a substitute model to execute white-box attacks and transfer the generated adversarial example to attack the victim model . To construct the substitute model, attackers employ the same training set as the victim model or reverse-engineer/synthesize a similar dataset.</p></li><li><p>基于查询的攻击：</p><ul><li>hard-label attacks：Hard-label attacks assume that the attacker can only access the predicted label of the victim model</li><li>soft-label attacks：soft-label attacks  assume the prediction score of each class is available</li></ul></li><li><p>对查询攻击的噪声优化方法：</p><ul><li>gradient estimation</li><li>heuristic random search</li><li>genetic algorithms</li></ul></li></ul><hr><h3 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h3><p>“Meanwhile, on the other hand, the adversarial patch for online services could also act as a deterrent against unauthorized users who attempt to upload our photographs to those services for video composition” (Cheng 等, 2024, p. 2) 对于版权的保护也是一个意义方面，所以讲故事的时候要考虑其社会价值和意义</p><hr><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>1.为什么要计算基准补丁和对抗补丁之间的误差，而不是直接计算原始图片和对抗补丁之间的误差？</p><pre><code>因为补丁的引入本身可能会干扰模型识别的结果，所以引入基准补丁来作为判准</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GAN</title>
      <link href="/posts/28928.html"/>
      <url>/posts/28928.html</url>
      
        <content type="html"><![CDATA[<p>生成对抗网络（Generative Adversarial Network，简称 GAN）的目标是通过生成器（Generator）和判别（Discriminator）的对抗训练，生成逼真的数据，可以用于图像处理</p><p>基本结构为：</p><ul><li>生成器(Generator)：尽量生成接近真实分布的数据</li><li>判别器(Discriminator)：分辨数据是生成器生成的还是真实的</li></ul><p>判别器损失，目的是最大化判别的正确性：</p><script type="math/tex; mode=display">\begin{equation}L_D = -\mathbb{E}_{x \sim p_{\text{data}}} [\log D(x)] - \mathbb{E}_{z \sim p_z} [\log(1 - D(G(z)))]\end{equation}</script><p>生成器损失，目的是使判别器判别为真实样本：</p><script type="math/tex; mode=display">L_G = -\mathbb{E}_{z \sim p_z} [\log D(G(z))]</script><p>训练过程：<br>生成器和判别器交替训练，固定训练器参数来更新判别器，和固定判别器参数来更新生成器</p><p>参考代码(MNIST):<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建文件夹</span></span><br><span class="line">os.makedirs(<span class="string">&quot;./images/gan/&quot;</span>, exist_ok=<span class="literal">True</span>)         <span class="comment">## 记录训练过程的图片效果</span></span><br><span class="line">os.makedirs(<span class="string">&quot;./save/gan/&quot;</span>, exist_ok=<span class="literal">True</span>)           <span class="comment">## 训练完成时模型保存的位置</span></span><br><span class="line">os.makedirs(<span class="string">&quot;./datasets/mnist&quot;</span>, exist_ok=<span class="literal">True</span>)      <span class="comment">## 下载数据集存放的位置</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 超参数配置</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--n_epochs&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">50</span>, <span class="built_in">help</span>=<span class="string">&quot;number of epochs of training&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">64</span>, <span class="built_in">help</span>=<span class="string">&quot;size of the batches&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--lr&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0002</span>, <span class="built_in">help</span>=<span class="string">&quot;adam: learning rate&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--b1&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.5</span>, <span class="built_in">help</span>=<span class="string">&quot;adam: decay of first order momentum of gradient&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--b2&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.999</span>, <span class="built_in">help</span>=<span class="string">&quot;adam: decay of first order momentum of gradient&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--n_cpu&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">2</span>, <span class="built_in">help</span>=<span class="string">&quot;number of cpu threads to use during batch generation&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--latent_dim&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>, <span class="built_in">help</span>=<span class="string">&quot;dimensionality of the latent space&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--img_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">28</span>, <span class="built_in">help</span>=<span class="string">&quot;size of each image dimension&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--channels&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&quot;number of image channels&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--sample_interval&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">500</span>, <span class="built_in">help</span>=<span class="string">&quot;interval betwen image samples&quot;</span>)</span><br><span class="line">opt = parser.parse_args()</span><br><span class="line"><span class="comment">## opt = parser.parse_args(args=[])                 ## 在colab中运行时，换为此行</span></span><br><span class="line"><span class="built_in">print</span>(opt)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 图像的尺寸:(1， 28， 28),  和图像的像素面积:(784)</span></span><br><span class="line">img_shape = (opt.channels, opt.img_size, opt.img_size)</span><br><span class="line">img_area = np.prod(img_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置cuda:(cuda:0)</span></span><br><span class="line">cuda = <span class="literal">True</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## mnist数据集下载</span></span><br><span class="line">mnist = datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./datasets/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.Compose(</span><br><span class="line">            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>])]</span><br><span class="line">        ), </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 配置数据到加载器</span></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    mnist,</span><br><span class="line">    batch_size=opt.batch_size,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## ##### 定义判别器 Discriminator ######</span></span><br><span class="line"><span class="comment">## 将图片28x28展开成784，然后通过多层感知器，中间经过斜率设置为0.2的LeakyReLU激活函数，</span></span><br><span class="line"><span class="comment">## 最后接sigmoid激活函数得到一个0到1之间的概率进行二分类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(img_area, <span class="number">512</span>),                   <span class="comment">## 输入特征数为784，输出为512</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),            <span class="comment">## 进行非线性映射</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">256</span>),                        <span class="comment">## 输入特征数为512，输出为256</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),            <span class="comment">## 进行非线性映射</span></span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>),                          <span class="comment">## 输入特征数为256，输出为1</span></span><br><span class="line">            nn.Sigmoid(),                               <span class="comment">## sigmoid是一个激活函数，二分类问题中可将实数映射到[0, 1],作为概率值, 多分类用softmax函数</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        img_flat = img.view(img.size(<span class="number">0</span>), -<span class="number">1</span>)            <span class="comment">## 鉴别器输入是一个被view展开的(784)的一维图像:(64, 784)</span></span><br><span class="line">        validity = self.model(img_flat)                 <span class="comment">## 通过鉴别器网络</span></span><br><span class="line">        <span class="keyword">return</span> validity                                 <span class="comment">## 鉴别器返回的是一个[0, 1]间的概率</span></span><br><span class="line"></span><br><span class="line">      </span><br><span class="line"><span class="comment">## ###### 定义生成器 Generator #####</span></span><br><span class="line"><span class="comment">## 输入一个100维的0～1之间的高斯分布，然后通过第一层线性变换将其映射到256维,</span></span><br><span class="line"><span class="comment">## 然后通过LeakyReLU激活函数，接着进行一个线性变换，再经过一个LeakyReLU激活函数，</span></span><br><span class="line"><span class="comment">## 然后经过线性变换将其变成784维，最后经过Tanh激活函数是希望生成的假的图片数据分布, 能够在-1～1之间。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        <span class="comment">## 模型中间块儿</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">block</span>(<span class="params">in_feat, out_feat, normalize=<span class="literal">True</span></span>):           <span class="comment">## block(in， out )</span></span><br><span class="line">            layers = [nn.Linear(in_feat, out_feat)]             <span class="comment">## 线性变换将输入映射到out维</span></span><br><span class="line">            <span class="keyword">if</span> normalize:</span><br><span class="line">                layers.append(nn.BatchNorm1d(out_feat, <span class="number">0.8</span>))    <span class="comment">## 正则化</span></span><br><span class="line">            layers.append(nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>))      <span class="comment">## 非线性激活函数</span></span><br><span class="line">            <span class="keyword">return</span> layers</span><br><span class="line">        <span class="comment">## prod():返回给定轴上的数组元素的乘积:1*28*28=784</span></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            *block(opt.latent_dim, <span class="number">128</span>, normalize=<span class="literal">False</span>),       <span class="comment">## 线性变化将输入映射 100 to 128, 正则化, LeakyReLU</span></span><br><span class="line">            *block(<span class="number">128</span>, <span class="number">256</span>),                                   <span class="comment">## 线性变化将输入映射 128 to 256, 正则化, LeakyReLU</span></span><br><span class="line">            *block(<span class="number">256</span>, <span class="number">512</span>),                                   <span class="comment">## 线性变化将输入映射 256 to 512, 正则化, LeakyReLU</span></span><br><span class="line">            *block(<span class="number">512</span>, <span class="number">1024</span>),                                  <span class="comment">## 线性变化将输入映射 512 to 1024, 正则化, LeakyReLU</span></span><br><span class="line">            nn.Linear(<span class="number">1024</span>, img_area),                          <span class="comment">## 线性变化将输入映射 1024 to 784</span></span><br><span class="line">            nn.Tanh()                                           <span class="comment">## 将(784)的数据每一个都映射到[-1, 1]之间</span></span><br><span class="line">        )</span><br><span class="line">    <span class="comment">## view():相当于numpy中的reshape，重新定义矩阵的形状:这里是reshape(64, 1, 28, 28)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):                                       <span class="comment">## 输入的是(64， 100)的噪声数据</span></span><br><span class="line">        imgs = self.model(z)                                     <span class="comment">## 噪声数据通过生成器模型</span></span><br><span class="line">        imgs = imgs.view(imgs.size(<span class="number">0</span>), *img_shape)                 <span class="comment">## reshape成(64, 1, 28, 28)</span></span><br><span class="line">        <span class="keyword">return</span> imgs                                              <span class="comment">## 输出为64张大小为(1, 28, 28)的图像</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 创建生成器，判别器对象</span></span><br><span class="line">generator = Generator()</span><br><span class="line">discriminator = Discriminator()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 首先需要定义loss的度量方式  （二分类的交叉熵）</span></span><br><span class="line">criterion = torch.nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 其次定义 优化函数,优化函数的学习率为0.0003</span></span><br><span class="line"><span class="comment">## betas:用于计算梯度以及梯度平方的运行平均值的系数</span></span><br><span class="line">optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))</span><br><span class="line">optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 如果有显卡，都在cuda模式中运行</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    generator = generator.cuda()</span><br><span class="line">    discriminator = discriminator.cuda()</span><br><span class="line">    criterion = criterion.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## ----------</span></span><br><span class="line"><span class="comment">##  Training</span></span><br><span class="line"><span class="comment">## ----------</span></span><br><span class="line"><span class="comment">## 进行多个epoch的训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(opt.n_epochs):                               <span class="comment">## epoch:50</span></span><br><span class="line">    <span class="keyword">for</span> i, (imgs, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):                  <span class="comment">## imgs:(64, 1, 28, 28)     _:label(64)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">## =============================训练判别器==================</span></span><br><span class="line">        <span class="comment">## view(): 相当于numpy中的reshape，重新定义矩阵的形状, 相当于reshape(128，784)  原来是(128, 1, 28, 28)</span></span><br><span class="line">        imgs = imgs.view(imgs.size(<span class="number">0</span>), -<span class="number">1</span>)                          <span class="comment">## 将图片展开为28*28=784  imgs:(64, 784)</span></span><br><span class="line">        real_img = Variable(imgs).cuda()                            <span class="comment">## 将tensor变成Variable放入计算图中，tensor变成variable之后才能进行反向传播求梯度</span></span><br><span class="line">        real_label = Variable(torch.ones(imgs.size(<span class="number">0</span>), <span class="number">1</span>)).cuda()      <span class="comment">## 定义真实的图片label为1</span></span><br><span class="line">        fake_label = Variable(torch.zeros(imgs.size(<span class="number">0</span>), <span class="number">1</span>)).cuda()     <span class="comment">## 定义假的图片的label为0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## ---------------------</span></span><br><span class="line">        <span class="comment">##  Train Discriminator</span></span><br><span class="line">        <span class="comment">## 分为两部分：1、真的图像判别为真；2、假的图像判别为假</span></span><br><span class="line">        <span class="comment">## ---------------------</span></span><br><span class="line">        <span class="comment">## 计算真实图片的损失</span></span><br><span class="line">        real_out = discriminator(real_img)                          <span class="comment">## 将真实图片放入判别器中</span></span><br><span class="line">        loss_real_D = criterion(real_out, real_label)               <span class="comment">## 得到真实图片的loss</span></span><br><span class="line">        real_scores = real_out                                      <span class="comment">## 得到真实图片的判别值，输出的值越接近1越好</span></span><br><span class="line">        <span class="comment">## 计算假的图片的损失</span></span><br><span class="line">        <span class="comment">## detach(): 从当前计算图中分离下来避免梯度传到G，因为G不用更新</span></span><br><span class="line">        z = Variable(torch.randn(imgs.size(<span class="number">0</span>), opt.latent_dim)).cuda()      <span class="comment">## 随机生成一些噪声, 大小为(128, 100)</span></span><br><span class="line">        fake_img = generator(z).detach()                                    <span class="comment">## 随机噪声放入生成网络中，生成一张假的图片。 </span></span><br><span class="line">        fake_out = discriminator(fake_img)                                  <span class="comment">## 判别器判断假的图片</span></span><br><span class="line">        loss_fake_D = criterion(fake_out, fake_label)                       <span class="comment">## 得到假的图片的loss</span></span><br><span class="line">        fake_scores = fake_out                                              <span class="comment">## 得到假图片的判别值，对于判别器来说，假图片的损失越接近0越好</span></span><br><span class="line">        <span class="comment">## 损失函数和优化</span></span><br><span class="line">        loss_D = loss_real_D + loss_fake_D                  <span class="comment">## 损失包括判真损失和判假损失</span></span><br><span class="line">        optimizer_D.zero_grad()                             <span class="comment">## 在反向传播之前，先将梯度归0</span></span><br><span class="line">        loss_D.backward()                                   <span class="comment">## 将误差反向传播</span></span><br><span class="line">        optimizer_D.step()                                  <span class="comment">## 更新参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## -----------------</span></span><br><span class="line">        <span class="comment">##  Train Generator</span></span><br><span class="line">        <span class="comment">## 原理：目的是希望生成的假的图片被判别器判断为真的图片，</span></span><br><span class="line">        <span class="comment">## 在此过程中，将判别器固定，将假的图片传入判别器的结果与真实的label对应，</span></span><br><span class="line">        <span class="comment">## 反向传播更新的参数是生成网络里面的参数，</span></span><br><span class="line">        <span class="comment">## 这样可以通过更新生成网络里面的参数，来训练网络，使得生成的图片让判别器以为是真的, 这样就达到了对抗的目的</span></span><br><span class="line">        <span class="comment">## -----------------</span></span><br><span class="line">        z = Variable(torch.randn(imgs.size(<span class="number">0</span>), opt.latent_dim)).cuda()      <span class="comment">## 得到随机噪声</span></span><br><span class="line">        fake_img = generator(z)                                             <span class="comment">## 随机噪声输入到生成器中，得到一副假的图片</span></span><br><span class="line">        output = discriminator(fake_img)                                    <span class="comment">## 经过判别器得到的结果</span></span><br><span class="line">        <span class="comment">## 损失函数和优化</span></span><br><span class="line">        loss_G = criterion(output, real_label)                              <span class="comment">## 得到的假的图片与真实的图片的label的loss</span></span><br><span class="line">        optimizer_G.zero_grad()                                             <span class="comment">## 梯度归0</span></span><br><span class="line">        loss_G.backward()                                                   <span class="comment">## 进行反向传播</span></span><br><span class="line">        optimizer_G.step()                                                  <span class="comment">## step()一般用在反向传播后面,用于更新生成网络的参数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 打印训练过程中的日志</span></span><br><span class="line">        <span class="comment">## item():取出单元素张量的元素值并返回该值，保持原元素类型不变</span></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">&quot;[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [D real: %f] [D fake: %f]&quot;</span></span><br><span class="line">                % (epoch, opt.n_epochs, i, <span class="built_in">len</span>(dataloader), loss_D.item(), loss_G.item(), real_scores.data.mean(), fake_scores.data.mean())</span><br><span class="line">            )</span><br><span class="line">        <span class="comment">## 保存训练过程中的图像</span></span><br><span class="line">        batches_done = epoch * <span class="built_in">len</span>(dataloader) + i</span><br><span class="line">        <span class="keyword">if</span> batches_done % opt.sample_interval == <span class="number">0</span>:</span><br><span class="line">            save_image(fake_img.data[:<span class="number">25</span>], <span class="string">&quot;./images/gan/%d.png&quot;</span> % batches_done, nrow=<span class="number">5</span>, normalize=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 保存模型</span></span><br><span class="line">torch.save(generator.state_dict(), <span class="string">&#x27;./save/gan/generator.pth&#x27;</span>)</span><br><span class="line">torch.save(discriminator.state_dict(), <span class="string">&#x27;./save/gan/discriminator.pth&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h3 id="改进："><a href="#改进：" class="headerlink" title="改进："></a>改进：</h3><p>提高稳定性，使用CGAN，加入了半监督或者有监督的学习</p><hr><p>二维高斯分布：</p><p><img src="/posts/28928/gaosi.webp" alt></p>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Square-based adversarial patch optimization</title>
      <link href="/posts/54157.html"/>
      <url>/posts/54157.html</url>
      
        <content type="html"><![CDATA[<p><strong>Square-based adversarial patch optimization</strong> 是一种对抗性补丁生成方法，其核心思想是通过<strong>方块（square）形式的区域划分和优化</strong>来生成对抗性补丁，以在目标任务中实现攻击效果。它是一种专门设计用于黑盒攻击的优化方法，适用于需要高效和大规模生成对抗性补丁的场景</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h3><p>对抗性补丁是一种特殊形式的对抗样本，它通过在输入图像的特定区域添加显著干扰（如某种图案或贴纸）来欺骗机器学习模型。传统补丁优化方法存在以下问题：</p><ol><li><p><strong>计算复杂度高</strong>：</p><ul><li>全图优化需要对大量像素进行梯度计算，尤其在黑盒环境下显得尤为困难。</li></ul></li><li><p><strong>黑盒场景中的挑战</strong>：</p><ul><li>黑盒攻击无法直接访问模型的梯度信息，需要通过迭代查询模型输出进行优化，这种方式通常效率较低。</li></ul></li><li><p><strong>扩展性问题</strong>：</p><ul><li>大规模补丁生成在计算资源受限时难以实施。</li></ul></li></ol><p>为了应对这些挑战，square-based adversarial patch optimization 提出了基于方块的策略，显著提高了效率和扩展性</p><h3 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a><strong>核心方法</strong></h3><h4 id="1-方块划分（Square-Partitioning）"><a href="#1-方块划分（Square-Partitioning）" class="headerlink" title="1. 方块划分（Square Partitioning）"></a><strong>1. 方块划分（Square Partitioning）</strong></h4><ul><li><p><strong>目标区域划分</strong>：</p><ul><li>将对抗性补丁划分为多个<strong>小方块</strong>（squares），每个方块作为优化的基本单元。</li><li>通过将全图优化转化为局部小范围优化，降低了计算复杂度。</li></ul></li><li><p><strong>区域选择策略</strong>：</p><ul><li>使用<strong>概率方块采样（probabilistic square sampling）</strong>：<ul><li>根据攻击目标的敏感区域分配更高的采样概率。</li><li>动态调整采样分布以逐步逼近最佳攻击效果。</li></ul></li></ul></li></ul><h4 id="2-基于得分的梯度估计（Score-based-Gradient-Estimation）"><a href="#2-基于得分的梯度估计（Score-based-Gradient-Estimation）" class="headerlink" title="2. 基于得分的梯度估计（Score-based Gradient Estimation）"></a><strong>2. 基于得分的梯度估计（Score-based Gradient Estimation）</strong></h4><ul><li><p><strong>模拟梯度信息</strong>：</p><ul><li>在黑盒场景中，无法直接获取模型的梯度。</li><li>通过查询模型的输出（如损失值、分类得分等），间接估计补丁对模型的攻击效果。</li></ul></li><li><p><strong>局部优化</strong>：</p><ul><li>对每个方块的像素值进行迭代优化，通过得分信息调整像素值，以逐步增强补丁的攻击能力。</li></ul></li></ul><h4 id="3-扩展性与高效性"><a href="#3-扩展性与高效性" class="headerlink" title="3. 扩展性与高效性"></a><strong>3. 扩展性与高效性</strong></h4><ul><li><strong>局部与全局结合</strong>：<ul><li>局部方块优化降低了每次迭代的计算成本，全局多方块协作确保攻击效果。</li></ul></li><li><strong>并行化优化</strong>：<ul><li>多个方块可以同时优化，提高效率，适合高维输入数据</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pixel-level_regression</title>
      <link href="/posts/5.html"/>
      <url>/posts/5.html</url>
      
        <content type="html"><![CDATA[<h2 id="单目深度估计"><a href="#单目深度估计" class="headerlink" title="单目深度估计"></a>单目深度估计</h2><p><a href="https://blog.csdn.net/guai7guai11/article/details/129291724">blog</a><br><a href="https://hugging-face.cn/docs/transformers/tasks/monocular_depth_estimation">huggingface</a><br>深度估计，就是获取图像中场景里的每个点到相机的距离信息，这种距离信息组成的图我们称之为深度图<br>![[Pasted image 20241122141944.png]]</p><ol><li><strong>深度网络</strong>：<ul><li>常用的网络结构包括：<ul><li><strong>卷积神经网络（CNN）</strong>：提取局部特征。</li><li><strong>编码器-解码器（Encoder-Decoder）结构</strong>：编码全局上下文信息，解码为逐像素的深度预测。</li><li><strong>Transformer</strong>：利用其捕获全局特征的能力提升精度。</li></ul></li></ul></li><li><strong>监督学习与无监督学习</strong>：<ul><li><strong>监督学习</strong>：需要带有深度标签的数据（如通过激光雷达标注）。</li><li><strong>无监督学习</strong>：通过图像对（如立体对、视频帧）间的几何关系训练模型。</li></ul></li><li><strong>损失函数</strong>：<ul><li>常见的损失包括：<ul><li><strong>L1/L2 损失</strong>：度量预测深度与真实深度的差异。</li><li><strong>结构相似性（SSIM）损失</strong>：增强深度预测的边缘对齐。</li></ul></li></ul></li></ol><p>应用场景：</p><ul><li><strong>自动驾驶</strong>：检测障碍物距离。</li><li><strong>增强现实（AR）</strong>：构建虚拟与现实交互的深度场景。</li><li><strong>机器人导航</strong>：帮助机器人感知周围的3D环境。</li></ul><p>简单来说就是给图测深度</p><h2 id="光流估计"><a href="#光流估计" class="headerlink" title="光流估计"></a>光流估计</h2><h4 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a><strong>任务定义</strong></h4><p>光流估计旨在计算时间序列(多个图片)中的每个像素的运动，即预测一个像素从一帧到下一帧的位置变化（用二维向量表示）。这实际上是像素级别的运动场估计。</p><h4 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a><strong>关键技术</strong></h4><ol><li><p><strong>经典方法</strong>：</p><ul><li><strong>稀疏光流</strong>（Sparse Optical Flow）：<ul><li>仅估计图像中特定关键点的运动。</li><li>示例算法：Lucas-Kanade 方法。</li></ul></li><li><strong>稠密光流</strong>（Dense Optical Flow）：<ul><li>为所有像素预测运动矢量。</li><li>示例算法：Horn-Schunck 方法。</li></ul></li></ul></li><li><p><strong>深度学习方法</strong>：</p><ul><li>利用卷积神经网络（CNN）或更复杂的架构，如：<ul><li><strong>FlowNet</strong>：<ul><li>基于卷积神经网络的端到端光流估计模型。</li></ul></li><li><strong>RAFT（Recurrent All-Pairs Field Transforms）</strong>：<ul><li>通过迭代优化计算高精度光流。</li></ul></li></ul></li></ul></li><li><p><strong>损失函数</strong>：</p><ul><li>常见损失包括：<ul><li><strong>像素差异损失</strong>：约束相邻帧中匹配像素的颜色相似性。</li><li><strong>平滑损失</strong>：鼓励光流场在局部区域的连续性。</li></ul></li></ul></li></ol><hr><h3 id="MDE-与-OFE-的联系与区别"><a href="#MDE-与-OFE-的联系与区别" class="headerlink" title="MDE 与 OFE 的联系与区别"></a>MDE 与 OFE 的联系与区别</h3><div class="table-container"><table><thead><tr><th><strong>维度</strong></th><th><strong>MDE</strong></th><th><strong>OFE</strong></th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td><td>两帧或多帧 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>每个像素的深度值</td><td>每个像素的二维位移向量</td></tr><tr><td><strong>目标</strong></td><td>恢复 3D 信息</td><td>捕获像素的运动信息</td></tr><tr><td><strong>应用场景</strong></td><td>自动驾驶、增强现实、机器人导航</td><td>视频稳定、目标跟踪、帧插值</td></tr><tr><td><strong>挑战</strong></td><td>深度缺失、复杂场景</td><td>遮挡、快速运动、噪声干扰</td></tr></tbody></table></div>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>black-box attack</title>
      <link href="/posts/3.html"/>
      <url>/posts/3.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-Training-a-Substitute-Model"><a href="#1-Training-a-Substitute-Model" class="headerlink" title="1. Training a Substitute Model"></a><strong>1. Training a Substitute Model</strong></h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a><strong>原理</strong></h3><ul><li>训练一个替代模型（substitute model）来近似目标模型的行为。</li><li>使用替代模型生成对抗样本，然后利用对抗样本在目标模型上的<strong>迁移性</strong>（transferability）来实现攻击。</li></ul><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a><strong>步骤</strong></h3><ol><li><p><strong>数据收集</strong>：</p><ul><li>收集目标模型输入和输出的对应数据（通过查询目标模型）。</li><li>输入可以是原始数据集的一部分，输出是目标模型的预测结果。</li></ul></li><li><p><strong>替代模型训练</strong>：</p><ul><li>用收集的数据训练一个与目标模型功能类似的替代模型。</li><li>替代模型不需要与目标模型结构相同，但需要尽量拟合目标模型的决策边界。</li></ul></li><li><p><strong>生成对抗样本</strong>：</p><ul><li>在替代模型上利用常见的白盒攻击方法（如 FGSM、PGD）生成对抗样本。</li></ul></li><li><p><strong>攻击目标模型</strong>：</p><ul><li>将生成的对抗样本输入目标模型，通过迁移性实现攻击。</li></ul></li></ol><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a><strong>优点</strong></h3><ul><li><strong>查询次数少</strong>：只需有限的目标模型查询用于数据收集。</li><li><strong>效率高</strong>：生成对抗样本后，可以离线测试多个目标模型。</li><li><strong>迁移性利用</strong>：在多种目标模型之间具有一定的通用性。</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a><strong>缺点</strong></h3><ul><li><strong>依赖迁移性</strong>：对抗样本在目标模型上的成功率依赖于替代模型和目标模型的相似性。</li><li><strong>训练成本高</strong>：需要额外训练一个替代模型，尤其在复杂任务中可能耗时较长。</li><li><strong>不适用于动态更新的目标模型</strong>：如果目标模型频繁变化，替代模型可能需要重新训练。</li></ul><hr><h2 id="2-Iteratively-Querying-the-Model"><a href="#2-Iteratively-Querying-the-Model" class="headerlink" title="2. Iteratively Querying the Model"></a><strong>2. Iteratively Querying the Model</strong></h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a><strong>原理</strong></h3><ul><li>利用目标模型的查询接口，通过反复查询并利用返回的输出（如分类得分或概率分布）来优化对抗样本。</li><li>该方法通常结合<strong>优化技术</strong>（如梯度估计或遗传算法）逐步逼近目标模型的决策边界。</li></ul><h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a><strong>步骤</strong></h3><ol><li><p><strong>初始化对抗样本</strong>：</p><ul><li>从一个原始输入开始，初始化一个潜在的对抗样本。</li></ul></li><li><p><strong>优化对抗样本</strong>：</p><ul><li>利用目标模型返回的输出，逐步调整对抗样本的像素值或特征，使其朝着最大化目标（如误分类的可能性）方向优化。</li><li><strong>常用优化策略</strong>：<ul><li><strong>梯度估计</strong>：在黑盒场景中通过数值方法近似目标模型的梯度，指导优化过程（如 ZOO 或 NES 方法）。</li><li><strong>进化算法</strong>：利用遗传算法或其他进化策略调整输入值。</li></ul></li></ul></li><li><p><strong>检查攻击效果</strong>：</p><ul><li>在每次查询后，评估当前对抗样本是否成功欺骗目标模型。</li><li>如果攻击成功，则终止；否则继续优化。</li></ul></li></ol><h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a><strong>优点</strong></h3><ul><li><strong>无需替代模型</strong>：直接操作目标模型，无需构建额外的近似网络。</li><li><strong>灵活性强</strong>：可以针对不同类型的目标模型（如分类、回归等）调整优化策略。</li></ul><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a><strong>缺点</strong></h3><ul><li><strong>高查询成本</strong>：需要多次查询目标模型，特别是在高维输入或复杂任务中。</li><li><strong>效率问题</strong>：每次优化需要反复迭代，可能导致攻击时间较长。</li><li><strong>防御风险</strong>：目标模型可能限制查询次数或使用检测机制防御频繁查询。</li></ul><hr><h2 id="两种方法的对比"><a href="#两种方法的对比" class="headerlink" title="两种方法的对比"></a><strong>两种方法的对比</strong></h2><div class="table-container"><table><thead><tr><th>特性</th><th><strong>Training a Substitute Model</strong></th><th><strong>Iteratively Querying the Model</strong></th></tr></thead><tbody><tr><td><strong>查询次数</strong></td><td>较少</td><td>较多</td></tr><tr><td><strong>依赖目标模型信息</strong></td><td>中等（需部分输入输出对）</td><td>高（需查询结果）</td></tr><tr><td><strong>迁移性</strong></td><td>强，依赖替代模型与目标模型的相似性</td><td>不依赖迁移性，直接优化目标模型</td></tr><tr><td><strong>适用场景</strong></td><td>目标模型不易查询，且有充足资源训练替代模型</td><td>可频繁查询目标模型，且允许高查询成本</td></tr><tr><td><strong>实现难度</strong></td><td>中等（需额外训练替代模型）</td><td>较高（需设计有效的查询与优化策略）</td></tr><tr><td><strong>防御风险</strong></td><td>低，查询次数较少，隐蔽性较强</td><td>高，频繁查询易被检测或限制</td></tr></tbody></table></div>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>white-box attack</title>
      <link href="/posts/2.html"/>
      <url>/posts/2.html</url>
      
        <content type="html"><![CDATA[<p>白盒攻击是对抗性攻击的一种形式，指攻击者能够完全访问目标模型的结构、参数和梯度信息。利用这些信息，攻击者可以精确地构造对抗样本，以最大化模型误判的可能性</p><p>简单来讲，就是针对目标要达到的攻击效果，来有目的性的更新白盒模型的梯度</p><hr><h2 id="常见白盒攻击方法"><a href="#常见白盒攻击方法" class="headerlink" title="常见白盒攻击方法"></a><strong>常见白盒攻击方法</strong></h2><h3 id="1-FGSM（Fast-Gradient-Sign-Method）"><a href="#1-FGSM（Fast-Gradient-Sign-Method）" class="headerlink" title="1. FGSM（Fast Gradient Sign Method）"></a><strong>1. FGSM（Fast Gradient Sign Method）</strong></h3><ul><li><strong>提出者</strong>：Goodfellow 等人在 2014 年的论文中提出。</li><li><strong>核心思想</strong>：<ul><li>使用目标模型的损失函数梯度信息，沿输入的梯度方向快速调整输入样本，以生成对抗样本。</li></ul></li><li><strong>公式</strong>： <script type="math/tex">x′=x+ϵ⋅sign(∇xJ(θ,x,y))x' = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))</script><ul><li>$xx$：原始输入。</li><li>$x′x’$：对抗样本。</li><li>$ϵ\epsilon$：攻击强度，表示每次扰动的幅度。</li><li>$∇xJ(θ,x,y)\nabla_x J(\theta, x, y)$：损失函数 JJ 关于输入 xx 的梯度。</li><li>$sign\text{sign}$：符号函数，确保扰动方向一致。</li></ul></li><li><strong>特点</strong>：<ul><li>计算简单，适合快速生成对抗样本。</li><li>可控扰动幅度 $ϵ\epsilon$决定攻击强度。</li></ul></li></ul><p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 攻击的模型是alexnet</span></span><br><span class="line">model = models.alexnet(pretrained=<span class="literal">True</span>).to(device).<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 先预测出这张img的标签，需要先取他的数据再转移到cpu再转成numpy</span></span><br><span class="line">label = np.argmax(model(img).data.cpu().numpy())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;label=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(label)) </span><br><span class="line"><span class="comment"># 图像数据梯度可以获取</span></span><br><span class="line">img.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 冻结模型梯度</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 注意这里，和一般的深度学习训练不一样</span></span><br><span class="line">optimizer = torch.optim.Adam([img]，lr=<span class="number">0.01</span>)</span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">target = <span class="number">288</span> <span class="comment"># 定向攻击的标签</span></span><br><span class="line">target = Variable(torch.Tensor([<span class="built_in">float</span>(target)]).to(device).long())</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># forward + backward</span></span><br><span class="line">    output = model(img)</span><br><span class="line"></span><br><span class="line">    loss = loss_func(output, target)</span><br><span class="line">    label = np.argmax(output.data.cpu().numpy())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch=&#123;&#125; loss=&#123;&#125; label=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss, label))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果定向攻击成功</span></span><br><span class="line">    <span class="keyword">if</span> label == target:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><hr><h3 id="2-BIM（Basic-Iterative-Method）"><a href="#2-BIM（Basic-Iterative-Method）" class="headerlink" title="2. BIM（Basic Iterative Method）"></a><strong>2. BIM（Basic Iterative Method）</strong></h3><ul><li><strong>扩展于 FGSM</strong>，通过多次迭代调整输入，逐步逼近模型决策边界。</li><li><p><strong>公式</strong>： <script type="math/tex">x′(t+1)=clipx,ϵ(x′(t)+α⋅sign(∇xJ(θ,x′(t),y)))x'^{(t+1)} = \text{clip}_{x, \epsilon} \big( x'^{(t)} + \alpha \cdot \text{sign}(\nabla_x J(\theta, x'^{(t)}, y)) \big)</script></p><ul><li>$α\alpha$：每次迭代的步长。</li><li>$clipx,ϵ\text{clip}_{x, \epsilon}$：限制对抗样本的扰动在 ϵ\epsilon 范围内，防止过度修改输入。</li></ul></li><li><p><strong>特点</strong>：</p><ul><li>相比 FGSM，更强的攻击效果，但计算成本较高。</li></ul></li></ul><hr><h3 id="3-PGD（Projected-Gradient-Descent）"><a href="#3-PGD（Projected-Gradient-Descent）" class="headerlink" title="3. PGD（Projected Gradient Descent）"></a><strong>3. PGD（Projected Gradient Descent）</strong></h3><ul><li><strong>进一步扩展 BIM</strong>，通过优化目标模型的损失函数进行对抗样本生成。</li><li>允许在一定的扰动限制范围（如 ϵ\epsilon-ball）内不断优化输入。</li><li><strong>特点</strong>：<ul><li>是一种强大的多步迭代攻击方法。</li><li>被认为是评估对抗鲁棒性的标准攻击方法。</li></ul></li></ul><hr><h3 id="4-CW（Carlini-and-Wagner-Attack）"><a href="#4-CW（Carlini-and-Wagner-Attack）" class="headerlink" title="4. CW（Carlini and Wagner Attack）"></a><strong>4. CW（Carlini and Wagner Attack）</strong></h3><ul><li><strong>目标</strong>：<ul><li>最大程度降低对抗样本的可察觉性，同时保持高攻击成功率。</li></ul></li><li><strong>方法</strong>：<ul><li>通过优化问题，将对抗样本生成形式化为目标函数最小化问题： <script type="math/tex">min⁡x∥x−x′∥p+c⋅f(x′)\min_x \|x - x'\|_p + c \cdot f(x')</script><ul><li>$f(x′)f(x’)$：表示模型的错误分类目标。</li><li>$∥x−x′∥p|x - x’|_p$：控制输入与对抗样本的相似性。</li><li>$cc$：平衡两个目标之间的权重。</li></ul></li></ul></li><li><strong>特点</strong>：<ul><li>非常强大，但计算开销较大。</li></ul></li></ul><hr><h3 id="5-DeepFool"><a href="#5-DeepFool" class="headerlink" title="5. DeepFool"></a><strong>5. DeepFool</strong></h3><ul><li><strong>目标</strong>：<ul><li>寻找模型分类边界附近的最小扰动，生成对抗样本。</li></ul></li><li><strong>特点</strong>：<ul><li>能生成扰动幅度极小的对抗样本。</li><li>假设模型是线性分类器，逐步逼近真实分类边界。</li></ul></li></ul><hr><h3 id="6-One-Pixel-Attack"><a href="#6-One-Pixel-Attack" class="headerlink" title="6. One-Pixel Attack"></a><strong>6. One-Pixel Attack</strong></h3><ul><li><strong>核心思想</strong>：<ul><li>在输入图像中仅修改一个像素值，测试模型的鲁棒性。</li></ul></li><li><strong>特点</strong>：<ul><li>扰动小，便于解释。</li><li>攻击能力较弱，适用于简单的模型和任务。</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attack as Defense</title>
      <link href="/posts/4.html"/>
      <url>/posts/4.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/2410.14966">原文</a></p><h2 id="意义和提出"><a href="#意义和提出" class="headerlink" title="意义和提出"></a>意义和提出</h2></blockquote><p>传统的模型编辑是编辑作为推理的结果，所以传统后门的效果是当进行模型编辑的时候，后台给模型加一个后门标识，产生指定的更改结果</p><p>本文的模型编辑后台加一个mask noise，来保护指定的区域，在推理阶段不能根据text更改</p><p>并且使用的是原模型，只在推理阶段进行添加noise就可以达到保护的效果，所以称为运行时</p><p>利用的是后门可以指定激活一些神经元的特性，所以可以通过激活指定神经元来达到保护指定的区域的效果，同时保证其他区域的编辑和生成不受影响，确实很像后门</p><p>三个优化目标：</p><ul><li>植入损失，</li><li>不完全触发损失，保持保护区域的激活 </li><li>隐藏损失，保证编辑区域的可用性</li></ul><p>三方：</p><ul><li>模型商</li><li>用户</li><li>图片版权拥有者</li></ul><p>很简单的思路：给定模型，对抗模型，训练noise来干扰功能，具体的idea是针对什么模型作出干扰，考虑两方面：</p><ul><li>模型的选择，根据有没有实际的应用价值，以及前人有没有做过相关工作</li><li>没有工作就老方法解决新问题，探索新的模型的安全问题，Attack as Defense属于这个</li><li>有工作就新方法解决老问题，使用新的对抗或者优化方法，BadPart属于这个</li></ul><hr><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="期望效果"><a href="#期望效果" class="headerlink" title="期望效果"></a>期望效果</h3><blockquote><p>训练出一个产生noise的模型，对于给定的mask和图片，可以生成对抗性的noise，能有效对抗给定的模型，也算是一个黑盒方法</p></blockquote><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><h4 id="原始的后门模型"><a href="#原始的后门模型" class="headerlink" title="原始的后门模型"></a>原始的后门模型</h4><p>修复损失(inpainting loss)：</p><ul><li>原始图像和生成图像的像素差异$L_{inpaint}$</li><li>生成器的损失$L_{adv}$</li></ul><p>其中的inpaint损失改为具有后门监督的损失，可以训练出具有后门的修复模型</p><h4 id="Run-time-backdoor"><a href="#Run-time-backdoor" class="headerlink" title="Run-time backdoor"></a>Run-time backdoor</h4><p>基于现有模型，不需要训练阶段的后门植入</p><script type="math/tex; mode=display">\mathcal{L}_{implant} = \mathbb{E}_{\phi, \mathcal{P}(x), m}\space[\begin{Vmatrix}\phi_x - \mathcal{IM}(\mathcal{P}(x),\mathcal{T}(m))\end{Vmatrix}^2]</script><script type="math/tex; mode=display">\mathcal{L}_{hide} = \mathbb{E}_{ \mathcal{P}(x), m}\space[\begin{Vmatrix}\mathcal{IM}(x,m_0) - \mathcal{IM}(\mathcal{P}(x),m_0)\end{Vmatrix}^2]</script><p>其中：</p><ul><li>$\mathcal{P}(x)=x\otimes\mathcal{P}$ P是可训练的受保护区域的参数(?模型中的表现是什么:原始图片上加了一个可训练的随机噪声noise)</li><li>$\phi_x$是target</li><li>$\mathcal{IM}(\mathcal{P}(x),\mathcal{T}(m))$是加了扰动的保护图片</li><li>$m_0$是除了保护区域之外的区域掩码</li></ul><p>有加一个扩展掩码，因为实验结果表明只有部分区域能够收到保护，所以需要向外扩展一圈</p><p>更改后的公式：</p><script type="math/tex; mode=display">\mathcal{L}_{incomplete} = \mathbb{E}_{\phi, \mathcal{P}(x), m}\space[\begin{Vmatrix}\phi_x - \mathcal{IM}(\mathcal{P}(x),\mathcal{E}(m))\end{Vmatrix}^2]</script><script type="math/tex; mode=display">\mathcal{L}_{hide} = \mathbb{E}_{ \mathcal{P}(x), m}\space[\begin{Vmatrix}\mathcal{IM}(x,\mathcal{E}'(m)) - \mathcal{IM}(\mathcal{P}(x),\mathcal{E}'(m))\end{Vmatrix}^2]</script><p>其中，</p><ul><li>扩展区域掩码为$\mathcal{E}(m)$</li><li>不属于原有的掩码但是属于现有掩码的部分为$\mathcal{E}’(m)$<script type="math/tex; mode=display">\mathcal{E}'(m) = max(0,min(1,\mathcal{E}(m) - \mathcal{T}(m))))</script></li></ul><p>总损失：</p><script type="math/tex; mode=display">\mathcal{L}_{total} = \mathcal{L}_{impaint} + \mathcal{L}_{inconplete} + \mathcal{L}_{hide}</script><p>经实验证明，设置超参：</p><ul><li>hide的权重为2</li><li>noise的迭代训练次数为20</li></ul><p>简而言之就是给了两个优化目标，一个是保护区域，指标是能够加了保护噪音，一个是未保护区域保持原有功能</p><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p>整体过程：对于给定的模型，根据原始图片和修改的mask，计算模型修改加了noise以后的修改结果和target之间的均方误差，然后更新noise来达到对抗修改的效果</p><h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><p>$\phi_x$:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># target是颜色为和原始图像中的平均颜色差别最大的颜色，尺寸为noise的目标图片</span></span><br><span class="line">target = generate_target(ori_fname, (adv_noise.shape[<span class="number">3</span>],adv_noise.shape[<span class="number">2</span>])).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个新的颜色为find_color_with_max_difference的图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_target</span>(<span class="params">image, size</span>):</span><br><span class="line">    img = PIL.Image.<span class="built_in">open</span>(image)</span><br><span class="line">    width, height = img.size</span><br><span class="line">    max_diff_color = find_color_with_max_difference(img)</span><br><span class="line"></span><br><span class="line">    new_img = PIL.Image.new(<span class="string">&#x27;RGB&#x27;</span>, (width, height), max_diff_color).resize(size)</span><br><span class="line">    new_img = preprocess(new_img)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 和图像平均颜色差异最大的颜色</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_color_with_max_difference</span>(<span class="params">img</span>):</span><br><span class="line">    pixels = np.array(img)</span><br><span class="line"></span><br><span class="line">    average_color = np.mean(pixels, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    average_color = <span class="built_in">tuple</span>(average_color.astype(<span class="built_in">int</span>))</span><br><span class="line"></span><br><span class="line">    unique_colors = [(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>)]</span><br><span class="line">    random.seed(<span class="number">42</span>)</span><br><span class="line">    torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">42</span>)</span><br><span class="line">    <span class="comment"># for _ in range(4):</span></span><br><span class="line">    <span class="comment">#     color = tuple(np.random.randint(0, 256, 3))</span></span><br><span class="line">    <span class="comment">#     unique_colors.append(color)</span></span><br><span class="line">    differences = [np.<span class="built_in">sum</span>((np.<span class="built_in">abs</span>(np.array(average_color) - np.array(color))/<span class="number">255</span>)**<span class="number">2</span>) <span class="keyword">for</span> color <span class="keyword">in</span> unique_colors]</span><br><span class="line"></span><br><span class="line">    max_diff_index = np.argmax(differences)</span><br><span class="line">    max_diff_color = unique_colors[max_diff_index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max_diff_color</span><br></pre></td></tr></table></figure></p><p>扩展掩码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">expand_mask</span>(<span class="params">mask_tensor, exp_size</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(mask_tensor.shape) == <span class="number">4</span> <span class="keyword">and</span> mask_tensor.shape[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">and</span> mask_tensor.shape[<span class="number">1</span>] == <span class="number">1</span>, \</span><br><span class="line">        <span class="string">&quot;the shape of mask has to be:  1x1xheightxwidth&quot;</span></span><br><span class="line"></span><br><span class="line">    kernel = torch.ones(<span class="number">1</span>, <span class="number">1</span>, exp_size, exp_size, dtype=torch.float32, device=mask_tensor.device)</span><br><span class="line">    padding_size = (exp_size - <span class="number">1</span>) // <span class="number">2</span> <span class="comment">#大小不变</span></span><br><span class="line"></span><br><span class="line">    expanded_mask = F.conv2d(mask_tensor.<span class="built_in">float</span>(), kernel, padding=padding_size)</span><br><span class="line">    expanded_mask = (expanded_mask &gt; <span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> expanded_mask</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;main&#x27;&#x27;&#x27;</span></span><br><span class="line">expd_mask = expand_mask(batch_ben[<span class="string">&#x27;mask&#x27;</span>], <span class="number">17</span>)</span><br><span class="line">batch_adv_expd[<span class="string">&#x27;mask&#x27;</span>] = expd_mask.to(<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure></p><p>$\mathcal{E}’(m)$（barch_adv_hide）:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>] = batch_adv_expd[<span class="string">&#x27;mask&#x27;</span>] - batch_ben[<span class="string">&#x27;mask&#x27;</span>]</span><br><span class="line">batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>].data = torch.clip(batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>].data, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>] = (batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>] &gt; <span class="number">0</span>) * <span class="number">1</span></span><br><span class="line">batch_ben_hide[<span class="string">&#x27;mask&#x27;</span>].data = batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>].data</span><br></pre></td></tr></table></figure></p><p>超参，hide的权重为2，noise的迭代训练次数为20:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">loss = loss_attack + loss_incp + <span class="number">2</span>*loss_hide</span><br><span class="line"><span class="comment"># the number of training iterations for the protective noise to 20</span></span><br><span class="line"><span class="keyword">for</span> i_iter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br></pre></td></tr></table></figure></p><p>loss，对应公式区域:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">lama模型修改以后返回的结果,adv图像加了噪声</span></span><br><span class="line"><span class="string">attack_res: adv掩码原始掩码</span></span><br><span class="line"><span class="string">adv_hide_res: adv_hide掩码为E&#x27;(m)</span></span><br><span class="line"><span class="string">adv_expd_res: adv_expd掩码为扩展</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">attack_res = model(batch_adv)[predict_config.out_key][<span class="number">0</span>]</span><br><span class="line">adv_expd_res = model(batch_adv_expd)[predict_config.out_key][<span class="number">0</span>]</span><br><span class="line">adv_hide_res = model(batch_adv_hide)[predict_config.out_key][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">ben_hide_res = model(batch_ben_hide)[predict_config.out_key][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss</span></span><br><span class="line">loss_attack = mse_masked(attack_res, target, batch_adv[<span class="string">&#x27;mask&#x27;</span>])</span><br><span class="line">loss_incp = mse_masked(adv_expd_res, target, batch_adv_expd[<span class="string">&#x27;mask&#x27;</span>])</span><br><span class="line">loss_hide = mse_masked(adv_hide_res, ben_hide_res, batch_adv_hide[<span class="string">&#x27;mask&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;iter&quot;</span>, i_iter, <span class="string">&quot;, attack loss: &quot;</span>, loss_attack.data, <span class="string">&quot;, incp loss: &quot;</span>, loss_incp.data, <span class="string">&quot;, hide loss: &quot;</span>, loss_hide.data)</span><br><span class="line">loss = loss_attack + loss_incp + <span class="number">2</span>*loss_hide</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></p><p>同时题外考虑，对于公司来讲，也要权衡新方法的开销成本以及带来的经济效益，做tradeoff来判断是否以更小的成本带来更大的经济价值，这个过程需要远见和魄力，这也是学术和工业界的区别，学术不需要考虑实际成本，工业界没有足够的创造力</p><hr><h2 id="Writing"><a href="#Writing" class="headerlink" title="Writing"></a>Writing</h2><p><img src="/posts/4/intro.png" alt></p><blockquote><p>introduction的惯例，写总结,读文章可以先读这个部分和摘要，把这篇文章的地位和意义搞清楚，再去了解背景细节</p><p>Method要体现出和原始方法的对比</p></blockquote><hr><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ol><li><p>为什么将noise训练的target设置为和原始图片颜色差异最大的纯色图片？<br> 颜色差异最大是因为目标越极端，修改模型越依赖准确的梯度来进行优化，极端的颜色目标在颜色空间中与原始图片的距离最远，意味着要修改成原始图片的颜色的开销最大，为了将图片修改成这个目标颜色，修改模型需要在颜色空间中执行极大的变换，在这种极端目标下，噪声的扰动会显著干扰修改模型的梯度流，从而更难以找到有效的修改方向</p></li><li><p>如何保证noise以纯色图片为target但是优化结果是肉眼不可见的图片？<br> 限制noise在训练过程中的取值范围：<br> <code>adv_noise.data = torch.clip(adv_noise.data, -0.025, 0.025)</code>范围小就肉眼不可见<br> 可以不用纯色，差异大就可以，使用纯色是因为好生成</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TrojanNN</title>
      <link href="/posts/11148.html"/>
      <url>/posts/11148.html</url>
      
        <content type="html"><![CDATA[<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><p>不需要使用原本的数据集的后门攻击，</p><h2 id="“INTRODUCTION”-pdf"><a href="#“INTRODUCTION”-pdf" class="headerlink" title="“INTRODUCTION” (pdf)"></a>“INTRODUCTION” (<a href="zotero://open-pdf/library/items/VJ7XHLCZ?page=3">pdf</a>)</h2><p>对比增量学习，增量学习因为原本的权重很大，所以没法是模型直接改变行为</p><p>提出trojan trigger，即使用触发器来达到植入后门，不会影响其他数据并且使用少数据集进行训练</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>三个步骤：</p><ul><li><p>触发器生成器</p></li><li><p>训练数据生成器</p></li><li><p>模型重训练</p></li></ul><h3 id="Trojan-trigger-generation"><a href="#Trojan-trigger-generation" class="headerlink" title="Trojan trigger generation"></a>Trojan trigger generation</h3><p>通过选定某些层的某些神经元作为最大值激活的目标，来反向梯度传播调整后门标识的值，即在标识和神经元之间建立强联系，然后通过神经元的选择和训练，和最终结果建立强联系</p><ol><li><p>神经元的选择</p></li><li><p>反向梯度传播的详细过程</p></li></ol><h3 id="Training-data-generation"><a href="#Training-data-generation" class="headerlink" title="Training data generation"></a>Training data generation</h3><p>通过反向梯度传播来找到每一个分类结果的最大化样本图片作为模拟训练集</p><p>细节：</p><p>就是一个基本的梯度下降，loss是均方误差</p><p>mask是用来和梯度相乘，来掩盖掉其他部分的梯度的</p><p>DENOISE Function 降噪，思想是使用最小化总方差，值为像素之间的平方误差之和，来使像素之间平滑，作用是可以提升准确率</p><h3 id="Retraining-model"><a href="#Retraining-model" class="headerlink" title="Retraining model"></a>Retraining model</h3><p>训练过程中要注意的</p><ol><li><p>生成的数据集是原始分类图片＋加了后门以后的</p></li><li><p>建立选定的神经元激活和伪装的结果之间的强联系</p></li><li><p>减少伪装的结果以及其他结果的权重，来防止权重膨胀，因为改变以后的模型权重会增大对于伪造目标分类的概率</p></li></ol><p>面临着两个重要选择：</p><ol><li><p>标识的选择(难以做到权重补偿)</p></li><li><p>激活神经元的选择(不能直接选择激发输出节点)</p></li></ol><h3 id="Internal-Neuron-Selection"><a href="#Internal-Neuron-Selection" class="headerlink" title="Internal Neuron Selection"></a>Internal Neuron Selection</h3><p>一个神经元指的是权重矩阵一列的值：</p><p>$xW$</p><p>两个layer的神经元之间的权重指的就是第一层的输出结果在第二层上的映射，所以就是第二层的权重值</p><p>神经元的选择是选择与上一层连接权重最大的神经元，因为这样更好做反向梯度传播来达到目标激活，否则达不到目标激活</p><h2 id="Denfense"><a href="#Denfense" class="headerlink" title="Denfense"></a>Denfense</h2><p>检测每种类型的占比，肉眼可见的伪造额度类别的占比大</p><hr><h1 id="随想"><a href="#随想" class="headerlink" title="随想"></a>随想</h1><p>“ATTACK DEMONSTRATION” (<a href="zotero://select/library/items/MB8A8962">Liu 等, p. 2</a>) (<a href="zotero://open-pdf/library/items/VJ7XHLCZ?page=4">pdf</a>)</p><p>仅仅是添加stamp的话，对于网络来说是加强数据的联系或者权重，来产生预计的效果，并没有很复杂</p><p>注意力机制能否有对应应用？　</p><p>对视觉和听觉都进行了attack，是否有统一的方法？</p><p>自己玩一玩，动态视觉识别验证，特定触发器</p><p>重复语句太多，论文一直这样吗？可能讲故事也是一个刚需</p><p>调整像素使得选定的神经元的值最大，来达到输入带有trigger时能够激活神经元</p><p>为什么不影响正常分类？</p><p>怎么compensate的？</p><p>任意徽标不起作用，因为会产生均匀的小影响</p><p>直接选择激活输出节点也不行，原因是：</p><ol><li><p>只选择目标节点激活不够</p></li><li><p>没训练模型的其他部分</p></li></ol><hr><p>典型的，虽然简单，但是思想和方法是开创性的基石文章<br>代码写的一坨，跑都跑不起，连个requirement都没有，毕竟是2018的老古董了</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fine-tune</title>
      <link href="/posts/46217.html"/>
      <url>/posts/46217.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="/posts/15915.html"/>
      <url>/posts/15915.html</url>
      
        <content type="html"><![CDATA[<p>理解一样东西需要细节数学运算和宏观意义把控，缺一不可<br>理论的理解和实践的熟练也缺一不可</p><h2 id="相关学习资料"><a href="#相关学习资料" class="headerlink" title="相关学习资料"></a>相关学习资料</h2><p>论文原文：<br><a href="https://arxiv.org/pdf/1706.03762">Attention is all you need</a></p><p>注意力机制讲解：<br><a href="https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.337.search-card.all.click">3B1B</a><br><a href="https://www.bilibili.com/video/BV1dt4y1J7ov/?spm_id_from=333.337.search-card.all.click">attention</a></p><p>transformer代码实现：<br><a href="https://adaning.github.io/posts/63679.html">Whole code</a><br><a href="https://www.bilibili.com/video/BV1Kq4y1H7FL/?p=2&amp;vd_source=d288520c13d72c2ea1d57b46f2d9b2a4">李沐</a></p><p>hugging-face:<br><a href="https://huggingface.co/blog/encoder-decoder">encoder &amp; decoder</a></p><h2 id="论文写作"><a href="#论文写作" class="headerlink" title="论文写作"></a>论文写作</h2><ol><li><p>相关工作写清楚论文中用到的知识，哪些是前人已有的工作，哪些是作为自己的创新和不同而提出的</p></li><li><p>论文本质上是给别人介绍自己的工作，想一下如果论文火了能不能成为经典</p></li><li><p>ai之所以是玄学是因为，关键在于根据loss来反向传播更新权重，但是当参数量很大的时候，权重的具体表现，权重的改变对整个模型带来的影响，包括对数据有很多可行的处理方式，都是无法直观的理解不同设计的意义，以及不同改变带来的结果是怎么样的，都只能做事后诸葛亮</p></li><li><p>并不是每一步都有显著的意义，太不美了，不如数学，比如为啥要除的是 $\sqrt{d_t}$而不是$d_t$，</p></li></ol><h2 id="Transformer-架构理解"><a href="#Transformer-架构理解" class="headerlink" title="Transformer 架构理解"></a>Transformer 架构理解</h2><p>总架构如图(摘自论文)：</p><p><img src="/posts/15915/Pasted%20image%2020240924164008.png" alt></p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>总的来说，transformer是一个seq2seq的序列转录模型，主体有encoder和decoder两个部分:</p><ul><li>encoder接收原始输入，对输入进行一系列处理以后产生编码输出，传递给decoder</li><li>decoder接收<strong>整体的</strong>encoder的输出和上一个decoder的输出(预测阶段)或者整个的目标序列(训练阶段)，来生成对下一个token的预测</li></ul><h3 id="地位"><a href="#地位" class="headerlink" title="地位"></a>地位</h3><p>在transformer之前，NLP使用RNN来预测，但是RNN需要之前的时间步作为下一个时间步的输入传递，速度慢，并且无法获取<strong>全局</strong>的信息<br>Transformer提出了<strong>全部基于注意力机制</strong>时间序列预测模型，意味着训练阶段不需要等待上一个时间步的信息输入，每次参数调整都是基于全局，直接获取全局信息，只有预测阶段的推理需要用到时间步循环</p><p>从抽象上来看，tranformer将每个具有不同上下文环境的输入token映射到了一个超高维的空间中，来记录不同上下文环境的token的位置，并在预测阶段进行拟合<br>这种做法像是把上下文信息(或者现实世界的其他格式信息)也作为了数据的一部分进行具象化，来以很多参数记录</p><blockquote><p>为什么其他模型没有transformer这么具有统治性的地位？其设计思想背后的精髓之处是什么？<br>存疑，缺乏宏观视野</p></blockquote><h3 id="具体架构"><a href="#具体架构" class="headerlink" title="具体架构"></a>具体架构</h3><p>根据图片能看出，总体是常见的翻译模型的架构，有两部分，编码器和解码器，编码器接收原始输入，解码器接收编码器输出和目标序列的输入或者已经预测的目标序列(见后面详解)</p><p>编码器负责将输入的序列首先转换成向量的形式(映射到向量空间)，然后通过注意力机制来调整向量学习输入向量全局的信息，再通过前馈层来扩展特征信息，输出具有<strong>输入全局上下文</strong>的输入向量</p><p>解码器较为复杂，解码器接收的输入有两部分，<strong>一部分是编码器输出的原始序列向量</strong>，另一部分(Outputs)是：</p><ul><li>训练阶段：整个目标序列的输入</li><li>预测阶段：之前生成的所有token<br>最终的输出是：</li><li>训练阶段：对于给出的目标序列中，对于每一个词的下一个词的预测概率(经过softmax之后的logist)，输出整个预测的序列，并和给定的正确的目标序列做损失函数来调整参数</li><li>预测阶段：经过softmax生成一个预测单词，并加入下一个时间步的token的decoder输入中</li></ul><p>训练阶段：</p><p>预测阶段：<br><img src="/posts/15915/Pasted%20image%2020240927233853.png" alt></p><p>以上是对整个transformer的输入和输出的介绍，下面详解编码器和解码器的详细架构</p><p>首先分别介绍几个模块</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>embedding层是One-hot的替代，使用低维稠密的one-hot矩阵进行计算更快速。embedding会使用Word2vec训练出一个词嵌入矩阵，每一个词对应一个词向量，将句子中的词使用one-hot编码，再与词嵌入矩阵相乘，即可有词矩阵，词向量转化在二维空间中，可以代表不同词之间的含义的关系<br>在transformer中，embedding词向量的维度是<strong>512</strong>，并且在embedding之后还要数值的扩大，因为embedding编码之后因为有L2正则化，所以输入维度越大，每个向量的值越小，所以transformer乘了一个$\sqrt{512}$来扩大，来能够匹配position encoding的大小</p><h3 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h3><p>对于输入的词向量，由于注意力机制关注的是整个序列的所有token的相互之间的影响，但是我们希望的是前一个token对后一个token的预测，所以序列中token的顺序也至关重要，所以对于输入的embedding向量，还要进行position encoding编码来标明每个token的位置信息</p><p>具体的标注方式如下：</p><script type="math/tex; mode=display">PE_{pos,2i} = sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{pos, 2i+1} = cos(pos/10000^{2i/d_{model}})</script><p>其中的$pos$是token的位置，$d_{model}$是512，通过计算正余弦来生成一个尺寸也是512的向量，并与embedding向量相加，获得带有位置编码的向量</p><p>那么问题来了，为什么仅仅是相加就能让模型识别到向量的位置信息，模型如何辨别向量的值是position encoding相加得来，还是原本的值？<br><strong>答：这tm是玄学</strong></p><p>代码实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">1024</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        positional_encoding = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 位置编码的缩放因子</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * </span><br><span class="line">            (-torch.log(torch.Tensor([<span class="number">10000</span>])) / d_model)) <span class="comment"># [max_len / 2]</span></span><br><span class="line">        positional_encoding[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position* div_term)</span><br><span class="line">        positional_encoding[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position* div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 传入参数形状为[seq_len, batchsize, embedding_size(d_model)]</span></span><br><span class="line">        positional_encoding = positional_encoding.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注册缓冲区来持久性保存位置编码</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, positional_encoding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x即为传入的序列向量,只加到x的序列长度</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), ...]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure></p><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>从输入输出上看，注意力机制实现了更新输入向量来达到记录向量之间的关系</p><p>从含义上看，注意力机制设计的目的是根据输入的所有向量来找到每一个向量之间的关系，所以输入是要使用注意力机制的向量，输出是对向量进行相互之间的注意力更新，产生上下文来存储信息</p><p>为什么transformer要使用注意力机制？<br>因为注意力机制能关注到全局信息，并且不需要上一个时间步的输出，同时更新所有信息，其底层的运算就是简单的矩阵运算，所以可高度并行</p><h4 id="缩放点积注意力-Scaled-Dot-Product-Attention"><a href="#缩放点积注意力-Scaled-Dot-Product-Attention" class="headerlink" title="缩放点积注意力(Scaled Dot Product Attention)"></a>缩放点积注意力(Scaled Dot Product Attention)</h4><p>总的注意力机制公式为：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>基本缩放点积的注意力机制如下：<br><img src="/posts/15915/Pasted%20image%2020240929131447.png" alt></p><p>注意力机制中有三个矩阵：Ｑ，Ｋ，V<br>其中Q为query，Ｋ为key，Ｖ为value<br>对于特定的<strong>查询</strong>，寻找和<strong>键值</strong>之间的相似度，计算出相似度矩阵，并加权乘V矩阵，然后以此来更新向量(相加，表现为残差)，在查询之后使用softmax来讲注意力权重归一化</p><p>Q的维度比嵌入向量小很多，意义是映射到一个带有查询含义的更小的空间，所以寻找相似度的方法是，Q与K的乘积越大，代表越相似<br>而缩放点积指的是在计算出相似度权重矩阵$QK^T$之后，为了防止过大的值导致梯度更新困难，所以除以一个$\sqrt{d_k}$来缩小点积结果</p><p><img src="/posts/15915/Pasted%20image%2020240928172915.png" alt></p><p>其中V的尺寸很大(和嵌入矩阵维度一样)，所以进行大矩阵的低秩分解，分解成$d_q(d_v) * embedding_size$大小(以及转置大小)的矩阵，来减少参数</p><p>每一个单头注意力有唯一一个$w_q, w_k, w_v$</p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缩放点积注意力</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, atten_mask</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Q: [batch, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">        K: [batch, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">        V: [batch, n_heads, len_v, d_v]</span></span><br><span class="line"><span class="string">        attn_mask: [batch, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k)</span><br><span class="line">        <span class="comment"># 什么掩码？</span></span><br><span class="line">        scores.masked_fill_(atten_mask, -<span class="number">1e9</span>)</span><br><span class="line">        <span class="comment"># 对于每个查询（Query），计算其与所有键（Key）的相似度分布。</span></span><br><span class="line">        <span class="comment"># 获取相似度矩阵attn和增值向量prob</span></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores) <span class="comment"># [batch, n_heads, len_q, len_k]</span></span><br><span class="line">        prob = torch.matmul(attn, V) <span class="comment"># [batch, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="keyword">return</span> prob, attn</span><br></pre></td></tr></table></figure></p><h4 id="自注意力-Self-attention"><a href="#自注意力-Self-attention" class="headerlink" title="自注意力(Self-attention)"></a>自注意力(Self-attention)</h4><p>transformer中使用的是自注意力机制，即输入的Q,K,V是同一个矩阵——输入向量，意义是注意力更新的是输入向量本身<br>有三个注意力权重$w_q, w_k, w_v$与输入向量相乘<br>自注意力是Q,K,V是同一个矩阵X，X做一定线性变换$w_q, w_k, w_v$来作为Q,K,V矩阵，来更新自己<br>不同的注意力是根据Q,K计算权重矩阵的的方式不同</p><h4 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h4><p>为了向CNN的多个过滤器一样提取一个序列中的多种注意力机制，所以使用多重注意力，每个头有一个自己的$w_q, w_k, w_v$，其中$w_v$很特殊，多头中只有一个子矩阵(待进一步研究)，</p><p>muti-head attention相当于卷积网络的多层输出，每一层提取不一样的特征，transformer中投影了ｈ次，投影到低维再最后把所有结果加回来</p><script type="math/tex; mode=display">MultiHead(Q,K,V) = Concat(head_1,head_2,...,head_h)W^O</script><script type="math/tex; mode=display">where\ head_i = Attention(QW_i^Q,KW_I^K,VW_I^V)</script><p><img src="/posts/15915/Pasted%20image%2020240929165402.png" alt></p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 多头需要原始的残差给到Norm层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_heads=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.attention = ScaledDotProductAttention()</span><br><span class="line">        <span class="comment"># 整体权重矩阵</span></span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_O = nn.Linear(d_model, d_model, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.layer_norm = AddNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, atten_mask</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        To make sure multihead attention can be used both in encoder and decoder, </span></span><br><span class="line"><span class="string">        we use Q, K, V respectively.</span></span><br><span class="line"><span class="string">        input_Q: [batch, len_q, d_model]</span></span><br><span class="line"><span class="string">        input_K: [batch, len_k, d_model]</span></span><br><span class="line"><span class="string">        input_V: [batch, len_v, d_model]</span></span><br><span class="line"><span class="string">        输入的是原始的总维度</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        residual, batch = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if atten_mask.dim() == 2:</span></span><br><span class="line">        <span class="comment">#     atten_mask = atten_mask.unsqueeze(1).unsqueeze(2)  # (batch_size, 1, seq_len)</span></span><br><span class="line">        <span class="comment"># print(f&quot;atten_mask shape before repeat: &#123;atten_mask.shape&#125;&quot;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将乘积结果切分为多头</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [batch, n_heads, len_q, d_k]</span></span><br><span class="line">        K = self.W_K(input_K).view(batch, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [batch, n_heads, len_k, d_k]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [batch, n_heads, len_v, d_v]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 和attn匹配</span></span><br><span class="line">        atten_mask = atten_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 传入缩放点积</span></span><br><span class="line">        prob, attn = self.attention(Q,K,V,atten_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#　匹配总输入尺度,transpose后如果接permute或者view必须要加contiguous</span></span><br><span class="line">        prob = prob.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() <span class="comment"># [batch, len_q, n_heads, d_v]</span></span><br><span class="line">        prob = prob.view(batch, -<span class="number">1</span>, n_heads * d_v).contiguous() <span class="comment"># [batch, len_q, n_heads * d_v]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 包括输入与输入的微调，其中output就是微调的结果</span></span><br><span class="line">        output = self.W_O(prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(residual, output), attn</span><br></pre></td></tr></table></figure></p><h4 id="Transformer中的注意力"><a href="#Transformer中的注意力" class="headerlink" title="Transformer中的注意力"></a>Transformer中的注意力</h4><p>transformer使用多头自注意力，在decoder中，首先目标(预测)序列经过一个带有mask的多头自注意力，然后输出作为Q，encoder的输入作为V和K，来更新预测向量</p><h4 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h4><p>在transformer中有两种mask，一种是对于序列长度不一致的padding，要使用mask来将无意义的padding去掉，一种是对于decoder的注意力机制，在更新目标序列或预测序列的向量时要使用mask来让后面的token不影响前面的token的向量更新，因为后面token对于前面token应该是未知的</p><p>对于padding_mask，有两种使用场景</p><ul><li>encoder_input和decoder_input需要mask</li><li>encoder_Input和decoder_input之间需要mask</li></ul><p>首先是对于输入向量的基本padding:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences, max_length</span>):</span><br><span class="line">    encoder_inputs, decoder_inputs, decoder_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        <span class="comment"># 获得index序列</span></span><br><span class="line">        <span class="built_in">print</span>(sentences)</span><br><span class="line">        encoder_input = [source_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]</span><br><span class="line">        decoder_input = [target_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]</span><br><span class="line">        decoder_output = [target_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad encoder_input</span></span><br><span class="line">        encoder_input += [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(encoder_input))</span><br><span class="line">        encoder_input = encoder_input[:max_length]  <span class="comment"># Truncate if longer</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad decoder_input</span></span><br><span class="line">        decoder_input += [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(decoder_input))</span><br><span class="line">        decoder_input = decoder_input[:max_length]  <span class="comment"># Truncate if longer</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pad decoder_output</span></span><br><span class="line">        decoder_output += [<span class="number">0</span>] * (max_length - <span class="built_in">len</span>(decoder_output))</span><br><span class="line">        decoder_output = decoder_output[:max_length]  <span class="comment"># Truncate if longer</span></span><br><span class="line"></span><br><span class="line">        encoder_inputs.append(encoder_input) <span class="comment"># [seq_index1, seq_index2]</span></span><br><span class="line">        decoder_inputs.append(decoder_input)</span><br><span class="line">        decoder_outputs.append(decoder_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(encoder_inputs), torch.LongTensor(decoder_inputs), torch.LongTensor(decoder_outputs)</span><br></pre></td></tr></table></figure></p><p>padding_mask:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成padding掩码，大小和相似度矩阵相同</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Padding, because of unequal in source_len and target_len.   </span></span><br><span class="line"><span class="string">    parameters:</span></span><br><span class="line"><span class="string">    seq_q: [batch, seq_len]</span></span><br><span class="line"><span class="string">    seq_k: [batch, seq_len] </span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">    mask: [batch, len_q, len_k]                         </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 两个seq_len的长度是不同的，# 注意使用残差连接</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, norm_size = d_model, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm,self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=p_drop)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(norm_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(self.dropout(y) + x)</span><br><span class="line">    batch, len_q = seq_q.size()</span><br><span class="line">    batch, len_k = seq_k.size()</span><br><span class="line">    <span class="built_in">print</span>(len_q, len_k)</span><br><span class="line">    <span class="comment"># we define index of PAD is 0, if tensor equals (zero) PAD tokens</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># [batch, 1, len_k]   </span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch, len_q, len_k) <span class="comment"># [batch, len_q, len_k]</span></span><br></pre></td></tr></table></figure></p><p>attention_mask:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequent_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    atten_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    subsequent_mask = np.triu(np.ones(atten_shape), k=<span class="number">1</span>)            </span><br><span class="line">    subsequent_mask = torch.from_numpy(subsequent_mask)</span><br><span class="line">    <span class="keyword">return</span> subsequent_mask</span><br></pre></td></tr></table></figure></p><h4 id="Norm"><a href="#Norm" class="headerlink" title="Norm"></a>Norm</h4><p>batchnorm: 在一个batch的样本里面，将所有样本的同一个特征(列)变为均值为0，方差为1<br>layernorm：对每一个样本做normlization，而不是feature</p><h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h4><p>MLP：单隐藏层，从512维度扩展到2048再到512</p><script type="math/tex; mode=display">FNN(x) = ReLU(xW_1 + b_1)W_2 + b_2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Linear - Relu - Linear</span></span><br><span class="line">    <span class="comment"># hidden_size = 2048</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedForwardNetwork, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.layer_norm = AddNorm(d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x:[batch, seq_len, d_model]</span></span><br><span class="line">        residual = x</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.layer_norm(residual, x)</span><br></pre></td></tr></table></figure><p>attention之后的向量中已经包含了序列信息，所以直接对每个词向量单独做MLP就可以</p><p>MLP的作用是将序列信息映射成更为复杂的语义空间，加大拟合的程度，有更多参数</p><h3 id="全流程"><a href="#全流程" class="headerlink" title="全流程"></a>全流程</h3><p>encoder:<br>encoder输入做embedding编码，以及position编码，投入到N=6的layer中，每一层经过一个自注意力和一个MLP，最后获得总的encoder输出<br>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重复的layer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self._self_mutihead_attention = MultiHeadAttention(n_heads)</span><br><span class="line">        self.norm = AddNorm(d_model)</span><br><span class="line">        self.ffn = FeedForwardNetwork()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, pad_mask</span>):</span><br><span class="line">        <span class="comment"># x: [batch_size, seq_len, embedding_size]</span></span><br><span class="line">        x, attn= self._self_mutihead_attention(x, x, x, pad_mask)</span><br><span class="line">        x = self.ffn(x)</span><br><span class="line">        <span class="keyword">return</span> x, attn</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(source_vocab_size, d_model)</span><br><span class="line">        self.position_encoding = PositionalEncoding(d_model=d_model)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="built_in">input</span> = x</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.position_encoding(x.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        pad_mask = get_attn_pad_mask(<span class="built_in">input</span>, <span class="built_in">input</span>) </span><br><span class="line">        encoder_self_attns = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># encoder_output: [batch, source_len, d_model]</span></span><br><span class="line">            <span class="comment"># encoder_self_attn: [batch, n_heads, source_len, source_len]</span></span><br><span class="line">            x, encoder_self_attn = layer(x, pad_mask)</span><br><span class="line">            encoder_self_attns.append(encoder_self_attn)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, encoder_self_attns</span><br></pre></td></tr></table></figure></p><p>decoder：<br>训练阶段，输入目标序列，进行embedding和position编码，输入N=6的layer中，每一层经过掩码自注意力，与encoder一起的注意力，以及MLP获得输出，再经过线性层和Softmax获得每一个词之后的词的预测，根据目标序列来计算损失，反向传播，<strong>特性是每一次的预测词只与上一个的token有关</strong><br>预测阶段也是一样，只是每次预测一个词，并加入到序列中参与下一个时间步的attention</p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.mutihead_attention = MultiHeadAttention()</span><br><span class="line">        self.ffn = FeedForwardNetwork()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, encoder_output, encoder_decoder_mask, self_mask</span>):</span><br><span class="line">        x, decoder_self_attn = self.mutihead_attention(x, x, x, encoder_decoder_mask)</span><br><span class="line">        x, decoder_encoder_attn = self.mutihead_attention(encoder_output, encoder_output, x, self_mask)</span><br><span class="line">        x = self.ffn(x)</span><br><span class="line">        <span class="keyword">return</span> x, decoder_self_attn, decoder_encoder_attn</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(target_vocab_size, d_model)</span><br><span class="line">        self.position_encoding = PositionalEncoding(d_model=d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_input, encoder_input, encoder_output</span>):</span><br><span class="line"></span><br><span class="line">        decoder_output = self.embedding(decoder_input)</span><br><span class="line">        decoder_output = self.position_encoding(decoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 有decoder自己的mask，包括pad和seq，也有pad类型的encoder_decoder_mask</span></span><br><span class="line">        <span class="comment"># 和Encoder相对应, 但Decoder和Encoder使用了两个不同的Embedding. 对于Mask, 可以把自回归Mask和Padding Mask用torch.gt整合成一个Mask, 送入其中.</span></span><br><span class="line">        decoder_pad_mask = get_attn_pad_mask(decoder_input, decoder_input).to(device)</span><br><span class="line">        decoder_seq_mask = get_attn_subsequent_mask(decoder_input).to(device)</span><br><span class="line"></span><br><span class="line">        decoder_self_mask = torch.gt(decoder_pad_mask + decoder_seq_mask, <span class="number">0</span>)</span><br><span class="line">        encoder_decoder_pad_mask = get_attn_pad_mask(decoder_input, encoder_input).to(device)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;encoder_decoder_pad_mask shape: <span class="subst">&#123;encoder_decoder_pad_mask.shape&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        decoder_self_attns = <span class="built_in">list</span>()</span><br><span class="line">        decoder_encoder_attns = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:        </span><br><span class="line"></span><br><span class="line">            decoder_output, decoder_self_attn, decoder_encoder_attn  = layer(</span><br><span class="line">                decoder_output, </span><br><span class="line">                encoder_output, </span><br><span class="line">                encoder_decoder_pad_mask,</span><br><span class="line">                decoder_self_mask </span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            decoder_encoder_attns.append(decoder_encoder_attn)</span><br><span class="line">            decoder_self_attns.append(decoder_self_attn)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> decoder_output, decoder_self_attns, decoder_encoder_attns</span><br></pre></td></tr></table></figure></p><p>整体的transformer代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder()</span><br><span class="line">        self.decoder = Decoder()</span><br><span class="line">        self.fc = nn.Linear(d_model, target_vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 不用softmax</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, decoder_input</span>):</span><br><span class="line">        encoder_output, encoder_attns = self.encoder(encoder_input)</span><br><span class="line">        decoder_output, decoder_self_attns, decoder_encoder_attns = self.decoder(decoder_input, encoder_input, encoder_output)</span><br><span class="line">        outputs_logist = self.fc(decoder_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs_logist.view(-<span class="number">1</span>, outputs_logist.size(-<span class="number">1</span>)), encoder_attns, decoder_self_attns, decoder_encoder_attns</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes</title>
      <link href="/posts/11340.html"/>
      <url>/posts/11340.html</url>
      
        <content type="html"><![CDATA[<p>物理云：通过计算机网络提供的数据中心的计算资源服务</p><p>云计算：资源池化来提供服务的模型，有iaas，paas…，层不一样</p><p>无服务计算：只关心应用程序的逻辑，其他的任务如服务的实现交给cloud provider</p><p>Faas:会计算这个函数，这种输入用到多少CPU等资源消耗，进而进行计费</p><p>云端开发程序的一个目的是随着用户越来越多，弹性的扩展资源，是基于云提供商提供的服务开发一个更为上层的服务</p><p>KVM在内核去监管用户空间的虚拟机应用，硬件的虚拟化由QEMU实现，虚拟机对虚拟硬件资源的请求会翻译为内核空间的指令，通过KVM调度真实硬件资源</p><p>云的开销部分来自于Hypervisor等带来的资源消耗，以及OS的许可</p><p>容器没有虚拟化，是基于OS的运行环境打包，其大小也是弹性的，取决于你需要多少资源<br>所以编排容器的时候需要声明容器的环境以及容器之间的交互情况，所谓环境是包管理和资源分配</p><p>镜像(image)：系统状态的快照，存有相关的静态资源，运行时成为系统组件<br>所以容器镜像实在基础镜像之上添加层，容器是镜像的运行载体，镜像也由容器创建</p><p>Dockerfile会按层通过docker builder构建镜像，在base image上添加不同的层来添加环境组件</p><p><img src="/posts/11340/figure2.png" alt><br>容器的生命周期由deamon管理</p><p>容器之间的隔离通过namespace</p><p>Docker uses a union file system to combine these layers into a single image. Changes in one layer do not affect the underlying layers. This makes images lightweight, as only the changes are stored.</p><p>Each layer represents a snapshot of the filesystem, which can be useful for rolling back changes or tracking modifications.</p><p>微服务架构去部署应用程序可以解耦合依赖关系，将不同组件放到不同容器中，只暴露API接口</p><p>容器编排是对容器进行多功能的管理和优化，包括创建副本，动态增加等<br>Docker管理容器的生命周期，kubernetes监控docker的运行，负责容器编排<br>也就是说，docker提供容器技术<br>Kubelet 通过容器运行时接口（CRI）与 Docker（或其他容器运行时）进行通信，以创建、启动和停止容器<br>每一个节点一个docker daemon                </p><p>kubernetes将很多组件用作他自己本身的架构</p><p>kubelet可以帮助找到container，最重要的是API-server，用户和系统运行的接口<br>etcd分布式数据库存储着调度信息</p><p>pod内部的container之间共享网络和存储，IP相同，可挂载共享卷来持久化存储<br>pod将容器进行逻辑分割，提供管理抽象，所以pod与docker之间解偶</p><p>一个pod里面的一个container就像是一个host里面的一个app</p><p><img src="/posts/11340/figure1.png" alt="image"></p><p>kubectl run [podname] —image=[默认搜索dockerhub]</p><p>-i 是标准输入，-t是tty</p><p>kubectl describe <code>component name</code> <code>specific name</code></p><p>一个cluster中的pod之间可以通过pod的虚拟ip进行通信</p><p>horizontal scaling: an dynamically add or remove nodes to accommodate changing  workload demands</p><p>本机访问节点：kubectl port-forward —address 0.0.0.0 mypod 8888:8080</p><p>kubelet belongs to control plane</p><p>kubelet excute the schedule</p><p>objects: controller organize the system resources to run or operate</p><p>different pods can install different database</p><p>default type: Cluster IP</p><p>Deployment:</p><ul><li>滚动更新和重建更新</li></ul><p>Endpoint:</p><ul><li>对pods抽象的低级别扩展</li><li>增强Service对pods控制的自定义性</li></ul><p>存储：</p><ul><li>外部存储，无法提供healthy check</li><li>k8s提供的功能，提供存储管理<ul><li>暂时存储</li><li>永久存储</li></ul></li></ul><p>emptyDir Volume: </p><ul><li>周期：Pod内</li><li>目录： /opt/data</li></ul><p>nodeAffinity: </p><ul><li>确保PV绑定到节点上</li></ul><p>ConfigMap：</p><ul><li>主要作用是将配置信息与容器化应用分离，使得配置可以在不重新构建镜像的情况下进行修改</li></ul><p>一个容器运行一个进程，启动容器设置初始化任务</p><p>Downward API：</p><ul><li>注入静态的环境变量和元信息到Volumn上</li></ul><p>Init container：</p><ul><li>容器之间共享存储，所以可以将一些功能分开</li></ul><p>Sidecar container(动态更新存储数据)：</p><ul><li>数据同步器</li><li>传递和监控容器信息</li><li>管理流量和增强安全性(nvoy in Istio)</li><li>更新配置</li><li>安全验证</li></ul><p>Ambassador pattern：</p><ul><li>远程访问外部数据时实现解耦合</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Miscellanea</title>
      <link href="/posts/undefined.html"/>
      <url>/posts/undefined.html</url>
      
        <content type="html"><![CDATA[<ul><li>返回登录：Ctrl + Alt + Delete 或者 sudo systemctl restart sddm.service</li><li>搜多文件内容：<code>grep -rn &quot;runlog&quot; *</code>   </li><li>从大到小显示存储占用排名：<code>sudo du -ah / | sort -rh | head -n 20</code></li></ul><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><p>一般不允许自己安装包，有几个方式来安装：</p><ol><li>使用模块化系统加载预装软件包：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">module avail  <span class="comment"># 查看可用软件模块</span></span><br><span class="line">module load python/3.8  <span class="comment"># 加载特定版本的 Python</span></span><br></pre></td></tr></table></figure></li><li><p>使用conda:<br>curl安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line"><span class="built_in">chmod</span> +x Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">./Miniconda3-latest-Linux-x86_64.sh -b -p <span class="variable">$HOME</span>/miniconda</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;<span class="variable">$HOME</span>/miniconda/bin:<span class="variable">$PATH</span>&quot;</span></span><br><span class="line">conda init bash</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><p>可以用conda开玩了，除了没图形界面和GPU，其他的都是一样的<br>matplotlib库也可以使用</p></li><li><p>远程连接</p><ol><li><p>vscode</p></li><li><p>ssh</p></li></ol></li><li><p>测试计算性能</p></li></ol><ul><li>CPU:<ul><li>大矩阵乘法<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">N = <span class="number">8000</span>  </span><br><span class="line">A = np.random.rand(N, N)</span><br><span class="line">B = np.random.rand(N, N)</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">C = np.dot(A, B)</span><br><span class="line">end_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;矩阵乘法完成，耗时: <span class="subst">&#123;end_time - start_time:<span class="number">.2</span>f&#125;</span> 秒&quot;</span>) </span><br></pre></td></tr></table></figure></li><li>蒙特卡罗法计算 π 值<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">monte_carlo_pi</span>(<span class="params">num_samples</span>):</span><br><span class="line">    inside_circle = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">        x = random.uniform(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        y = random.uniform(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> x**<span class="number">2</span> + y**<span class="number">2</span> &lt;= <span class="number">1</span>:</span><br><span class="line">            inside_circle += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span> * inside_circle / num_samples</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试计算 π 值的时间</span></span><br><span class="line">num_samples = <span class="number">10</span>**<span class="number">8</span>  <span class="comment"># 样本数量，可以增大以增加计算量</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">pi_estimate = monte_carlo_pi(num_samples)</span><br><span class="line">end_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;估算的 π 值: <span class="subst">&#123;pi_estimate&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;计算耗时: <span class="subst">&#123;end_time - start_time:<span class="number">.2</span>f&#125;</span> 秒&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="System"><a href="#System" class="headerlink" title="System"></a>System</h3><ul><li>查看cpu数和核数：<code>cat /proc/cpuinfo | grep cores</code></li><li>cpu数：<code>cat /proc/cpuinfo | grep processor | wc -l</code></li><li>RedHat用rpm包管理器：<code>sudo rpm -ivh package.rpm  # 安装一个 RPM 包</code></li><li>查看内核：<code>uname -a</code></li><li></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图书馆窗边有感</title>
      <link href="/posts/56897.html"/>
      <url>/posts/56897.html</url>
      
        <content type="html"><![CDATA[<p>24年3月27日，阴     </p><p>成都的天像是小孩的脸，喜笑颜开与闷闷不乐之间仿佛没有什么界限，窗边的风也随着心情的起伏时而拂过我在键盘上的指尖，平淡中也许有几分惬意    </p><p>还是一个往常的日子，向往常一样起床，洗漱，做到图书馆的窗旁，向往常一样认为今日不往常，总是在夹杂着安逸和焦虑的复杂心态中带上耳机，沉浸到人类最伟大的作品——音乐中，感受着音乐的艺术幻境带给我的平静，幻想着今日能解决困扰了我许许多多个“今日”的问题    </p><p>窗边是人群的流水，和几颗饱满丰腴的树。大学是个神奇的地方，晴天是充满朝气的，而阴天似乎也很少有不快，只是有一种安逸的宁静，仿佛汇聚了最多的善意，没有高中的硝烟四漫，也没有公司的惶惶不可终日，有的是散漫之上的希望，如黄沙中的树，也如废土里的花。人群流动着，上下课的高峰像是染剂四散在水中的刹那，看得出来，奔去上课的同学们被书包压弯了肩，下课奔出的同学们教学楼的门口的美景惊昂了头，那脸上的释然和仿佛刚从监狱里释放的自由快乐是掩盖不来的，不知道十年之后，他们是否会怀念这一段最青春的时光</p><p>而我，一个泛泛之辈，在窗边写着在现在的我看来，足以骄傲到取悦自己小小见解，这见解是否对的，我不知道，也不在乎，这些由自己产出的文字能在短暂的时间中取悦自己，就足够了，“正确”在此刻毫无意义，就像这段文字一样，意义是人赋予的</p><p>今日也许是有些不同的，在昨日忍无可忍数据库老师以低效来糟蹋我交与她的三个小时后，感叹人类愚蠢的心在那时达到了巅峰。所以毅然决然要捋清楚知识体系的学习方法，以及时间的合理安排，便做博客记录。然而，随着思考的深入，不由产生了对自己所做一切的意义何在的疑问。仅对自己而言，或许并没有在意一些事情的得失与成败，诚然，从小被世俗的成功观念影响的我确实会对一些成就而产生悲喜，但悲喜过后又是追求下一个世俗成功的无限循环，于是对意义的思考又转向了悲喜本身的价值。</p><p>人的感受是有生理基础的，多巴胺，内啡肽，生理物质刺激大脑产生奖励机制，刺激人们产生愉悦的感觉，产生成就感、充实感、吃到美食和恋爱的快乐愉悦，和摄取毒品不同的是，人们冠以这种获取方式为”不正确”的，因为这样会使人们的大脑反馈和正常人不同，从而破坏奖励机制，丧失社会功能，但所为功成名就和日常中的快乐不也是这种物质和感受吗，所以差别在哪里</p><p>对刺激物质的频繁获取会对脏器产生影响。人在获取快乐的时候是处于高度兴奋的状态的，这意味着全身器官的高速运转，和能量的快速消耗，人们习惯于简单的获取快乐之后会渴望越来越频繁，也就是成瘾，频繁意味着超出了身体所能负荷的极限，没有休息的余地，从而搞垮身体，降低各种激素的分泌水平。而成瘾之后的截断意味这带来负面情绪，这是及其强大的suffuring</p><p>马克思说过，人是社会关系的集合体，人类最大的痛苦之一就是孤独，孤独带来的痛苦可以让短暂的自我陶醉背后只剩空虚，羁绊才是人生的意义，达成成就后他人的认可，亲人的快乐，也许我所追求的就是带给别人笑容吧，人们的灵魂生来孤独，孤独的灵魂相互照耀才能取暖，两份快乐永远比一份快乐要多的多</p><p>人类真是个奇怪的物种，利他才能感受到自己的精神满足和价值所在，这又何尝不是高等生命的精彩之处呢</p><p>写到这里，肚子的哀嚎提醒我要先满足自己的生存需求才能给旁人创造伟大的价值，回首看向窗边，窗边的人群还在流动着，希望能一直流动下去</p>]]></content>
      
      
      
        <tags>
            
            <tag> 思考 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于对学习方法的思考</title>
      <link href="/posts/53885.html"/>
      <url>/posts/53885.html</url>
      
        <content type="html"><![CDATA[<p>时间是经不起考验的，当回首度过的时光，总觉得意犹未尽，总有可以优化和提升的地方，理想之境是无愧于自己度过的时光，以高效和有价值来作为回忆的温床<br>时间是我拥有的最锐利的武器，用之不善，则如将士钝了刀，任有一身武艺，却不得十步杀一人<br>而将士有宝刀，则杀敌事半功倍，在配之以一个明确的志向，自能达到理想之境  </p><p>知识是有限的，有限在于其毕竟为人类所创建之认知，总有办法可以解释<br>知识也是无限的，无限在于历代所积淀的经验和学识之深厚，岂是一朝一夕能掌握，况且惊才艳艳之辈数不胜数，知识大厦太厚太高    </p><p>所以作为一个普通人，在有限的时间里用正确的方法来高效掌握知识，需要看清知识的本质，以及人脑对知识的组织方式，才能将working的时间组织的有条理 </p><p>时间与精力都是有限的资源，如何根据自己所拥有的时间和精力，正确的组织和调度资源来完成任务    </p><p>然而悲惨的意识到，如果将学习的过程看作是机器的运转，那乐趣可能只在于高效本身了，所以兴趣来源于什么呢？所达成的成就的正向反馈吗？<br>成就的意义是什么，只自己而言，真的有那么在意一些事情的成败和得失吗，<strong>拥有</strong>本身能带来什么乐趣呢?   </p><p>创造价值，利人利己</p><h2 id="学科理论大厦的构成"><a href="#学科理论大厦的构成" class="headerlink" title="学科理论大厦的构成"></a>学科理论大厦的构成</h2><p>自古而今，一门学科的产生和延续，意味着一个有着精神意义或者生产力知识体系的产生<br>自然科学探究世界<br>工程科学创造工具<br>人文科学组织人类<br>任何一门学科理论的建立和延续必然有其独特的价值，</p><h2 id="知识的组织方式"><a href="#知识的组织方式" class="headerlink" title="知识的组织方式"></a>知识的组织方式</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 思考 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSAPP笔记</title>
      <link href="/posts/48790.html"/>
      <url>/posts/48790.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>qemu原理探究</title>
      <link href="/posts/29352.html"/>
      <url>/posts/29352.html</url>
      
        <content type="html"><![CDATA[<h2 id="虚拟化技术"><a href="#虚拟化技术" class="headerlink" title="虚拟化技术"></a>虚拟化技术</h2><p>几个疑问：</p><ul><li>为什么python,Java等要采用虚拟机来执行？</li><li>既然虚拟机最终要落在物理机的身上，为什么不去做和C一样的，针对编译器的适配，为什么不设计为编译执行的语言，而要加一个虚拟机中间层？</li></ul><p>python和java等解释性语言的执行：<br>源代码 - 字节码 - 解释器 - 虚拟机 - 物理机</p><p>python等易于开发的语言在设计时为了保证尽量减少底层差异对开发者的影响，对物理机做了一层虚拟机抽象，统筹和封装了硬件资源的操作，同时也隔离了python对底层硬件的操作</p><p>虚拟机跨平台的实现，是很多虚拟机是由C编写的，同时也考虑了对不同体系结构的优化和适配(提高字节码的执行效率)</p><p>C语言跨平台的实现：不同的编译器会针对特定平台和体系结构进行优化，生成不同的机器码</p><p>搭嘎，C也有虚拟机，如LLVM中的JIT(Just-in-Time)，和一些嵌入式中的实现</p><p>虚拟机化技术自1974年提出到现在，愈发常见和常用，其核心概念离不开Hypervisor(虚拟机监视器)，接下来深入探讨</p><h3 id="Hyervisor"><a href="#Hyervisor" class="headerlink" title="Hyervisor"></a>Hyervisor</h3><p>有两种方案，一种是直接对硬件层的Hypervisor，在此基础上建立Guest OS，代表作有：</p><ul><li>KVM</li><li>Virtual PC</li></ul><p>相当于Hypervisor就是主操作系统，特点是高效率</p><p>另一种是在主机的OS上运行一个Hypevisor，再在其上运行Guest OS，代表作有VMWare Workstation</p><p><img src="/posts/29352/figure.webp" alt></p><h4 id="KVM"><a href="#KVM" class="headerlink" title="KVM"></a>KVM</h4><p><img src="/posts/29352/figure2.png" alt></p><p>其中KVM是在支持VMX扩展模块的处理器中，能够使linux除了内核态，用户态意外多加一个客户态，客户态用来运行虚拟机，KVM可将linux内核转化为一个Hypervisor，客户态可以运行虚拟机，常见的是和QEMU搭配使用     </p><p>KVM设置了客户虚拟机的地址空间、模拟I/O和BIOS，而对显卡、网卡等抽象需要用户层的虚拟机来实现  </p><h4 id="QEMU"><a href="#QEMU" class="headerlink" title="QEMU"></a>QEMU</h4><p>QEMU是一个运行在用户态(或者说客户态)的虚拟机管理程序，本身提供了对许多硬件设备(如CPU)的抽象和模拟，但是性能有限，所以常和KVM搭配使用</p><p>特点是可以使用和模拟不同的指令集</p><p>用户模式可以使用不同指令集编译单个程序，也可以实现交叉编译<br>系统模式可以模拟一个完整的计算机系统</p><p>其处于GUEST和HOST之间</p><p>KVM提供CPU的虚拟机扩展，以及内核层次的虚拟机支持(将内核化为Hypervisor)，QEMU使用了KVM提供的特殊系统调用等接口来进行内核设置，所以KVM是QEMU的一种特殊运行模式，能直接用主机的CPU跑任务</p><p>其关系是:<br><strong>KVM通过QEMU来实现第一种的Hypervisor高性能支持，QEMU通过KVM来达到虚拟机的硬件加速</strong></p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>查看处理器是否支持KVM：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ LC_ALL=C lscpu | grep Virtualization</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>待办</title>
      <link href="/posts/7648.html"/>
      <url>/posts/7648.html</url>
      
        <content type="html"><![CDATA[<p>本学期待办</p><ul><li>SCU-OS社团成立(行政事项规划，社团定位以及成果产出规划)</li><li>内核设计赛(MIT 6.828课程，NJU-OS)</li><li>龙芯杯(到CPU五级流水设计，汇编，微体系架构)</li><li>CTF(Java，狂刷题)</li><li>恶意代码科研</li><li>Berkeley-CS61B</li><li>数模提前准备</li><li>大创-AI相关，答辩准备</li></ul><p>鼠了算了</p>]]></content>
      
      
      
        <tags>
            
            <tag> 待办 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo</title>
      <link href="/posts/37733.html"/>
      <url>/posts/37733.html</url>
      
        <content type="html"><![CDATA[<h2 id="Hexo-美化记录"><a href="#Hexo-美化记录" class="headerlink" title="Hexo 美化记录"></a>Hexo 美化记录</h2><p>页脚养鱼：<br>footer.styl<br><figure class="highlight styl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">background-color</span>: <span class="built_in">alpha</span>(<span class="variable">$dark</span>-black, .<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="selector-id">#footer-wrap</span></span><br><span class="line">   <span class="attribute">position</span>: absolute</span><br><span class="line">   <span class="attribute">padding</span>: <span class="number">1.2rem</span> <span class="number">1rem</span> <span class="number">1.4rem</span></span><br><span class="line">   <span class="attribute">color</span>: <span class="variable">$light</span>-grey</span><br><span class="line">   <span class="attribute">text-align</span>: center</span><br><span class="line">   <span class="attribute">left</span>: <span class="number">0</span></span><br><span class="line">   <span class="attribute">right</span>: <span class="number">0</span></span><br><span class="line">   <span class="attribute">top</span>:<span class="number">0</span></span><br><span class="line">   <span class="attribute">bottom</span>: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">   <span class="selector-id">#footer</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">hexo-config</span>(<span class="string">&#x27;footer_bg&#x27;</span>) != false</span><br><span class="line">       <span class="selector-pseudo">&amp;:before</span></span><br><span class="line">       <span class="attribute">position</span>: absolute</span><br><span class="line">       <span class="attribute">width</span>: <span class="number">100%</span></span><br><span class="line">       <span class="attribute">height</span>: <span class="number">100%</span></span><br><span class="line">       <span class="attribute">background-color</span>: <span class="built_in">alpha</span>(<span class="variable">$dark</span>-black, .<span class="number">1</span>)</span><br><span class="line">       <span class="attribute">content</span>: <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>js</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- <span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">- <span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.jsdelivr.net/gh/xiabo2/CDN@latest/fishes.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>固定宽度：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 鱼塘固定宽度 */</span></span><br><span class="line"><span class="selector-tag">canvas</span><span class="selector-pseudo">:not</span>(<span class="selector-id">#ribbon-canvas</span>), <span class="selector-id">#web_bg</span> &#123;</span><br><span class="line">    <span class="attribute">margin-bottom</span>: -<span class="number">0.5rem</span>;</span><br><span class="line">    <span class="attribute">display</span>: block;</span><br><span class="line">    <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">    <span class="attribute">height</span>: <span class="number">160px</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>开启懒加载：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-lazyload-image --save</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6.S081</title>
      <link href="/posts/53038.html"/>
      <url>/posts/53038.html</url>
      
        <content type="html"><![CDATA[<p><strong>开坑</strong><br><strong>课程</strong>：MIT 6.S081<br><strong>lab</strong>：xv6<br><strong>目标</strong>：掌握os，尽量手搓一个demo  </p><hr><h2 id="Chapter-0"><a href="#Chapter-0" class="headerlink" title="Chapter 0"></a>Chapter 0</h2><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>shell维护三个文件描述符，1，0，2</p><p>进程维护文件描述符表    </p><p>read系统调用会前进文件的偏移量，返回0作为文件结尾</p><p>2 &gt;&amp; 1:将标准错误流重定向到标准输出流</p><p>管道是一对读取和写入的文件描述符，shell使用fork来产生子进程，新的cmd来运行从管道的读描述符来获取数据</p><p>文件描述符偏移量相同的两种情况：dup调用和fork出子进程</p><p>close释放文件描述符</p><p>exec来重定向文件描述符，一个进程执行新进程时，替换掉的是进程的内存映像，但是维护的文件描述符表不会发生改变</p><p>若子进程不在退出前关闭文件描述符，父进程就会在子进程偏移的地方继续写，所以子进程的文件偏移后的文件描述符的位置会保留给父进程    </p><p>当进程试图访问设备文件时，内核会将对于设备文件的读写操作转发给相应的设备驱动程序或者内核代码，而不是用文件系统来操作</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> p[<span class="number">2</span>];</span><br><span class="line"><span class="type">char</span> *argv[<span class="number">2</span>];</span><br><span class="line">argv[<span class="number">0</span>] = <span class="string">&quot;wc&quot;</span>;</span><br><span class="line">argv[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">pipe(p);</span><br><span class="line"><span class="keyword">if</span>(fork() == <span class="number">0</span>) &#123;</span><br><span class="line">    close(<span class="number">0</span>);</span><br><span class="line">    dup(p[<span class="number">0</span>]);</span><br><span class="line">    close(p[<span class="number">0</span>]);</span><br><span class="line">    close(p[<span class="number">1</span>]);</span><br><span class="line">    exec(<span class="string">&quot;/bin/wc&quot;</span>, argv);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    write(p[<span class="number">1</span>], <span class="string">&quot;hello world\n&quot;</span>, <span class="number">12</span>);</span><br><span class="line">    close(p[<span class="number">0</span>]);</span><br><span class="line">    close(p[<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>notice：重定向是进程级别而不是系统级别<br>p[0]是读端口，读端口绑定到标准输入(通过dup(p[0]))，其他父子进程从标准输入读取数据时实际会从读端口读取   </p><h3 id="Lab1"><a href="#Lab1" class="headerlink" title="Lab1:"></a>Lab1:</h3><h4 id="父子进程间的管道通信"><a href="#父子进程间的管道通信" class="headerlink" title="父子进程间的管道通信"></a>父子进程间的管道通信</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/types.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/stat.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;user/user.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> *argv[])</span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &gt; <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;meaningless parameter\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> pid;</span><br><span class="line">    <span class="type">int</span> p1[<span class="number">2</span>];</span><br><span class="line">    <span class="type">int</span> p2[<span class="number">2</span>];</span><br><span class="line">    <span class="type">char</span> byte;</span><br><span class="line">    pipe(p1);</span><br><span class="line">    pipe(p2);</span><br><span class="line">    </span><br><span class="line">    pid = fork(); <span class="comment">//fork复制之后，子进程会复制所有管道，所以父进程中用不到的就可以关闭了</span></span><br><span class="line">    <span class="keyword">if</span> (pid &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        close(p1[<span class="number">0</span>]);</span><br><span class="line">        close(p2[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span>(write(p1[<span class="number">1</span>],<span class="string">&quot;1&quot;</span>,<span class="number">1</span>) != <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;error parent send\n&quot;</span>);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (read(p2[<span class="number">0</span>], &amp;byte, <span class="number">1</span>) == <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d:recieve pong\n&quot;</span>, getpid());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;error parent recieve\n&quot;</span>);</span><br><span class="line">            close(p1[<span class="number">0</span>]);</span><br><span class="line">            close(p2[<span class="number">1</span>]);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        close(p1[<span class="number">0</span>]);</span><br><span class="line">        close(p2[<span class="number">1</span>]);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (pid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        close(p1[<span class="number">1</span>]);</span><br><span class="line">        close(p2[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">if</span>(read(p1[<span class="number">0</span>],&amp;byte,<span class="number">1</span>) == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d:recieved ping\n&quot;</span>, getpid());</span><br><span class="line">            <span class="keyword">if</span>(write(p2[<span class="number">1</span>], <span class="string">&quot;1&quot;</span>, <span class="number">1</span>) != <span class="number">1</span>)&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;error child send\n&quot;</span>);</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;error child recieve\n&quot;</span>);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;fork error&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>notice：父子进程不同步，无法保证父进程发送数据时，子进程已经准备好接收数据，出现<strong>条件竞争</strong>，所以要使用同步机制来保证</p><h4 id="管道实现素数筛"><a href="#管道实现素数筛" class="headerlink" title="管道实现素数筛"></a>管道实现素数筛</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/types.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/stat.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;user/user.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INDEX_READ 0</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INDEX_WRITE 1</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX 35</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 思路：对于数字的输出和处理都在子进程中完成，关闭不需要的读端口，直到所有端口全部关闭，前后有固定关系，考虑递归</span></span><br><span class="line"><span class="comment">// 父进程的行为和子进程对孙进程的不同</span></span><br><span class="line"><span class="comment">// 管道是一个有缓冲的队列！！！！</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">child_read</span><span class="params">(<span class="type">int</span> pipelines_ptc[])</span>&#123;</span><br><span class="line">    <span class="type">int</span> pid;</span><br><span class="line">    <span class="type">int</span> pipelines_ctgc[<span class="number">2</span>];</span><br><span class="line">    <span class="type">int</span> num;</span><br><span class="line">    pipe(pipelines_ctgc);</span><br><span class="line">    close(pipelines_ptc[INDEX_WRITE]);</span><br><span class="line">    <span class="comment">// 打印初始值，并丢掉</span></span><br><span class="line">    <span class="keyword">if</span> (read(pipelines_ptc[INDEX_READ], &amp;num, <span class="keyword">sizeof</span>(num)) &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;prime %d\n&quot;</span>, num);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        close(pipelines_ctgc[INDEX_READ]);</span><br><span class="line">        close(pipelines_ctgc[INDEX_WRITE]);</span><br><span class="line">        close(pipelines_ptc[INDEX_READ]);</span><br><span class="line">        close(pipelines_ptc[INDEX_WRITE]);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 子进程与孙进程</span></span><br><span class="line">    <span class="keyword">if</span> ((pid = fork()) &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;child process\n&quot;</span>);</span><br><span class="line">        <span class="comment">// 子进程</span></span><br><span class="line">        close(pipelines_ctgc[INDEX_READ]);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;close success!\n&quot;</span>);</span><br><span class="line">        <span class="type">int</span> each_num;</span><br><span class="line">        <span class="keyword">while</span> (read(pipelines_ptc[INDEX_READ], &amp;each_num, <span class="keyword">sizeof</span>(each_num)) &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Read success!\n&quot;</span>);</span><br><span class="line">            <span class="keyword">if</span> (each_num % num != <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                write(pipelines_ctgc[INDEX_WRITE], &amp;each_num, <span class="keyword">sizeof</span>(each_num));</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;write success!\n&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        close(pipelines_ctgc[INDEX_WRITE]);</span><br><span class="line">        close(pipelines_ptc[INDEX_READ]);</span><br><span class="line">        wait(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="comment">// 孙进程</span></span><br><span class="line">        child_read(pipelines_ctgc);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span>&#123;</span><br><span class="line">    <span class="type">int</span> pipelines_ptc[<span class="number">2</span>];</span><br><span class="line">    <span class="type">int</span> pid;</span><br><span class="line">    pipe(pipelines_ptc);</span><br><span class="line">    <span class="keyword">if</span> ((pid = fork()) &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//父进程</span></span><br><span class="line">        close(pipelines_ptc[INDEX_READ]);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt; MAX; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            write(pipelines_ptc[INDEX_WRITE], &amp;i, <span class="keyword">sizeof</span>(i));</span><br><span class="line">            wait(<span class="number">0</span>);        <span class="comment">//等待子进程完成</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="comment">// 子进程</span></span><br><span class="line">        child_read(pipelines_ptc);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="find-命令实现"><a href="#find-命令实现" class="headerlink" title="find 命令实现"></a>find 命令实现</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/types.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/stat.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;user/user.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/fs.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 递归遍历所有目录来寻找文件并打印，判断是否有文件夹，没有返回</span></span><br><span class="line"><span class="comment">// 接收文件名和寻找开始的路径</span></span><br><span class="line"><span class="comment">// 获取路径字符串的文件名</span></span><br><span class="line"><span class="type">char</span>*</span><br><span class="line"><span class="title function_">fmtname</span><span class="params">(<span class="type">char</span> *path)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">char</span> buf[DIRSIZ+<span class="number">1</span>];</span><br><span class="line">    <span class="type">char</span> *p;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Find first character after last slash.</span></span><br><span class="line">    <span class="keyword">for</span>(p=path+<span class="built_in">strlen</span>(path); p &gt;= path &amp;&amp; *p != <span class="string">&#x27;/&#x27;</span>; p--)</span><br><span class="line">      ;</span><br><span class="line">    p++;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Return blank-padded name.</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">strlen</span>(p) &gt;= DIRSIZ)</span><br><span class="line">      <span class="keyword">return</span> p;</span><br><span class="line">    memmove(buf, p, <span class="built_in">strlen</span>(p));</span><br><span class="line">    <span class="built_in">memset</span>(buf+<span class="built_in">strlen</span>(p), <span class="string">&#x27; &#x27;</span>, DIRSIZ-<span class="built_in">strlen</span>(p));</span><br><span class="line">    <span class="keyword">return</span> buf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">find</span><span class="params">(<span class="type">char</span> *path, <span class="type">char</span> *name)</span>&#123;</span><br><span class="line">    <span class="type">char</span> buf[<span class="number">512</span>];</span><br><span class="line">    <span class="type">char</span> *p;</span><br><span class="line">    <span class="type">int</span> fd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dirent</span> <span class="title">de</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">stat</span> <span class="title">st</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>((fd = open(path, <span class="number">0</span>)) &lt; <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">&quot;find: cannot open %s\n&quot;</span>, path);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(fstat(fd, &amp;st) &lt; <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="number">2</span>, <span class="string">&quot;ls: cannot stat %s\n&quot;</span>, path);</span><br><span class="line">    close(fd);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span>(st.type)&#123;</span><br><span class="line">        <span class="comment">// 寻找的目录是文件</span></span><br><span class="line">        <span class="keyword">case</span> T_FILE:</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">strcmp</span>(fmtname(path), name) == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, path);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> T_DIR:</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">strlen</span>(path) + <span class="number">1</span> + DIRSIZ + <span class="number">1</span> &gt; <span class="keyword">sizeof</span> buf)&#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;find: path too long\n&quot;</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">strcpy</span>(buf, path);</span><br><span class="line">            p = buf+<span class="built_in">strlen</span>(buf);</span><br><span class="line">            *p++ = <span class="string">&#x27;/&#x27;</span>;</span><br><span class="line">            <span class="keyword">while</span> (read(fd, &amp;de, <span class="keyword">sizeof</span>(de)) == <span class="keyword">sizeof</span>(de))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (de.inum == <span class="number">0</span> || !<span class="built_in">strcmp</span>(de.name, <span class="string">&quot;.&quot;</span>) || !<span class="built_in">strcmp</span>(de.name, <span class="string">&quot;..&quot;</span>))</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">                memmove(p, de.name, DIRSIZ);</span><br><span class="line">                p[DIRSIZ] = <span class="number">0</span>; <span class="comment">// ?</span></span><br><span class="line">                <span class="comment">// 目录当文件处理</span></span><br><span class="line">                <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(de.name, name))</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>, buf);</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(stat(buf, &amp;st) &lt; <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">&quot;find: cannot stat %s\n&quot;</span>, buf);</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 不用管文件的情况</span></span><br><span class="line">                <span class="keyword">switch</span> (st.type)</span><br><span class="line">                &#123;</span><br><span class="line">                <span class="keyword">case</span> T_DIR:</span><br><span class="line">                    find(buf, name);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">default</span>:</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            close(fd);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    close(fd);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Usage: &#x27;find dir filename&#x27;\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">2</span>; i &lt; argc; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        find(argv[<span class="number">1</span>], argv[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="xargs-命令实现"><a href="#xargs-命令实现" class="headerlink" title="xargs 命令实现"></a>xargs 命令实现</h4><ul><li><p>对于管道连接的命令，以<code>find . b | xargs grep hello</code>为例，argv[0]是 xargs，argv[1]是grep，一共有三个参数，如果想要读取<code>find . b</code>，那么需要从标准输入中读取，也即是使用read函数读取fd为0时的数据</p></li><li><p>每一次读取一行，将该行所有空格替换为\0，这样命令就可以被分割。然后将argv[]指向这些命令。如果遇到换行符，执行fork，父进程等待子进程结束</p></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/types.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/stat.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;user/user.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/fs.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel/param.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> STDIN 0</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> *argv[])</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">char</span> buf[<span class="number">1024</span>];</span><br><span class="line">    <span class="type">char</span> c;</span><br><span class="line">    <span class="type">char</span> *Argv[MAXARG];</span><br><span class="line">    <span class="type">int</span> index=<span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=argc<span class="number">-1</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        Argv[i<span class="number">-1</span>]=argv[i];<span class="comment">//ignore xargs(argv[0])</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        index=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">memset</span>(buf,<span class="number">0</span>,<span class="keyword">sizeof</span>(buf));</span><br><span class="line">        <span class="type">char</span> *p=buf;</span><br><span class="line">        i=argc<span class="number">-1</span>;<span class="comment">//注意i要写在这里</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> num=read(STDIN,&amp;c,<span class="number">1</span>);<span class="comment">//读取标准输入,注意是&amp;c</span></span><br><span class="line">            <span class="keyword">if</span>(num!=<span class="number">1</span>)</span><br><span class="line">                <span class="built_in">exit</span>(<span class="number">0</span>);<span class="comment">//程序的终止条件</span></span><br><span class="line">            <span class="keyword">if</span>(c==<span class="string">&#x27; &#x27;</span>||c==<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                buf[index++]=<span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">                Argv[i++]=p;<span class="comment">//参数</span></span><br><span class="line">                p=&amp;buf[index];<span class="comment">//更新参数首地址</span></span><br><span class="line">                <span class="keyword">if</span>(c==<span class="string">&#x27;\n&#x27;</span>) </span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="comment">//character </span></span><br><span class="line">            &#123;</span><br><span class="line">                buf[index++]=c;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        Argv[i]=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> pid = fork();</span><br><span class="line">        <span class="keyword">if</span>(pid==<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            exec(Argv[<span class="number">0</span>],Argv);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            wait(<span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="无缓冲通道CSP线程"><a href="#无缓冲通道CSP线程" class="headerlink" title="无缓冲通道CSP线程"></a>无缓冲通道CSP线程</h3><p>CSP并发编程是一种进程间的通信方式，不需要两个进程共享状态，而是通过消息传递的方式来进行通信，可以解决死锁和条件竞争的问题，其中使用管道(pipeline)的无缓冲I/O是一种常见的来实现同步的方法：<br><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;time&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">worker</span><span class="params">(ch <span class="keyword">chan</span> <span class="type">bool</span>)</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">&quot;Worker: Waiting for task...&quot;</span>)</span><br><span class="line">    &lt;-ch <span class="comment">// 从通道接收数据，这里会阻塞直到数据发送方准备好</span></span><br><span class="line">    fmt.Println(<span class="string">&quot;Worker: Performing task...&quot;</span>)</span><br><span class="line">    time.Sleep(<span class="number">2</span> * time.Second) <span class="comment">// 模拟工作</span></span><br><span class="line">    fmt.Println(<span class="string">&quot;Worker: Task completed!&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">bool</span>) <span class="comment">// 创建一个无缓冲通道</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> worker(ch) <span class="comment">// 启动一个goroutine执行worker函数</span></span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">1</span> * time.Second) <span class="comment">// 等待一会儿</span></span><br><span class="line"></span><br><span class="line">    fmt.Println(<span class="string">&quot;Main: Sending task...&quot;</span>)</span><br><span class="line">    ch &lt;- <span class="literal">true</span> <span class="comment">// 向通道发送数据，这里会阻塞直到接收方准备好</span></span><br><span class="line">    fmt.Println(<span class="string">&quot;Main: Task sent!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    time.Sleep(<span class="number">3</span> * time.Second) <span class="comment">// 等待工作完成</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h2 id="Chapter-1"><a href="#Chapter-1" class="headerlink" title="Chapter 1"></a>Chapter 1</h2><h3 id="Note-1"><a href="#Note-1" class="headerlink" title="Note"></a>Note</h3><p>将操作系统作为库调用会带来一系列问题<br>操作系统将磁盘管理抽象成文件系统，并且提供了对文件系统的接口(系统调用)，来实现对应用程序访问存储的隔离<br>隔离的必要性：物理资源组织，异常处理和安全性<br>隔离方案：multiplexing，CPU分时复用<br>fork等操作系统接口抽象出可以和CPU交互的进程，进程在和应用程序交互，所以应用程序才能复用CPU</p><p>为什么虚拟地址能保证应用程序不直接访问物理地址的安全性？</p><ul><li>虚拟地址提供了对物理地址的抽象和扩展，便于操作系统对进程进行隔离</li><li>虚拟地址可以设置一些针对物理地址的权限，并且内存管理单元MMU(用于根据页表将虚拟地址映射为虚拟地址)也可以对访问进行拦截和保护保证</li></ul><p>exec系统调用是unix接口，将应用程序的内存镜像放到进程内存里</p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux_base</title>
      <link href="/posts/1f6200b9.html"/>
      <url>/posts/1f6200b9.html</url>
      
        <content type="html"><![CDATA[<p>深重悼念，<a href="http://www.cnblogs.com/vamei">Vamei</a></p><p>斯人已逝，文章永存</p><h3 id="Linux架构"><a href="#Linux架构" class="headerlink" title="Linux架构"></a>Linux架构</h3><p>开机时，计算机从主板BIOS(basic input/output system)中读出程序，该程序的作用是使CPU对各个硬件连接识别，然后指向启动计算机的硬件位置，可以选择启动计算机的位置</p><p>之后从选定的位置读取前512个字节，称为主引导记录MBR，MBR再从指定的分区加载引导加载程序(boot loader),引导加载程序加载操作系统内核(kernel)</p><p>内核直接管理硬件，内核之上是系统调用</p><p>内核通过驱动检测硬件以后，创建一个init进程，init运行一系列初始脚本，进行准备工作，对计算机进行一系列的初始化，之后弹出登录框，允许用户以某个组的某个用户登录</p><p>linux架构：</p><p><img src="/posts/1f6200b9/linux_base_01.jpg" alt="Linux架构"></p><p>如图，系统调用是对内和的进一步抽象，系统调用也是操作系统的原子操作，系统调用之上的封装是库函数</p><p>用<code>man 2 syscall</code>可以查看所有系统调用，也可以查看具体的系统调用说明，如<code>man 2 read</code><br>(其中2代表系统调用类，具体的数字和对应的类可以用<code>man man</code>查看)</p><p>shell是一个程序，有内置的函数以及可以运行可执行文件(包括命令)，默认的shell是bash，查看用<code>echo $SHELL</code>，还有其他种类的shell<br>shell也是可编程的，shell脚本<br>shell最大的作用是高效实现各个独立程序之间的协同</p><hr><h3 id="Linux进程"><a href="#Linux进程" class="headerlink" title="Linux进程"></a>Linux进程</h3><p>程序是指令的集合<br>进程是程序的执行过程，为程序执行开辟空间等<br>操作系统管理的是进程</p><p>可以用<code>ps</code>命令查看进程</p><p>linux的进程其实为初始init进程对自己不断调用fork复制的结果<br>所以进程结构为以init进程为树根的树状结构，使用<code>pstree</code>查看</p><p>子进程终结以后，退出信息会存到内核中，父进程会从内核中wait子进程，调出退出信息</p><p>如果父进程没有wait，成为孤儿进程(orphand)，父进程成为init</p><hr><h3 id="Linux信号"><a href="#Linux信号" class="headerlink" title="Linux信号"></a>Linux信号</h3><p>Linux进程之间相互独立，信号是一种进程之间相互传递信息的方式</p><p>信号由内核或者其他进程产生，可以对信号进行执行(deliver)或者阻塞(block)</p><p>内核中对每个进程存储有一个<strong>表</strong>，当传递某个信号，将信号加到表中，进程在执行完系统调用后会查看表，接收信号，可以用<code>man 7 signal</code>查看信号</p><p>signal.signal(signalnum, handler)函数，第一个参数为信号宏(如SIGINT),也可以为数字(具体看<code>man 7 siganl</code>);第二个参数为处理方法(可以自定义对象或者函数)</p><p>如：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">import</span> signal, time</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handler</span>(<span class="params">signum, time</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nI got aSIGINT, but I am notstopping&quot;</span>)</span><br><span class="line">signal.signal(signal.SIGINT, handler)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    time.sleep(<span class="number">.1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\r&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i), end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>(/r用处是光标回到行首)</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bash/bin/env</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">n=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="built_in">print</span>(n)</span><br><span class="line">    time.sleep(<span class="number">.3</span>)</span><br><span class="line">    n=n+<span class="number">1</span></span><br></pre></td></tr></table></figure><p>bg运行以后无法中断，是因为中断在while循环里运行，所以只能kill终止或者挂起</p><p>常见信号：<br>Ctrl+C      SIGINT<br>Ctrl+\      SIGQUIT<br>Ctrl+Z      SIGSTOP<br>kill -TERM \<pid>       SIGTERM(终止)</pid></p><p>信号处理(signal disposition)有三种方式：</p><ol><li>默认(default)    SIG_DFT</li><li>忽略(ignore)     SIG_IGN</li><li>自定义处理</li></ol><p>恢复暂停使用<code>fg</code>或<code>bg</code>命令，参数为$!代表上一条命令，也可以使用%n，n为jobs里查看的任务编号</p><p><code>jobs</code>会列出当前终端中未完成的任务，可以使用<code>pgrep</code>查找pid</p><p>后缀加一个&amp;可以让其在后台运行(但输出仍然在终端标准输出)</p><p>nohup可以让程序忽略SIGHUP信号，因为默认shell中运行的程序都是shell的子进程，所以关闭shell以后程序会默认终止，如：<br><code>nohup sleep 2000 &amp;</code></p><h4 id="远端连接-SSH"><a href="#远端连接-SSH" class="headerlink" title="远端连接(SSH)"></a>远端连接(SSH)</h4><p>命令：<br><code>ssh foo@bar.mit.edu</code></p><p>其中foo是用户名，@后是服务器，服务器可以通过URL或者IP指定</p><p>ssh可以直接远程执行命令，如：<br><code>ssh foo@server ls | grep pattern</code>是在本地查询远端ls命令<br>而<code>ls | ssh foo@server grep pattern</code>是在远端对本地ls进行查询</p><p>ssh密钥：</p><p>首先了解一下CA证书：</p><p><strong>CA证书</strong>指的是CA机构为每个合法公钥持有者办法的一个数字签名证书，证书内含有分配的公钥，一个用户将验证另一个用户的真伪，可以用CA公钥对那个证书上的签字进行验证<br>CA将CA为其分配的公钥和申请者的信息绑在一起，并为他形成签字</p><p>其次了解一下对称加密和非对称加密(RSA)</p><p>对称加密是指client端和server端加密解密用的是同一个密钥<br><img src="/posts/1f6200b9/linux_base_02.png" alt="client"><br><img src="/posts/1f6200b9/linux_base_03.png" alt="server"></p><p>server对每个client分配密钥，但是容易泄露</p><p>所以用非对称加密，使用一对公钥和私钥，server端有公钥和私钥，client访问时，server将公钥给client进行加密，密文传给server以后，用私钥进行解密，再进行验证<br><img src="/posts/1f6200b9/linux_base_04.png" alt="RSA"></p><p>但仍有被发送攻击者的公钥的风险(中间人攻击)，所以要根据公钥指纹认证公钥</p><p>有一种免登录的ssh方法：<strong>公钥登录</strong><br>将client的公钥记录到server上，具体过程为登录的时候server生成随机数R，公钥加密，传给client，client私钥解密，比较是否相等</p><p>私钥位置：<code>~/.ssh/id_rsa</code>(慎重保管！)</p><p>生成一对密钥：<br><code>ssh-keygen -o -a 100 -t ed25519 -f ~/.ssh/id_ed25519</code></p><p>ssh会查询.ssh/authorized_keys来确认哪些用户允许登录<br>拷贝其他server的认证公钥：<br><code>cat .ssh/id_ed25519 | ssh foo@remote &#39;cat &gt;&gt; ~/.ssh/authorized_keys&#39;</code></p><p>ssh复制文件:<br><code>scp path/to/local_file remote_host:path/to/remote_file</code></p><p>监听远程服务器需要<strong>端口转发</strong></p><hr><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><hr><h4 id="fork-函数"><a href="#fork-函数" class="headerlink" title="fork()函数"></a>fork()函数</h4><p>fork()函数用于创建进程，有两个返回值，在子进程中返回0，在父进程中返回子进程PID<br>fork函数创建子进程的方式为：</p><ol><li>申请PID</li><li>申请PCB</li><li>复制父进程PCB</li><li>初始化子进程</li><li>复制父进程的页(使用写时拷贝，写时拷贝指对只读区域在修改的时候要拷贝一份才可以做修改)</li></ol><p>fork函数执行的时候，一个函数会在其创建的子进程和父进程中各执行以此，但父进程和子进程那个先执行不一定，所以</p><hr><h4 id="堆-OS"><a href="#堆-OS" class="headerlink" title="堆(OS)"></a>堆(OS)</h4><p>操作系统中的堆指的是一段在程序运行的时候申请的内存空间，不同于栈，栈是一级缓存，用来存放函数参数名以及信息等，堆是二级缓存，程序运行结束的时候释放，程序员可以用malloc等函数申请<br>注意堆是在程序开始运行的时候就根据库函数申请了一段内存，OS存储堆的大小和初始地址，free()释放地址，程序员申请堆其实是在已经申请好的堆中获取存储大小，可以加速效率</p><h4 id="format-函数"><a href="#format-函数" class="headerlink" title="format()函数"></a>format()函数</h4><p>用于格式化字符串</p><ol><li><code>print(&quot;&#123;&#125;&#123;&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;))</code></li><li><code>site = &#123;&quot;name&quot;:&quot;1&quot;, &quot;url&quot;:&quot;www.marvoalou.github.io&quot;&#125;</code><br><code>print(&quot;&#123;name&#125;,&#123;url&#125;&quot;.format(**site))</code></li><li><code>list=[&#39;zxy&#39;,&#39;man&#39;]</code><br><code>print(&quot;&#123;0[0]&#125;&#123;0[1]&#125;&quot;.format(list))</code></li><li><code>print(&quot;网站名：&#123;name&#125;, 地址 &#123;url&#125;&quot;.format(name=&quot;菜鸟教程&quot;, url=&quot;www.runoob.com&quot;))</code></li><li><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AssignValue</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value</span>):</span><br><span class="line">    self.value = value</span><br><span class="line">my_value = AssignValue(<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;value 为: &#123;0.value&#125;&#x27;</span>.<span class="built_in">format</span>(my_value))  <span class="comment"># &quot;0&quot; 是可选的</span></span><br></pre></td></tr></table></figure></li><li><p>格式化：具体查询</p></li></ol><h4 id="Tmux"><a href="#Tmux" class="headerlink" title="Tmux"></a>Tmux</h4><p>功能：</p><ol><li><p>它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用</p></li><li><p>它可以让新窗口”接入”已经存在的会话</p></li><li><p>它允许每个会话有多个连接窗口，因此可以多人实时共享会话</p></li><li><p>它还支持窗口任意的垂直和水平拆分</p></li></ol><p>帮助：      <code>Ctrl+b ?</code></p><p>新建会话:       <code>tmux new -s &lt;session-name&gt;</code></p><p>分离会话：      <code>Ctrl+b d</code></p><p>查看所有会话：        <code>tmux ls</code>or<code>Ctrl+b s</code></p><p>接入会话：      <code>tmux attach -t &lt;num&gt;or&lt;session-name&gt;</code></p><p>结束会话：      <code>tmux kill-session -t &lt;name&gt;</code></p><p>切换：      <code>tmux switch -t &lt;name&gt;</code></p><p>重命名：        <code>tmux rename-session -t 0 &lt;new-name&gt;</code></p><p>划分窗格：      <code>tmux split-window &lt;-h&gt;(左右)</code>or<code>Ctrl+b %</code>or<code>Ctrl+b &quot;</code></p><p>移动光标：      <code>tmux select-pane -U/-D/-L/-R</code>or<code>Ctrl+b 方向键</code></p><p>交换位置：      <code>tmux swap-pane -U/-D</code></p><p>其余具体看教程</p><h4 id="alias"><a href="#alias" class="headerlink" title="alias"></a>alias</h4><p><code>alias ll=&quot;ls -lh&quot;</code></p><p>忽略别名：<br><code>\ls</code></p><p>禁用别名：<br><code>unalias ll</code></p><p>获取别名定义：<br><code>alias ll</code></p><p>别名放在.zshrc配置文件里</p><p>配置文件是隐藏文件，也称为点文件</p><p>常见的配置文件;<br><code>bash</code> - <code>~/.bashrc</code><br><code>git</code> - <code>~/.gitconfig</code><br><code>vim</code> - <code>~/.vimrc</code><br><code>ssh</code> - <code>~/.ssh/config</code><br><code>tmux</code> - <code>~/.tmux.conf</code></p><p>配置文件支持shell脚本，所以想要配置文件实现一些具体的功能(如在不同设备使用不同配置)，可以添加shell脚本</p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ml</title>
      <link href="/posts/d58e2a66.html"/>
      <url>/posts/d58e2a66.html</url>
      
        <content type="html"><![CDATA[<p><strong><code>2023</code>年<code>11</code>月<code>8</code>日学习<code>ML</code>，共耗时：<code>6</code>小时</strong><br><strong>学习资料：<a href="http://www.ai-start.com/ml2014/html/">machine learning</a></strong></p><blockquote><p>正视自己现在的态度：想做建大的事情并且快速获得可以擦后果别人的成果，想什么呢？<br>只有掌握技术，才能去享受技术，就像只有会游泳才能享受游泳的乐趣，如果中途半途而废了，那么之前做的一切都会没有价值<br>既然选择了走这么一条艰难地路，顶一个宏大的目标，那就坚持下去</p><h2 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h2><p>监督学习：回归问题或者对于已知种类的分类判断问题<br>无监督学习：对于未知种类的自动判断种类(聚类算法)    </p></blockquote><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>特征相当于变量，不同问题的特征值是不一样的，机器学习是对于某一种具有相同特征值的问题进行处理的方法<br>梯度下降对于参数求导数，来判断参数往哪个方向调整会产生预期结果</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>最简单的对于一元自变量：<br>多个数据来拟合一条直线，来进行预测或者分类，对于离散问题也可以使用线性回归：    </p><script type="math/tex; mode=display">y = \beta_0 + \beta_1 x_i</script><p>残差：  </p><script type="math/tex; mode=display">e = y - \widehat{y}</script><p>损失函数：残差平方和<br>可以使用最小二乘法来求解拟合直线参数 $\beta_0$ 和 $\beta_1$ :<br>对于残差平方和$Q,\beta_0,\beta_1$，图像是三维凸函数，所以极小值是对$\beta_0.\beta_1$求偏导为0的点   </p><script type="math/tex; mode=display">\frac{\partial Q}{\partial\beta_0} = 2\sum_{1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i) = 0</script><script type="math/tex; mode=display">\frac{\partial Q}{\partial\beta_1} = 2\sum_{1}^n(y_i - \widehat{\beta_0} - \widehat{\beta_1}x_i)x_i = 0</script><p>（多维的话就对多个进行偏导求解）<br>可以用来求解局部最小值或者全局最小值<br>拟合的时候不一定使用线性回归，也可能使用其他函数的回归，看对于数据点那个比较适用  </p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>思想是：<br>对于一个预测问题或者分类问题需要去数学建模来构造预测函数，于是建模有函数 $h(\Theta^TX)$ 作为预测假设<br>接下来需要改良假设的拟合效果，所以要通过学习来更新矩阵 $\Theta$<br>评判标准：代价函数 <script type="math/tex">J(\theta_1,\theta_2,\dots,\theta_n)</script> 代表了当前选取的参数的建模误差<br>为了要代价函数最小，所以根据代价函数的值来更新 <script type="math/tex">\theta_i</script><br>于是有对 $J$ 对 $\theta_i$ 求偏导的方法来更新 $\theta_i$ 值  </p><p>批量梯度下降(因为每一步用到了所有的数据点)：<br>选取随机的参数组合 <script type="math/tex">(\theta_0,\theta_1\dots\theta_n)</script>,来对每个参数组合求解局部最小值，来寻找全局最小值  </p><script type="math/tex; mode=display">\theta_j=\theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta_0, \theta-1)</script><p>其中 $\alpha$ 称为学习率，也看作步长，$\theta_0和\theta_1会及时更新$</p><p>(为什么要使用梯度下降：不是所有函数都能求偏导，函数也可能是黑盒，并且偏导等于0的点不一定是极值点) </p><h3 id="多维梯度下降"><a href="#多维梯度下降" class="headerlink" title="多维梯度下降"></a>多维梯度下降</h3><p>判断函数： <script type="math/tex">h_\theta(x) = \theta^T X</script><br>梯度下降函数： <script type="math/tex">\theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m((h/-\theta(x^{(i)}) - y^{(i)})x_j^{(i)})</script><br>注意多项式不一定是线性的，有可能是高次的，但是参数是线性的，不影响对于参数的梯度下降<br>选取正确的模型方程很重要</p><h3 id="梯度下降优化"><a href="#梯度下降优化" class="headerlink" title="梯度下降优化"></a>梯度下降优化</h3><p>统一特征尺度，为了准确度，需要将特征尺度统一到(0,1)的区间：  </p><script type="math/tex; mode=display">x_n = \frac{x_n - \mu_n}{s_n}</script><p>其中 $\mu_n,s_n$为平均值和标准差</p><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>对于<strong>线性回归问题</strong><br>当特征值少的时候，可以直接用正规方程的方法来得到最合适的参数矩阵：  </p><script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>直接可以求得最小代价函数的参数矩阵<br>对于小样本数但是需要多个参数的时候，使用正则化来减少特征值个数</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>线性回归拟合+预测值用sigmod函数来进行分类 </p><p>决策边界：<br>来判断分类种类的函数，所以对于比较复杂的分类种类，通常需要选取复杂的决策边界函数 $\theta^TX$ 才行   </p><p>对于决策边界$w_1 x_1+w_2 x_2 + b = 0$，有决策函数$g(w_1 x_1+w_2 x_2 + b)$<br>即$w_1 x_1+w_2 x_2 + b &gt; 0$为类别$Y = 1$，小于0则是另一类，用来表示$P(Y = 1 \mid x)$<br>其中x表示x向量$(x_1 , x_2)$，用$W^T$表示向量$(w_1, w_2)$,<br>定义对数几率 $\ln{\frac{y}{1-y}} = w_T x + b$ 与x成线性关系，其中y是为1的概率<br>所以逻辑回归要先拟合出决策边界，在将这个边界和分类概率建立联系<br>逻辑回归也需要使用特征缩放  </p><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>对于逻辑回归，假设函数给出的是离散值，所以需要sigmod函数来把连续值的输出离散化：</p><script type="math/tex; mode=display">h_\theta(x) = g(\Theta^TX)</script><script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}</script><p>如果按原来的方法构建代价函数的话，会产生一个非凸函数，会陷入局部最小值里<br>所以构建误差函数：  </p><script type="math/tex; mode=display">Cost(h_\theta(x),y) = -y \times \ln(h_\theta(x)) - (1-y)\times\ln(1-h_\theta(x))</script><p>然后求和求偏导得到代价函数 $J(\theta)$</p><h3 id="更优化的学习方法"><a href="#更优化的学习方法" class="headerlink" title="更优化的学习方法"></a>更优化的学习方法</h3><p>共轭梯度法 BFGS (变尺度法) 和L-BFGS (限制变尺度法)<br>有智能的内部循环（线性搜索）来自动选取学习率    </p><h3 id="一对多分类"><a href="#一对多分类" class="headerlink" title="一对多分类"></a>一对多分类</h3><p>对于有多个种类的分类方法，计算每一个类别的概率函数<br>将其中一个类看做正向类，其余看做负向类<br>有取 $\max(h_\theta^{(i)}(x))$</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>为了防止过拟合的问题，使用正则化来扩大某个参数的影响，以减小这个参数的值，做到减小这个参数影响力的效果<br>正则化线性回归的代价函数为：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}[((h_\theta(x^i)-y^i)^2 + \lambda\sum_{j=1}^{n}\theta_j^2)]</script><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>当特征变量多了以后，不同次方的组合数太多了，比如对图片的像素特征处理，所以要用到神经网络<br>神经网络通过多层权重(参数)矩阵能进行十分庞大的种类变换，而且变换是传播扩散的，也可以将特征变量通过权重矩阵扩展到多维，来产生任意维的输出<br>g(x)表示sigmod函数</p><p>每一次层的参数矩阵对上一层输出的特征值或者中间值进行处理，来生成下一层的输入</p><p>可以有多个结果的输出值，即为多分类问题，也可以改变偏差单位(bias unit)来进行逻辑运算操作，从左到右的网络称之为向前传播算法，多分类问题的输出变量是一个向量   </p><p>$s<em>l$代表l层，$\theta_l$代表l层到l+1层的参数矩阵，其大小为 $s</em>{l+1} \times s_l + 1$，所以神经网络的代价函数是多余每一个分类的代价函数的和和正则化每一个参数的平方和 </p><h3 id="反向传播梯度优化"><a href="#反向传播梯度优化" class="headerlink" title="反向传播梯度优化"></a>反向传播梯度优化</h3><p>为了计算对于某一层的某一个参数的对于代价函数 $J(\theta)$的偏导数，需要用到反向传播的方法来求<br>首先由正向传播计算得到神经网络的预测值，然后从后往前有：</p><script type="math/tex; mode=display">\sigma^{(l)} = (\Theta^{(l)})^T\sigma^{(l+1)}*g'(z^{(l)})</script><script type="math/tex; mode=display">\frac{\partial}{\partial\Theta_{ij}^{(l)}} J(Ttheta) = \alpha_j^{(l)}\sigma_i^{(l+1)}</script><p>表示第l层的第j个激活单元收到了第l层的第i行参数的影响而产生的误差单元<br>如果特征值是一个特征矩阵，那么会产生一个误差矩阵 $\triangle_{ij}^l$</p><h3 id="神经网络的训练步骤"><a href="#神经网络的训练步骤" class="headerlink" title="神经网络的训练步骤"></a>神经网络的训练步骤</h3><ul><li>参数随机初始化</li><li>正向传播计算 $h_\theta(x)$</li><li>计算代价函数</li><li>反向传播计算偏导</li><li>数值检验校验偏导</li><li>优化算法最小化代价函数</li></ul><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>基本的减小误差的思路：</p><ul><li>使用更多样本</li><li>减少特征数量</li><li>增加特征数量</li><li>增加多项式复杂度</li><li>减少正则化程度</li><li>增加正则化程度</li></ul><p>区部分数据集作为训练集、部分数据集作为测试集</p><p>进一步，可以使用交叉验证(cross validation error)来选取一部分数据作为交叉验证集，不同的数据集组合会对同一个模型产生不同的学习器(注意区分概念)，交叉验证集用于对不同学习器进行拟合度校验，有不同的算法和参数确定不同的模型，再在每个模型中通过交叉验证得到不同的学习器，对于k折交叉验证得到的学习器拟和的准确率取平均作为打分，来找到最佳的模型</p><p>数据拟合不好无非两种情况：过拟合和欠拟合</p><p>使用交叉验证时，随着多项式次数的增长，会由欠拟合到过拟合，错误率是下凸函数，所以要判断是偏差/欠拟合还是方差/过拟合</p><p>训练集误差和交叉验证集误差近似时：偏差/欠拟合 交叉验证集误差远大于训练集误差时：方差/过拟合</p><p>对于正则化参数的选择 $\lambda$ 也是，通常是使用 0-10之间的呈现2倍关系的值</p><p>训练集误差随着 $lambda$ 值的增大而增大，交叉验证集误差随着 $\lambda$值的增大而先增大后减小，由过拟合到欠拟合</p><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p>当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据</p><p>将训练集误差和交叉测试集误差作为训练样本规模的函数</p><p>区分欠拟合/过拟合：<br>对于欠拟合，增加训练集规模的时候训练误差和交叉测试集误差趋于稳定，不受影响<br>当过拟合的时候，交叉测试集误差明显大于训练集误差，增加训练集规模效果显著    </p><h3 id="误差检验"><a href="#误差检验" class="headerlink" title="误差检验"></a>误差检验</h3><p>思想：不要想着一开始就得到完美的模型，是需要用方法来验证以及用证据来指导进一步的优化和方向的，有时候不能仅凭直觉来解决问题  </p><ul><li>简单算法，使用交叉验证集测试</li><li>绘制学习曲线来决定进一步的选择</li><li>误差检验，来看是否有什么系统化的趋势</li></ul><h3 id="数值误差度量"><a href="#数值误差度量" class="headerlink" title="数值误差度量"></a>数值误差度量</h3><p>查准率(P)：判断正例中的真正例的比例<br>查全率(R)：真正例中判断出的比例<br>有TP\FN\FP\TN</p><script type="math/tex; mode=display">P = \frac{TP}{TP+FP}</script><script type="math/tex; mode=display">R = \frac{TP}{TP+FN}</script><p>一般两者矛盾，所以使用F1方法度量：  </p><script type="math/tex; mode=display">\frac{1}{F_1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R})</script><p>加权调和平均：  $F_\beta$</p><script type="math/tex; mode=display">\frac{1}{F_\beta} = \frac{1}{1+\beta^2}(\frac{1}{P} + \frac{\beta^2}{R})</script><ul><li>$\beta = 1$即为 $F_1$</li><li>$\beta &lt; 1$查准更多</li><li>$\beta &gt; 1$查全更多</li></ul><h3 id="数据的地位"><a href="#数据的地位" class="headerlink" title="数据的地位"></a>数据的地位</h3><blockquote><p>取得成功的人不是拥有最好算法的人，而是拥有最多数据的人</p></blockquote><p>所谓好的算法是数据集不足的时候也能给出低方差低偏差的预测结果</p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>math</title>
      <link href="/posts/a927044d.html"/>
      <url>/posts/a927044d.html</url>
      
        <content type="html"><![CDATA[<p>现在总有一种恍惚感和不安感，觉得自己的付出的时间没有产出，对未来很没有底气和把握，总觉得目前的学习状态不是一个优秀的人该有的状态</p><p>不得不说我一直忽略了一个东西:”数学”<br>太过注重所谓的技术了，结果是编程语言也用不好，只能对于框架开始自娱自乐了</p><p>有这么一句话：<br>一流程序员看数学<br>二流程序员看算法<br>三流程序员看逻辑<br>四流程序员看框架<br>五流程序员看Google</p><p>对于一个本科是计算机专业的人来讲，计算机也许只是跳板罢了，是一个可以让我走向很多岗位的跳板，我现在太把那些看起来堂而皇之的技术名词以及在既定框架下做着重复性工作的计算机技术看重了，对于一个重复性工作，是个人一直做上两年都可以做的滚瓜烂熟，没有任何门槛，只是工具的使用，所以没有任何价值，永远也可以被取代</p><p>然而我现在连这些简单的事情都做不好</p><p>代码只是工具罢了，只是让我们实现一个什么东西的工具，就像公式是让我们能把题解出来的公式，对于这些东西要<strong>理解本质，理解它是个什么东西，我们什么时候可以用到它</strong>，那就很好掌握，可以很快很简单</p><p>数学才是决定上限的，需要是造轮子的能力，需要的是可以有自己的新点子，创造性地想法，在这一领域实现”创造”，而不是复刻，才有价值，要实现创造需要完全理解</p><p>不要急躁，计算机的特点是达到理解的周期长，不像是高中数学和物理一样看一眼公式就可以理解了</p><p>所以需要，静下心来深刻理解每一个小的东西是干什么，比如遇到Django，要深刻理解这个函数是创建了一个什么，创建了这个东西可以干什么</p><p>切忌快速吸收和掌握的浮躁，那样会只限于表面，只限于它教你的知识，而不具备任何创造的能力</p><p><strong>根除沽名钓誉之心，摒弃急功近利之意，涤荡冒进浮躁之气</strong><br>未来能不能找到工作，未来要保研还是考研，这不是我一个低年级大学生应该考虑的问题，不要功利心太重了，也不要野心太大了，享受当下，向着热爱进发就好了</p><p>最大的问题出现在心态上</p><p>我初中之所以学得好，是因为不被“目标”所拘束住，只是关注于学习本身，用摆烂的心态去学习(渡过冷启动时期)真的是一种美德</p>]]></content>
      
      
      
        <tags>
            
            <tag> 思考 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>database</title>
      <link href="/posts/c953062e.html"/>
      <url>/posts/c953062e.html</url>
      
        <content type="html"><![CDATA[<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/OceanBase-Partner/lectures-on-dbms-implementation/blob/main/lecture-2.md">miniob_lecture</a><br><a href="https://zhuanlan.zhihu.com/p/149287061">知乎-B+树看这一篇就够了</a><br><a href="http://blog.codinglabs.org/articles/theory-of-mysql-index.html">MySQL索引背后的数据结构及算法原理</a></p><h2 id="磁盘存储"><a href="#磁盘存储" class="headerlink" title="磁盘存储"></a>磁盘存储</h2><p>存储大小：盘面——磁道——扇区<br>延时：寻道时间——旋转延迟——数据传输时间  </p><ul><li>增加奇偶位来判断数据是否正确</li></ul><p>数据要进行字节对齐<br>记录和块都有一个header来对时间戳或者偏移量来进行存储，便于数据的查找和维护，数据块对记录的存储是朝向header的栈<br>变长记录的存储：    </p><ul><li>变长字段：首地址(address) + 字段长度  （注意变长字段一般在定长字段之后存储）</li><li>重复字段：首地址 + 字段长度 + 重复次数    </li></ul><p>文件使用堆文件，<strong>记录</strong>能插则插，为了保证页面一致性所以通常一个堆文件是一种关系</p><h3 id="页"><a href="#页" class="headerlink" title="页"></a>页</h3><p>主流操作系统对文件的处理是将文件视为无结构的流文件，只关心数据的传输而不关心数据的格式，而DBMS将流文件划分为页(虚拟分页)来进行内存和磁盘数据的交换<br>文件分页，同一关系的文件放到一个页里<br>DBMS有一个间接层，将页面ID映射成文件路径和偏移量，单文件只映射成偏移量<br>数据库页是硬件页的整数倍(所以DBMS要保证一致性问题)，是缓冲和磁盘交换的基本单位<br>页头存储页面元数据，有些数据库要求页头自包含，数据区对每个记录的存储方式是采用插槽</p><h4 id="记录存储"><a href="#记录存储" class="headerlink" title="记录存储"></a>记录存储</h4><p>记录以插槽方式存储，插槽个数维护在页头中，维护：</p><ul><li>最后一个使用的插槽的起始位置</li><li>插槽数组数</li><li>已使用的槽的数量<br>插槽个数是变化的，记录插入也是变化的，所以槽数组(指的是页头)从前向后增长，而被插入的记录数据则是从页尾向前增长，顶到就是满了(啊哈~)</li></ul><p>记录组织结构，对于变长记录有两种：</p><ul><li>记录头存储第一个变长字段的起始位置，以及除第一个以外所有变长字段的偏移量</li><li>字段定长，维护一个溢出表，字段指针指向，当存储大数据的时候也要用溢出表(但是会增加IO)</li></ul><h3 id="buffer"><a href="#buffer" class="headerlink" title="buffer"></a>buffer</h3><p>DBMS为了实现数据的永久存储，面向的是磁盘，但是要以内存作为数据交换的媒介，所以将内存作为缓冲区，将数据以页为单位从磁盘提到内存中，这个过程由缓冲区管理器实现<br>缓冲池是内存用于缓冲页的空间，缓冲池管理器是给内存分配缓冲空间的子系统  </p><p>内存会维护一个页表，用于记录页面的磁盘映射和顺序关系，同时也维护了两种：    </p><ul><li>脏标志：表示有线程更新了磁内存中的页，需要从内存取出来更新磁盘</li><li>引用计数器：表示引用内存中的页的数量，大于0则不允许取出内存，相当于加锁</li></ul><p>这个缓冲池和操作系统的缓冲区很像，但是在内存中是分开的，会对OS的缓冲IO进行绕过来加速和简化DBMS对数据的访问处理  </p><p>缓冲区的页面<strong>替换算法</strong>：  </p><ul><li>LRU-K：记录几次历史的最近使用时间(时间戳放在页表)，以及时间间隔，来预测使用可能   (感觉很多os问题的解决方法就是空间换时间，没有什么很巧妙的方法)</li><li>淘汰局部化：对每个查询进行局部页面淘汰</li><li>优先级：根据页面上下文来判断其重要性</li></ul><p>对脏页的写回(后台写)：<br>DBMS定期扫描页表，发现脏页进行安全写回(保证没在被使用)，然后取消脏标志<br><strong>淘汰页面时对脏页的处理可能是一个很重要的优化点</strong>  </p><p>缓冲池优化：<br>多缓冲池：  </p><ul><li>每个数据库一个缓冲池</li><li>不同缓冲池量身定制不同的策略</li><li>对象ID，需要扩展元数据，使其包含关于每个缓冲池正在管理哪些数据库对象的信息，然后通过对象ID，就可以实现从对象到特定缓冲池的映射</li><li>散列，DBMS散列页面ID以选择访问哪个缓冲池<br>预取：<br>在处理第一组页面时，系统可以将第二组页面预取到缓冲池中</li></ul><h2 id="B-tree"><a href="#B-tree" class="headerlink" title="B+ tree"></a>B+ tree</h2><p>一颗，简单的，多路平衡树，但是代表了树的精华<br>B+ tree中的数据指针都存储在叶子结点上，指针指向的是磁盘区域，数据均以键值对形式存储，便于高效查找和维护，查找磁盘中的内容是按照键值key来进行查找的<br>B+ tree的阶：代表了每个内部节点能拥有的最大子节点个数m，能容纳的最大数据为m-1，阶太大的话会提高删除和增加节点的复杂性，阶太小的话会增加输的高度<br>B+ tree的结点数据个数最小不能小于[m/2]<br>内部节点存储的是用于查询的关键字，不一定要存储所有<br>这里的B+ tree根节点包含最大值</p><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>几种情况：</p><ul><li>叶节点数据值指针个数小于阶数，直接插入</li><li>叶节点数据值指针个数等于阶数，父节点数据指针个数小于阶数，分裂后插入，并将[m/2]的叶节点提到父节点作为索引</li><li>叶节点数据值指针个数等于阶数，父节点数据指针个数等于阶数，叶节点分裂一次，父节点再分裂一次，不断向上直到有未满阶的内部节点    </li><li>插入值大于最大值，则根节点和内部节点替换目前的最大值，然后正常插入分裂等</li></ul><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><ul><li>删除以后所在的叶子结点数据个数大于[m/2]，直接删除</li><li>删除最大最小值，直接全部替换次大次小值</li><li>删除以后所在的叶子结点数据个数小于[m/2]，像兄弟节点借一个，然后改变一下父节点</li><li>如果兄弟节点没有多余的关键字可以借，那删除之后合并，并回溯更改所有内部节点关键字</li><li>如果回溯的时候存在不满足B+ tree要求的情况，依照以上步骤进行处理</li></ul><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p>$\log_m (N)$</p><h2 id="SQL引擎结构"><a href="#SQL引擎结构" class="headerlink" title="SQL引擎结构"></a>SQL引擎结构</h2><p>parser : 将sql语句翻译成语法树<br>resolver : 对parser语法树进行进一步的约束检查和属性提取(会翻译成另一种数据结构)<br><a href="https://www.modb.pro/db/1701966184379928576">parser和resolver</a><br>transformer&amp;optimizor : 基于代价和基于改写的查询优化    </p><p>关系运算：常使用的有笛卡尔积和自然连接，查询的本质就是对集合关系的数学表达，查询优化要找到计算代价最小的集合关系计算方法    </p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OS-01</title>
      <link href="/posts/7b347bb3.html"/>
      <url>/posts/7b347bb3.html</url>
      
        <content type="html"><![CDATA[<p>进程是程序执行，以及内存空间，以及执行上下文等一系列要素</p><p>线程是一个进程中的多个小的执行单元，其共享进程的堆和方法栈的资源，可以并发，每个线程有自己的程序计数器</p><p>线程崩溃则整个进程崩溃，因为线程不是独立的，但是各个进程之间是完全独立的，所以多进程比多线程健壮</p><p>线程切换只是CPU的直接访问切换，但是进程切换是包括分配内存空间，保存和产生执行上下文等步骤的切换，所以线程切换比进程开销更小</p><p><strong>虚存：</strong>通过进程的分页设计，来让进程的一部分在运行进程的时候可以从外存实时加载到内存，以减少内存的开销</p><h3 id="虚存"><a href="#虚存" class="headerlink" title="虚存"></a>虚存</h3><p>先介绍几个定义：</p><ol><li>段：有代码段和数据段，每个段在内存中占用连续的物理空间</li><li>CS寄存器：存储代码段的段基址，或者存放段选择子</li><li>DS寄存器：存储数据段的段基址，或者存放段选择子</li><li>IP寄存器：存储代码段的段偏移量</li><li>全局段描述符表：存储在内存中，有各个(可执行文件)代码和数据段的基址</li><li>全局段描述符表寄存器：存储全局段描述符表在内存中的基址</li></ol><p>CPU访问内存中代码段和数据段的方式有以下几种：</p><h4 id="直接存取"><a href="#直接存取" class="headerlink" title="直接存取"></a>直接存取</h4><p>要执行可执行文件时，代码段和数据段被分配唯一的物理地址，但缺点在不能同时执行多次这个程序，而且地址锁定也不灵活</p><h4 id="段基址-段偏移量"><a href="#段基址-段偏移量" class="headerlink" title="段基址+段偏移量"></a>段基址+段偏移量</h4><p>为了灵活访问内存中的不同地方的代码，出现了段基址+段偏移量的方法，典型的是Intel的8086，如图：</p><p>CS寄存器，DS寄存器存放代码段和数据段的段基址，IP寄存器存放段偏移量，访问的时候CPU获取段基址和段偏移量(CS:IP方法)来进行定位</p><p>后续要获取有关段地址的信息时，可以直接从段寄存器里面获取，加快效率</p><h4 id="段选择子-段偏移量"><a href="#段选择子-段偏移量" class="headerlink" title="段选择子+段偏移量"></a>段选择子+段偏移量</h4><p>设置了全局段描述符表，来存放各个段的信息(包括位置基址)<br>段选择子是对某个特定段用于定位其在全局段描述符表中的位置，所以定位方式为：<br>先将段选择自防砸段寄存器中(如S寄存器)，CPU再从段寄存器中获取段选择子的13位高位来确定在全局段描述符表中的位置，再从全局段描述符表寄存器中获取全局段描述符表的基址，计算得段的实际地址，在加上IP寄存器中的偏移量，进行访问</p><h4 id="虚拟地址"><a href="#虚拟地址" class="headerlink" title="虚拟地址"></a>虚拟地址</h4><p>虚拟地址是一个进程在执行的时候所分配给的逻辑地址，它并不是在物理地址(内存)，而是一个进程所需要空间的虚拟编号，其实际对应的部分可能在物理内存中，也可能在磁盘中</p><p>为了充分利用物理内存，采用分页系统，将物理内存以4K为一个单位分成若干页，并且每一页编有物理页号，对应的，虚拟内存也以相同大小编成了若干虚拟页，有：<br>物理内存 = 物理页号(PPN) + 物理页偏移量(PPO)<br>虚拟内存 = 虚拟页号(VPN) + 虚拟页偏移量(VPO)</p><p>虚拟页相当于一张为了进程而设计的执行表，是针对进程全面的，要执行进程的时候，需要为了进程而访问的哪一块存储的的时候，找找虚拟页，如果已经要访问的存储在内存中那就再好不过了，如果不在的话，那要不就是在磁盘中，我们就把他放到内存里，要不就是压根没分配存储空间，那我们就给它在磁盘上分配一个再放到内存里</p><p>虚拟页和物理页有一定的映射关系，其映射方法通过硬件MMU实现，虚拟页三种映射状态如下：</p><ol><li>未分配，指虚拟页没有分配磁盘空间</li><li>未缓冲：指虚拟页没有分配到物理内存，而是在映射在磁盘的状态</li><li>已缓冲：指虚拟页已经有了物理内存的映射</li></ol><p>有几个虚拟页的映射是所有进程一致的：<br><strong>内核</strong>和<strong>共享库</strong></p><p>为了判断和管理虚拟页的映射状态，在内存中会有一个<strong>页表(PT)</strong><br>页表有两列：有效位和地址，有效位的0和1和地址列是否为null可以组合判断虚拟页是三种状态的哪一个</p><p>页表专门有一个页表基址寄存器(PTBR)</p><p>以下介绍虚拟地址翻译物理地址的方法：</p><p>有两种结果，命中或者未命中，命中就是虚拟页已缓冲，未命中就是未缓冲或者未分配</p><p>命中：</p><ol><li><p>CPU将虚拟地址(VA)送入MMU,MMU根据页表基址寄存器中页表的起始地址加上虚拟页号，找到了页表项的物理地址PTEA。</p></li><li><p>MMU将PTEA送入到高速缓冲或者内存。</p></li><li><p>从高速缓冲或者内存中找到页表项(PTE)，返回页表项(PTE)给MMU。</p></li><li><p>MMU根据PTE找出物理页号，然后加上虚拟页偏移量形成物理地址(PA),送入到高速缓冲或者内存。</p></li><li><p>高速缓冲或者内存获取数据，返回数据给处理器。</p></li></ol><p>未命中:</p><ol><li><p>CPU将虚拟地址(VA)送入MMU,MMU根据页表基址寄存器中页表的起始地址加上虚拟页号，找到了页表项的物理地址PTEA。</p></li><li><p>MMU将PTEA送入到高速缓冲或者内存。</p></li><li><p>从高速缓冲或者内存中找到页表项(PTE)，返回页表项(PTE)给MMU。</p></li><li><p>MMU根据PTE,发现页不在内存中，未命中，因此MMU发送一个缺页中断，交由缺页异常处理程序处理。</p></li><li><p>缺页异常处理程序根据页置换算法，选择出一个牺牲页，如果这个页面已经被修改了，则写出到磁盘上，最后将这个牺牲页的页表项有效位设置为0，存入磁盘地址。</p></li><li><p>缺页异常程序处理程序调入新的页面，如果该虚拟页尚未分配磁盘空间，则分配磁盘空间，然后磁盘空间的页数据拷贝到空闲的物理页上，并更新PTE的有效位为1，更新物理页号，缺页异常处理程序返回后，再回到发生缺页中断的指令处，重新按照页表项命中的步骤执行</p></li></ol><p>以下介绍虚拟地址翻译物理地址的方法：</p><p>有两种结果，命中或者未命中，命中就是虚拟页已缓冲，未命中就是未缓冲或者未分配</p><p>命中：</p><ol><li><p>CPU将虚拟地址(VA)送入MMU,MMU根据页表基址寄存器中页表的起始地址加上虚拟页号，找到了页表项的物理地址PTEA。</p></li><li><p>MMU将PTEA送入到高速缓冲或者内存。</p></li><li><p>从高速缓冲或者内存中找到页表项(PTE)，返回页表项(PTE)给MMU。</p></li><li><p>MMU根据PTE找出物理页号，然后加上虚拟页偏移量形成物理地址(PA),送入到高速缓冲或者内存。</p></li><li><p>高速缓冲或者内存获取数据，返回数据给处理器。</p></li></ol><p>未命中:</p><ol><li><p>CPU将虚拟地址(VA)送入MMU,MMU根据页表基址寄存器中页表的起始地址加上虚拟页号，找到了页表项的物理地址PTEA。</p></li><li><p>MMU将PTEA送入到高速缓冲或者内存。</p></li><li><p>从高速缓冲或者内存中找到页表项(PTE)，返回页表项(PTE)给MMU。</p></li><li><p>MMU根据PTE,发现页不在内存中，未命中，因此MMU发送一个缺页中断，交由缺页异常处理程序处理。</p></li><li><p>缺页异常处理程序根据页置换算法，选择出一个牺牲页，如果这个页面已经被修改了，则写出到磁盘上，最后将这个牺牲页的页表项有效位设置为0，存入磁盘地址。</p></li><li><p>缺页异常程序处理程序调入新的页面，如果该虚拟页尚未分配磁盘空间，则分配磁盘空间，然后磁盘空间的页数据拷贝到空闲的物理页上，并更新PTE的有效位为1，更新物理页号，缺页异常处理程序返回后，再回到发生缺页中断的指令处，重新按照页表项命中的步骤执行</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OS-02</title>
      <link href="/posts/e23d2a09.html"/>
      <url>/posts/e23d2a09.html</url>
      
        <content type="html"><![CDATA[<p>进程控制块：操作系统创建和管理，包含了进程的充分信息，可以方便于中断，因为保存了进程的寄存器以及上下文信息等</p><p>进程轨迹：进程执行的指令序列<br>进程轨迹的的交替方式可以描述处理器的行为<br>分派器(调度器)控制进程的切换</p><p>进程创建，可以由进程创建进程，一个进程的显示请求创建了另一个进程的时候，称为进程派生</p><p>两状态模型：运行态和非运行态，有一个进程队列</p><p>五状态模型：非运行态分为了阻塞态和就绪态，还有未加载到内存的新建态以及可以释放的退出态<br>具体看书上的图<br>阻塞——就绪：当等待的事件发生了以后，阻塞态的进程就成为了就绪态<br>运行——就绪：超时<br>运行——阻塞：等待事件</p><p>有阻塞队列和就绪队列，当一个事件发生时，扫描等待这个事件的阻塞队列，把阻塞进程放到就绪队列里</p><p>产生了新的问题：<br>由于处理器速度远大于IO操作的速度，所以可能会存在所有进程都处于IO的情况，内存已满进不来新的进程，解决方法是将阻塞的进程转移到磁盘的<strong>挂起队列(suspend queue)</strong>中，这个行为称之为<strong>交换</strong>，交换也是IO操作，但由于磁盘IO速度是最快的IO，所以往往可以提高效率</p><p>所以阻塞态多了一个交互的状态，成为<strong>挂起态</strong>，当事件发生的时候挂起态进程可以回到内存中运行<br>所以又多了两个状态：阻塞/挂起态和就绪/挂起态<br>成了七状态模型</p><p>资源：文件系统，IO，处理器，内存外存等<br>操作系统可以看做是管理资源的实体，所以需要知道进程和资源的当前状态，所以OS维护4类信息表：内存，I/O，文件和进程</p><p>进程映像包括：数据，程序，栈，进程控制块(属性集)，进程映像可以一部分在内存，一部分在外存，但是执行的时候要可以放到内存中，进程映像也是不连续分页的，操作系统的进程表要知道进程映像每页的位置<br>进程控制块包含的信息：</p><ul><li>进程标识信息(包含标识符等)</li><li>进程状态信息(处理器状态信息)</li><li>进程控制信息</li></ul><p>处理器设置有一个称为程序状态字(PSW)的寄存器，如Intel x86的EFLAGS寄存器，保存程序的大量状态信息</p><p>有个东西叫进程控制块，包含操作系统需要的一个进程的所有信息，为了安全由一个处理程序例程来作为修改进程控制块的仲裁程序</p><p>处理器有两种执行模式：<br>与操作系统相关联的内核模式<br>与用户相关联的用户模式</p><p>内核(操作系统的一部分，管理者重要的系统功能，具体看书)模式的是特权模式，可以访问信息，用户模式权限受限<br>处理器在执行时候区分两种模式的方法是通过PSW的前几位来判断权限</p><p>操作系统进程创建：</p><ol><li>分配一个唯一的进程标识符</li><li>为进程分配空间</li><li>初始化进程控制块：除非继承父进程或者显式请求否则初始时不用有任何资源，优先级默认最低等</li><li>设置链接：进调度队列</li><li>创建其他数据结构：如记账文件等</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL</title>
      <link href="/posts/4d712855.html"/>
      <url>/posts/4d712855.html</url>
      
        <content type="html"><![CDATA[<p><strong><code>2022</code>年<code>3</code>月<code>27</code>日学习<code>SQL</code>，共耗时：<code>4</code>小时</strong><br><strong>学习资料：<a href="https://www.liaoxuefeng.com/wiki/1177760294764384/1179611432985088">SQL教程</a></strong></p><p>姑且粗浅了解一下SQL，之后要用或者有时间再细学</p><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p><strong>字段：</strong>数据项，对应为数据库里的列</p><p><strong>记录：</strong>一条由一系列字段组合成的数据，对应为数据库里的行</p><p><strong>主键：</strong>用于对一条记录的唯一字段标识，一般用<code>id</code>或<code>GUID类型</code></p><p><strong>联合主键：</strong>多条字段设置为主键（不常用）</p><p><strong>外键：</strong>用于关联另一个表，使用外键约束会降低数据库的性能，外键约束构造：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> students</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">FOREIGN</span> KEY fk_class_id;</span><br></pre></td></tr></table></figure><p><strong>索引：</strong>索引用于快速检索记录，主键是最典型的索引，也可以设置多个索引提高查询效率，索引字段多用hash算法来构造，所以散列值越多，冲突越少，查询效率越高<br>创建索引：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> students</span><br><span class="line"><span class="keyword">ADD</span> INDEX idx_score (score);</span><br></pre></td></tr></table></figure><p>索引的优点是提高了<strong>查询效率</strong>，缺点是在插入、更新和删除记录时，需要同时修改索引，因此，索引越多，插入、更新和删除记录的速度就越慢。</p><p>创建唯一索引（不可重复）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> students</span><br><span class="line"><span class="keyword">ADD</span> <span class="keyword">UNIQUE</span> INDEX uni_name (name);</span><br></pre></td></tr></table></figure><p>添加唯一约束但是不创建索引：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> students</span><br><span class="line"><span class="keyword">ADD</span> <span class="keyword">CONSTRAINT</span> uni_name <span class="keyword">UNIQUE</span> (name);</span><br></pre></td></tr></table></figure><ol><li>找参数，判断字符型还是数字型</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$<span class="keyword">sql</span><span class="operator">=</span>&quot;SELECT * FROM users WHERE id=&#x27;$id&#x27; LIMIT 0,1&quot;;</span><br><span class="line">$<span class="keyword">sql</span><span class="operator">=</span>&quot;SELECT * FROM users WHERE id=$id LIMIT 0,1&quot;;  # 数字</span><br></pre></td></tr></table></figure><ol><li>判断字段数<code>order by 4</code></li><li>判断回显点<code>union select 1,2,3</code></li><li>判断数据库名<code>union select 1,2,database()</code>，表名<code>union select 1,2,group_concat(table_name) from information_schema.tables where table_schema=&#39;库名&#39;(或者最后加limit语句来获取指定的表)</code>，列名<code>union select 1,2,group(column_name) from information_schema.columns where table_name=&#39;表名&#39;</code></li><li>显示列内容<code>union select 1,2,group_concat(列名) from 表名</code></li></ol><p>网页SQL语句其实是php代码中的SQL，所以会有类似$id等参数引用来通过get获取</p><p>url中添加注释：<br>—+ 或者 —%20 或者 %23 ，不能用#</p><p>limit语句<br>group_concat()显示所有的内容，但是可能会受到现实的字数限制</p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS61A-note</title>
      <link href="/posts/83c24a0e.html"/>
      <url>/posts/83c24a0e.html</url>
      
        <content type="html"><![CDATA[<h2 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h2><h3 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h3><p>本质是更上一级的抽象<br>函数可以作为函数的参数,不能作为控制结构的<br>关键在于抽象出概念<br>函数所在的帧是直接父级  </p><p>eg1:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">average</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (x + y)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">improve</span>(<span class="params">update, close, guess=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> close(guess):</span><br><span class="line">        guess = update(guess)</span><br><span class="line">    <span class="keyword">return</span> guess</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">approx_eq</span>(<span class="params">x, y, tolerance=<span class="number">1e-3</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">abs</span>(x - y) &lt; tolerance</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sqrt</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sqrt_update</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> average(x, a/x)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sqrt_close</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> approx_eq(x * x, a)</span><br><span class="line">    <span class="keyword">return</span> improve(sqrt_update, sqrt_close)</span><br><span class="line"></span><br><span class="line">result = sqrt(<span class="number">256</span>)</span><br></pre></td></tr></table></figure><p>函数设计是一种思想，对问题从顶层到底层一步步抽象，，数学中将特殊问题一般化，而模板就是面向一般化的“步骤本身”</p><p>为了能使用抽象模板，一个技巧是以函数为参数可以得到需求形式的函数<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">compose1</span>(<span class="params">f, g</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">h</span>(<span class="params">x</span>):</span><br><span class="line">            <span class="keyword">return</span> f(g(x))</span><br><span class="line">        <span class="keyword">return</span> h</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add_one_and_square = compose1(square, successor)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add_one_and_square(<span class="number">12</span>)</span><br><span class="line"><span class="number">169</span></span><br></pre></td></tr></table></figure></p><h3 id="lamda"><a href="#lamda" class="headerlink" title="lamda"></a>lamda</h3><p>也可以使用lamda</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">compose1</span>(<span class="params">f,g</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span> x: f(g(x))</span><br></pre></td></tr></table></figure><p>或<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>compose1 = <span class="keyword">lambda</span> f,g: <span class="keyword">lambda</span> x: f(g(x))</span><br></pre></td></tr></table></figure></p><p>lamda：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">     <span class="keyword">lambda</span>            x            :          f(g(x))</span><br><span class="line"><span class="string">&quot;A function that    takes x    and returns     f(g(x))&quot;</span></span><br></pre></td></tr></table></figure></p><h3 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h3><p>思想：等到给参数的时候才进行进一步调用，所以要用到高阶</p><p>用法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">trace1</span>(<span class="params">fn</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapped</span>(<span class="params">x</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;-&gt; &#x27;</span>, fn, <span class="string">&#x27;(&#x27;</span>, x, <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> fn(x)</span><br><span class="line">        <span class="keyword">return</span> wrapped</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>@trace1</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">triple</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span> * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>triple(<span class="number">12</span>)</span><br><span class="line">-&gt;  &lt;function triple at <span class="number">0x102a39848</span>&gt; ( <span class="number">12</span> )</span><br><span class="line"><span class="number">36</span></span><br></pre></td></tr></table></figure></p><p>本质：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">triple</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span> * x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>triple = trace1(triple)</span><br></pre></td></tr></table></figure></p><p>装饰器接受一个函数为参数，返回的本质上也是这个函数</p><p>使用装饰函数，以目标函数作为参数，进行指定的<strong>处理</strong>(装饰)</p><p>可以多重装饰，只要返回值是原函数，装饰只是增加副作用<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">count</span>(<span class="params">f</span>):           <span class="comment">#这种格式的，经过count和memo的修饰，就会有父帧的call_count属性，以及一些和属性相关的功能，反正不修改函数本身</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">counted</span>(<span class="params">n</span>):</span><br><span class="line">            counted.call_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> f(n)</span><br><span class="line">        counted.call_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> counted</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">memo</span>(<span class="params">f</span>):</span><br><span class="line">        cache = &#123;&#125;</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">memorized</span>(<span class="params">n</span>):</span><br><span class="line">            <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> cache:</span><br><span class="line">                cache[n] = f(n)</span><br><span class="line">            <span class="keyword">return</span> cache[n]</span><br><span class="line">        <span class="keyword">return</span> memorized</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counted_fib = count(fib)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib =memo(count_fib)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">19</span>)</span><br><span class="line"><span class="number">4181</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counted_fib.call_count</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">34</span>)</span><br><span class="line"><span class="number">5702887</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counted_fib.call_count</span><br><span class="line"><span class="number">35</span></span><br></pre></td></tr></table></figure></p><p>函数式编程思想：<br>所谓函数就是接受对象作为参数，并返回对象的一个盒子，这样看来函数就是一种自己定义的数学运算符，即像是自己定义了一种新的计算，也像是一个数学公式<br>对象可以是类、基本数据、函数，而且函数本身也可以作为对象，这就意味着最小操作类型可以看作是对象，而所谓的类、函数等也不过是一种新的数据类型组织方式<br>所以编写程序的过程就是自己构造一个数学公式来产生目标输出的功能<br>所以关键是，要知道要接受一个怎么样的对象，返回一个什么样功能的对象  </p><p>再例如Web中：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">PlatformAuthenticated</span>(<span class="params">auth_level</span>):      <span class="comment">#将验证handler的函数作为修饰器，通过类设置一些外部属性，调用get方法时自动调用call，自动验证</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">StrictPlatformAuth</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, handler</span>):</span><br><span class="line">            self.handler = handler</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, request, response</span>):</span><br><span class="line">            auth = request.authorization(auth_level)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> auth <span class="keyword">or</span> <span class="keyword">not</span> check_auth(auth.username, auth.password):</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">&quot;BlahBlahBlah&quot;</span>)</span><br><span class="line">            self.handler(request, response)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> StrictPlatformAuth</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WebHandler</span>:</span><br><span class="line"><span class="meta">    @PlatformAuthenticated(<span class="params">ADMIN</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, request, response</span>):</span><br><span class="line">        do_something()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RobotWebHandler</span>:</span><br><span class="line"><span class="meta">    @PlatformAuthenticated(<span class="params">ROBOT</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, request, response</span>):</span><br><span class="line">        do_something()</span><br></pre></td></tr></table></figure></p><h3 id="柯里化"><a href="#柯里化" class="headerlink" title="柯里化"></a>柯里化</h3><p>多参数函数和指定参数函数之间的转换：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">curry2</span>(<span class="params">f</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回给定的双参数函数的柯里化版本&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">g</span>(<span class="params">x</span>):</span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">h</span>(<span class="params">y</span>):</span><br><span class="line">                <span class="keyword">return</span> f(x, y)</span><br><span class="line">            <span class="keyword">return</span> h</span><br><span class="line">        <span class="keyword">return</span> g</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">uncurry2</span>(<span class="params">g</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回给定的柯里化函数的双参数版本&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, y</span>):</span><br><span class="line">            <span class="keyword">return</span> g(x)(y)</span><br><span class="line">        <span class="keyword">return</span> f</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pow_curried = curry2(<span class="built_in">pow</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pow_curried(<span class="number">2</span>)(<span class="number">5</span>)</span><br><span class="line"><span class="number">32</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>map_to_range(<span class="number">0</span>, <span class="number">10</span>, pow_curried(<span class="number">2</span>))</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">32</span></span><br><span class="line"><span class="number">64</span></span><br><span class="line"><span class="number">128</span></span><br><span class="line"><span class="number">256</span></span><br><span class="line"><span class="number">512</span></span><br></pre></td></tr></table></figure></p><h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><p>递归思路：自顶向下，只关心下一层，不关心全部(return)<br>只有最后一层才有细节(if n==1)</p><p>要清楚<strong>迭代对象</strong>和<strong>迭代关系</strong><br>以函数为整体看待<br>对应数学归纳法，归纳的时候考虑的是过程中的一般情况以及临界条件，不应该自动带入考虑程序怎么跑<br>只要是序列间有关系的问题都可以用递归解决</p><p>hw03的count_change</p><p>hw04的balance,归纳法：假设这个函数已经成立，可以用这个函数本身来递推的实现功能，再验证边界成立<br>    preorder: 列表本身可以被迭代，所以可以写二重循环</p><h2 id="数据抽象"><a href="#数据抽象" class="headerlink" title="数据抽象"></a>数据抽象</h2><h3 id="序列"><a href="#序列" class="headerlink" title="序列"></a>序列</h3><p>序列解构：将序列元素的子序列元素映射到指定的name上</p><p>range是序列</p><p>有.count和.index行为<br>count行为匹配的是非重叠的字符串</p><p>字符串本质也是一种序列，换行符也会被认为是字符串元素<br>字符串的in匹配的是字符串</p><p>接口：使用map，reduce，filter来实现对数据的一系列组件操作，<br>也可以使用生成器表达式：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">acronym</span>(<span class="params">name</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">tuple</span>(w[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> name.split() <span class="keyword">if</span> iscap(w))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">sum_even_fibs</span>(<span class="params">n</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(fib(k) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>) <span class="keyword">if</span> fib(k) % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>使用构造器和选择器对抽象数据进行操作</p><h2 id="2-4-可变数据"><a href="#2-4-可变数据" class="headerlink" title="2.4 可变数据"></a>2.4 可变数据</h2><p>对象是一种数据类型，其中包含函数(操作数据的行为)以及数据，不仅是值，也是一个过程<br><strong>python万物皆对象</strong></p><h4 id="为什么要设计类？"><a href="#为什么要设计类？" class="headerlink" title="为什么要设计类？"></a>为什么要设计类？</h4><p>将数据和行为为基本元素抽象成一个新的数据类型，这种数据类型更加真实地映射了现实世界的对象，从功能上看更便于实现过程和行为的构建，从结构上看提供了强大的数据和行为之间的交互和传递能力(继承，多态)，以及极大地方便了对数据的处理和对行为的调用</p><h3 id="理解抽象"><a href="#理解抽象" class="headerlink" title="理解抽象"></a>理解抽象</h3><p>抽象的最终目的是产生更加通用的模板来简化操作，更通用、更简单地实现更多的任务<br>抽象必然是一层一层的，从计算机硬件电路到硬件模块的封装，从硬件到终端的交互，从基本数据类型到复合数据类型(OOP)，从符合数据类型到一系列数据和行为构成的集合体(包和库)，再到软件的实现<br>实现功能的抽象模块之间应该是尽量独立的，不互相影响的，否则负责的交互关系会使底层出现问题的时候产生不可预测的对高级抽象的影响，以及加大了抽象修改的难度<br>所以从底层一步步向上理解学习吧，才能有更扎实的基础</p><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>使用变异(mutating)操作会修改可变对象<br>对于可变对象，关联名称不是创建副本，而是绑定到同一个对象上：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>chinese = [<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;pear&#x27;</span>, <span class="string">&#x27;monkey&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>suits = chinese</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>suits.pop()</span><br><span class="line"><span class="string">&#x27;monkey&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>chinese</span><br><span class="line">[<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;pear&#x27;</span>]</span><br></pre></td></tr></table></figure></p><p>创建副本的方法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>nest = <span class="built_in">list</span>(suits)</span><br></pre></td></tr></table></figure></p><p>判断两个对象是否是同一个，用<code>is</code>或<code>is not</code>，判读是否相等用<code>==</code></p><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><p>不可变对象</p><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><p>描述性检索的数据抽象</p><p>dict()可以将键值对列表转化为字典：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dict</span>([(<span class="number">3</span>, <span class="number">9</span>), (<span class="number">4</span>, <span class="number">16</span>), (<span class="number">5</span>, <span class="number">25</span>)])</span><br><span class="line">&#123;<span class="number">3</span>: <span class="number">9</span>, <span class="number">4</span>: <span class="number">16</span>, <span class="number">5</span>: <span class="number">25</span>&#125;</span><br></pre></td></tr></table></figure></p><p>可以用推导式语法创建：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;x: x*x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">6</span>)&#125;</span><br><span class="line">&#123;<span class="number">3</span>: <span class="number">9</span>, <span class="number">4</span>: <span class="number">16</span>, <span class="number">5</span>: <span class="number">25</span>&#125;</span><br></pre></td></tr></table></figure></p><p>方法：</p><ul><li>get(key,default_value)    return the value if key exist or default value</li><li>items()                   return key and value that is iterable</li><li>values()                  return iterable values</li><li>keys()                    …………….keys</li></ul><h3 id="局部状态"><a href="#局部状态" class="headerlink" title="局部状态"></a>局部状态</h3><p>对函数可变数据的构建用的是nonlocal —— 用于将数据置为非局部状态<br><strong>python限制一个名称对应的实例必须绑定在同一个局部帧中，所以不存在两个帧使用同一个名称的情况</strong><br>查找非局部帧的值不需要非局部语句，修改需要<br>python预处理名称来限制绑定的帧只能在局部帧，所以不允许提前引用变量  </p><h4 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h4><p>非局部赋值实际上是将balance与with_draw单独绑定，便于各个函数独立分治自己的局部状态<br>创建一个实例就对应创建一个绑定<br>所以引出了一个问题：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   <span class="keyword">def</span> <span class="title function_">make_withdraw</span>(<span class="params">balance</span>):</span><br><span class="line"><span class="number">2</span>    <span class="keyword">def</span> <span class="title function_">withdraw</span>(<span class="params">amount</span>):</span><br><span class="line"><span class="number">3</span>        <span class="keyword">nonlocal</span> balance</span><br><span class="line"><span class="number">4</span>        <span class="keyword">if</span> amount &gt; balance:</span><br><span class="line"><span class="number">5</span>            <span class="keyword">return</span> <span class="string">&#x27;Insufficient funds&#x27;</span></span><br><span class="line"><span class="number">6</span>        balance = balance - amount</span><br><span class="line"><span class="number">7</span>        <span class="keyword">return</span> balance</span><br><span class="line"><span class="number">8</span>    <span class="keyword">return</span> withdraw</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">10</span>wd = make_withdraw(<span class="number">12</span>)</span><br><span class="line"><span class="number">11</span>wd2 = wd</span><br><span class="line"><span class="number">12</span>wd2(<span class="number">1</span>)</span><br><span class="line"><span class="number">13</span>wd(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>两个相同的名称连接到一个绑定上，所处的帧并没有改变，<strong>只有函数调用才会创建新帧</strong></p><h4 id="实现列表和字典结构"><a href="#实现列表和字典结构" class="headerlink" title="实现列表和字典结构"></a>实现列表和字典结构</h4><p>使用非局部状态函数可以实现可变数据类型，因为非局部帧相当于提供使数据可变的能力，既不帧提供操作数据的方法<br>用函数来作为可变数据结构<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">mutable_link</span>():</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个可变链表的函数&quot;&quot;&quot;</span></span><br><span class="line">        contents = empty</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dispatch</span>(<span class="params">message, value=<span class="literal">None</span></span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> contents</span><br><span class="line">            <span class="keyword">if</span> message == <span class="string">&#x27;len&#x27;</span>:</span><br><span class="line">                <span class="keyword">return</span> len_link(contents)</span><br><span class="line">            <span class="keyword">elif</span> message == <span class="string">&#x27;getitem&#x27;</span>:</span><br><span class="line">                <span class="keyword">return</span> getitem_link(contents, value)</span><br><span class="line">            <span class="keyword">elif</span> message == <span class="string">&#x27;push_first&#x27;</span>:</span><br><span class="line">                contents = link(value, contents)</span><br><span class="line">            <span class="keyword">elif</span> message == <span class="string">&#x27;pop_first&#x27;</span>:</span><br><span class="line">                f = first(contents)</span><br><span class="line">                contents = rest(contents)</span><br><span class="line">                <span class="keyword">return</span> f</span><br><span class="line">            <span class="keyword">elif</span> message == <span class="string">&#x27;str&#x27;</span>:</span><br><span class="line">                <span class="keyword">return</span> join_link(contents, <span class="string">&quot;, &quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> dispatch</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">to_mutable_link</span>(<span class="params">source</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个与原列表相同内容的函数列表&quot;&quot;&quot;</span></span><br><span class="line">        s = mutable_link()</span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> <span class="built_in">reversed</span>(source):</span><br><span class="line">            s(<span class="string">&#x27;push_first&#x27;</span>, element)</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><br>可以用两种方法来实现对数据类型的变异操作：</p><ul><li>将方法放到一个局部帧中</li><li>将方法放到不同局部帧中，提供一个消息接口来调用(消息传递)</li></ul><p>消息传递的实现可以模拟一个send_message的过程</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">dictionary</span>():</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个字典的函数实现&quot;&quot;&quot;</span></span><br><span class="line">        records = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">getitem</span>(<span class="params">key</span>):</span><br><span class="line">            matches = [r <span class="keyword">for</span> r <span class="keyword">in</span> records <span class="keyword">if</span> r[<span class="number">0</span>] == key]</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(matches) == <span class="number">1</span>:</span><br><span class="line">                key, value = matches[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">return</span> value</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">setitem</span>(<span class="params">key, value</span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> records</span><br><span class="line">            non_matches = [r <span class="keyword">for</span> r <span class="keyword">in</span> records <span class="keyword">if</span> r[<span class="number">0</span>] != key]</span><br><span class="line">            records = non_matches + [[key, value]]</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dispatch</span>(<span class="params">message, key=<span class="literal">None</span>, value=<span class="literal">None</span></span>):</span><br><span class="line">            <span class="keyword">if</span> message == <span class="string">&#x27;getitem&#x27;</span>:</span><br><span class="line">                <span class="keyword">return</span> getitem(key)</span><br><span class="line">            <span class="keyword">elif</span> message == <span class="string">&#x27;setitem&#x27;</span>:</span><br><span class="line">                setitem(key, value)</span><br><span class="line">        <span class="keyword">return</span> dispatch</span><br></pre></td></tr></table></figure><h3 id="声明式编程实践：约束传递-Propagating-Constraints"><a href="#声明式编程实践：约束传递-Propagating-Constraints" class="headerlink" title="声明式编程实践：约束传递(Propagating Constraints)"></a>声明式编程实践：约束传递(Propagating Constraints)</h3><p>声明式编程的特性是声明要解决的问题的结构<br>编程中的表达式的计算都是单项的，要实现多项的解方程求值要用变量关系的约束器(Constrainter)和提醒其他约束器的连接器(connector)构成网络<br>使用消息传递以及对消息进行响应的数据结构(类)来构建网络</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>connector [<span class="string">&#x27;set_val&#x27;</span>](source, value)  <span class="string">&quot;&quot;&quot;表示 source 在请求连接器将当前值设为 value&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>connector [<span class="string">&#x27;has_val&#x27;</span>]()  <span class="string">&quot;&quot;&quot;返回连接器是否已经具有值&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>connector [<span class="string">&#x27;val&#x27;</span>]  <span class="string">&quot;&quot;&quot;是连接器的当前值&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>connector [<span class="string">&#x27;forget&#x27;</span>](source)  <span class="string">&quot;&quot;&quot;告诉连接器 source 请求遗忘它的值&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>connector [<span class="string">&#x27;connect&#x27;</span>](source)  <span class="string">&quot;&quot;&quot;告诉连接器参与新的约束，即 source&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>constraint[<span class="string">&#x27;new_val&#x27;</span>]()  <span class="string">&quot;&quot;&quot;表示与约束相连的某个连接器具有新的值。&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>constraint[<span class="string">&#x27;forget&#x27;</span>]()  <span class="string">&quot;&quot;&quot;表示与约束相连的某个连接器遗忘了值。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">converter</span>(<span class="params">c, f</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;用约束条件连接 c 到 f ，将摄氏度转换为华氏度.&quot;&quot;&quot;</span></span><br><span class="line">        u, v, w, x, y = [connector() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]</span><br><span class="line">        multiplier(c, w, u)</span><br><span class="line">        multiplier(v, x, u)</span><br><span class="line">        adder(v, y, f)</span><br><span class="line">        constant(w, <span class="number">9</span>)</span><br><span class="line">        constant(x, <span class="number">5</span>)</span><br><span class="line">        constant(y, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> operator <span class="keyword">import</span> add, sub</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">adder</span>(<span class="params">a, b, c</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;约束a+b=c&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> make_ternary_constraint(a, b, c, add, sub, sub)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">constant</span>(<span class="params">connector, value</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;常量赋值.&quot;&quot;&quot;</span></span><br><span class="line">        constraint = &#123;&#125;</span><br><span class="line">        connector[<span class="string">&#x27;set_val&#x27;</span>](constraint, value)</span><br><span class="line">        <span class="keyword">return</span> constraint</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">make_ternary_constraint</span>(<span class="params">a, b, c, ab, ca, cb</span>):       <span class="comment">#不存储连接的连接器，只提供功能</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;约束ab(a,b)=c，ca(c,a)=b，cb(c,b)=a。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">new_value</span>():</span><br><span class="line">            av, bv, cv = [connector[<span class="string">&#x27;has_val&#x27;</span>]() <span class="keyword">for</span> connector <span class="keyword">in</span> (a, b, c)]</span><br><span class="line">            <span class="keyword">if</span> av <span class="keyword">and</span> bv:</span><br><span class="line">                c[<span class="string">&#x27;set_val&#x27;</span>](constraint, ab(a[<span class="string">&#x27;val&#x27;</span>], b[<span class="string">&#x27;val&#x27;</span>]))</span><br><span class="line">            <span class="keyword">elif</span> av <span class="keyword">and</span> cv:</span><br><span class="line">                b[<span class="string">&#x27;set_val&#x27;</span>](constraint, ca(c[<span class="string">&#x27;val&#x27;</span>], a[<span class="string">&#x27;val&#x27;</span>]))</span><br><span class="line">            <span class="keyword">elif</span> bv <span class="keyword">and</span> cv:</span><br><span class="line">                a[<span class="string">&#x27;set_val&#x27;</span>](constraint, cb(c[<span class="string">&#x27;val&#x27;</span>], b[<span class="string">&#x27;val&#x27;</span>]))</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forget_value</span>():</span><br><span class="line">            <span class="keyword">for</span> connector <span class="keyword">in</span> (a, b, c):</span><br><span class="line">                connector[<span class="string">&#x27;forget&#x27;</span>](constraint)</span><br><span class="line">        constraint = &#123;<span class="string">&#x27;new_val&#x27;</span>: new_value, <span class="string">&#x27;forget&#x27;</span>: forget_value&#125;</span><br><span class="line">        <span class="keyword">for</span> connector <span class="keyword">in</span> (a, b, c):</span><br><span class="line">            connector[<span class="string">&#x27;connect&#x27;</span>](constraint)</span><br><span class="line">        <span class="keyword">return</span> constraint</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">connector</span>(<span class="params">name=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;限制条件之间的连接器.&quot;&quot;&quot;</span></span><br><span class="line">        informant = <span class="literal">None</span></span><br><span class="line">        constraints = []</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">set_value</span>(<span class="params">source, value</span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> informant</span><br><span class="line">            val = connector[<span class="string">&#x27;val&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> val <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                informant, connector[<span class="string">&#x27;val&#x27;</span>] = source, value</span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="built_in">print</span>(name, <span class="string">&#x27;=&#x27;</span>, value)</span><br><span class="line">                inform_all_except(source, <span class="string">&#x27;new_val&#x27;</span>, constraints)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> val != value:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;Contradiction detected:&#x27;</span>, val, <span class="string">&#x27;vs&#x27;</span>, value)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forget_value</span>(<span class="params">source</span>):</span><br><span class="line">            <span class="keyword">nonlocal</span> informant</span><br><span class="line">            <span class="keyword">if</span> informant == source:</span><br><span class="line">                informant, connector[<span class="string">&#x27;val&#x27;</span>] = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="built_in">print</span>(name, <span class="string">&#x27;is forgotten&#x27;</span>)</span><br><span class="line">                inform_all_except(source, <span class="string">&#x27;forget&#x27;</span>, constraints)</span><br><span class="line">        connector = &#123;<span class="string">&#x27;val&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">                     <span class="string">&#x27;set_val&#x27;</span>: set_value,</span><br><span class="line">                     <span class="string">&#x27;forget&#x27;</span>: forget_value,</span><br><span class="line">                     <span class="string">&#x27;has_val&#x27;</span>: <span class="keyword">lambda</span>: connector[<span class="string">&#x27;val&#x27;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">                     <span class="string">&#x27;connect&#x27;</span>: <span class="keyword">lambda</span> source: constraints.append(source)&#125;</span><br><span class="line">        <span class="keyword">return</span> connector</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">inform_all_except</span>(<span class="params">source, message, constraints</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;告知信息除了source外的所有约束条件，。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> constraints:</span><br><span class="line">            <span class="keyword">if</span> c != source:</span><br><span class="line">                c[message]()</span><br></pre></td></tr></table></figure><h2 id="OOP"><a href="#OOP" class="headerlink" title="OOP"></a>OOP</h2><p>类的构造器：<code>__init__</code><br><code>self</code>绑定对象，调用时实例会绑定到self中<br>赋值将对象绑定到新名称不会创建新对象:<br><code>c = a</code><br>作为类的属性，方法只是一个函数，但作为实例的属性，它是一个绑定方法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(Account.deposit)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;Function&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(spock_account.deposit)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;method&#x27;</span>&gt;</span><br></pre></td></tr></table></figure></p><p>我们可以通过两种方式调用 deposit ：作为函数和作为绑定方法。在前一种情况下，我们必须显式地为 self 参数提供一个参数。在后一种情况下， self 参数会自动绑定。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>Account.deposit(spock_account, <span class="number">1001</span>)<span class="comment"># 函数 deposit 接受两个参数</span></span><br><span class="line"><span class="number">1011</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>spock_account.deposit(<span class="number">1000</span>) <span class="comment"># 方法 deposit 接受一个参数</span></span><br><span class="line"><span class="number">2011</span></span><br></pre></td></tr></table></figure></p><p>如果属性名称以下划线开头，则只能在类中本地访问，不能被用户访问</p><p><strong>类属性</strong>是所有实例共享的静态变量</p><p>对象的点表达式，如果是实例，存在则调用，不存在则创建，但是不会访问类属性，只有对类的调点表达式才会修改类属性<br>同名时，本地的实例属性是优先于类属性的</p><p>baseclass, superclass, parentclass<br>subclass, childclass</p><p>接受参数作为父类，重写属性或者方法，注意父类中对实例属性的调用最好都用self.attr，方便作继承</p><p>尽量不要使用Account.method，使用实例的方法更具普遍性</p><p><strong>重写</strong>：方法可以重写，注意最后好使用self.attribution来在类中定义属性或方法，因为在该类的子类中继承此方法，要使用的是子类的新值</p><p><strong>多继承</strong>：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">AsSeenOnTVAccount</span>(CheckingAccount, SavingsAccount):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, account_holder</span>):</span><br><span class="line">            self.holder = account_holder</span><br><span class="line">            self.balance = <span class="number">1</span>           <span class="comment"># 赠送的 1 $!</span></span><br></pre></td></tr></table></figure><br>对于多继承，调用不同类的同一个名称的方法时，检索顺序为从左到右，从子到基<br>查看解析对于类和属性的查询顺序：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[c.__name__ <span class="keyword">for</span> c <span class="keyword">in</span> AsSeenOnTVAccount.mro()]</span><br><span class="line">[<span class="string">&#x27;AsSeenOnTVAccount&#x27;</span>, <span class="string">&#x27;CheckingAccount&#x27;</span>, <span class="string">&#x27;SavingsAccount&#x27;</span>, <span class="string">&#x27;Account&#x27;</span>, <span class="string">&#x27;object&#x27;</span>]</span><br></pre></td></tr></table></figure></p><h3 id="使用函数来实现类"><a href="#使用函数来实现类" class="headerlink" title="使用函数来实现类"></a>使用函数来实现类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">make_instance</span>(<span class="params">cls</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;Return a new object instance, which is a dispatch dictionary.&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_value</span>(<span class="params">name</span>):</span><br><span class="line"><span class="keyword">if</span> name <span class="keyword">in</span> attributes:</span><br><span class="line"><span class="keyword">return</span> attributes[name]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">value = cls[<span class="string">&#x27;get&#x27;</span>](name)</span><br><span class="line"><span class="keyword">return</span> bind_method(value, instance)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_value</span>(<span class="params">name, value</span>):</span><br><span class="line">attributes[name] = value</span><br><span class="line">attributes = &#123;&#125;</span><br><span class="line">instance = &#123;<span class="string">&#x27;get&#x27;</span>: get_value, <span class="string">&#x27;set&#x27;</span>: set_value&#125;</span><br><span class="line"><span class="keyword">return</span> instance</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">bind_method</span>(<span class="params">value, instance</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;Return a bound method if is callable, or value otherwise.&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">callable</span>(value):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">method</span>(<span class="params">*args</span>):</span><br><span class="line"><span class="keyword">return</span> value(instance, *args)</span><br><span class="line"><span class="keyword">return</span> method</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><p>使用响应消息的调度字典来实现，如果获取到的name在attribute中找不到，则进入bind_method来寻找以及绑定类和方法，找到了是函数的话，返回使用alue的函数，并且将instance作为self</p><h3 id="专用方法"><a href="#专用方法" class="headerlink" title="专用方法"></a>专用方法</h3><ul><li><code>__repr__</code>在使用交互环境时自动调用  </li><li><code>__str__</code>在打印时自动调用  </li><li><code>__bool__</code>构造，可以判断逻辑时自动调用  </li><li><code>__len__</code>长度，使用len()时调用  </li><li><code>__getitem__</code>，获取元素时调用  </li><li><code>__call__</code>可以使类可以像高阶函数一样被传播调用  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Adder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">self.n = n</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, k</span>):</span><br><span class="line"><span class="keyword">return</span> self.n + k</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add_three_obj = Adder(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add_three_obj(<span class="number">4</span>)</span><br><span class="line"><span class="number">7</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="多重表示"><a href="#多重表示" class="headerlink" title="多重表示"></a>多重表示</h3><p>接口是一组共享的属性名称，以及对它们的行为的规范<br>从最高层抽象向下实现：数值类型——拥有的操作——具体属性定义<br>Complex实现了接口，保证了具体方法的属性的一致性，达成了命名规范<br>@property修饰符可以将函数变为无参数，直接当属性值使用<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Number</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line"><span class="keyword">return</span> self.add(other)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self, other</span>):</span><br><span class="line"><span class="keyword">return</span> self.mul(other)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Complex</span>(<span class="title class_ inherited__">Number</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, other</span>):</span><br><span class="line"><span class="keyword">return</span> ComplexRI(self.real + other.real, self.imag + other.imag)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mul</span>(<span class="params">self, other</span>):</span><br><span class="line">magnitude = self.magnitude * other.magnitude</span><br><span class="line"><span class="keyword">return</span> ComplexMA(magnitude, self.angle + other.angle)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> math <span class="keyword">import</span> atan2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">ComplexRI</span>(<span class="title class_ inherited__">Complex</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, real, imag</span>):</span><br><span class="line">self.real = real</span><br><span class="line">self.imag = imag</span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">magnitude</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> (self.real ** <span class="number">2</span> + self.imag ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">angle</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> atan2(self.imag, self.real)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="string">&#x27;ComplexRI(&#123;0:g&#125;, &#123;1:g&#125;)&#x27;</span>.<span class="built_in">format</span>(self.real, self.imag)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> math <span class="keyword">import</span> sin, cos, pi</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">ComplexMA</span>(<span class="title class_ inherited__">Complex</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, magnitude, angle</span>):</span><br><span class="line">self.magnitude = magnitude</span><br><span class="line">self.angle = angle</span><br><span class="line"><span class="meta">        @property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">real</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> self.magnitude * cos(self.angle)</span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imag</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> self.magnitude * sin(self.angle)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="string">&#x27;ComplexMA(&#123;0:g&#125;, &#123;1:g&#125; * pi)&#x27;</span>.<span class="built_in">format</span>(self.magnitude, self.angle/pi)</span><br></pre></td></tr></table></figure></p><p>优势：<br>类似complex的接口类是可以开发添加的</p><h3 id="泛型表示"><a href="#泛型表示" class="headerlink" title="泛型表示"></a>泛型表示</h3><h4 id="类型派发"><a href="#类型派发" class="headerlink" title="类型派发"></a>类型派发</h4><p>依据受到的参数的类型(类的种类)来派发响应，保证数据跨类型的时候仍然可以被正确地对待</p><p>内置的函数 isinstance 接受一个对象或一个类。如果对象的类是所给的类或者继承自所给的类，它会返回一个真值<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = ComplexRI(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>(c, ComplexRI)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>(c, Complex)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>(c, ComplexMA)</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure></p><p>基于不同类型的接口类一个type_tag，来在进行操作的时候检查type_tag来进行类型派发<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Number</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line"><span class="keyword">if</span> self.type_tag == other.type_tag:</span><br><span class="line"><span class="keyword">return</span> self.add(other)</span><br><span class="line"><span class="keyword">elif</span> (self.type_tag, other.type_tag) <span class="keyword">in</span> self.adders:</span><br><span class="line"><span class="keyword">return</span> self.cross_apply(other, self.adders)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self, other</span>):</span><br><span class="line"><span class="keyword">if</span> self.type_tag == other.type_tag:</span><br><span class="line"><span class="keyword">return</span> self.mul(other)</span><br><span class="line"><span class="keyword">elif</span> (self.type_tag, other.type_tag) <span class="keyword">in</span> self.multipliers:</span><br><span class="line"><span class="keyword">return</span> self.cross_apply(other, self.multipliers)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_apply</span>(<span class="params">self, other, cross_fns</span>):</span><br><span class="line">cross_fn = cross_fns[(self.type_tag, other.type_tag)]</span><br><span class="line"><span class="keyword">return</span> cross_fn(self, other)</span><br><span class="line">adders = &#123;(<span class="string">&quot;com&quot;</span>, <span class="string">&quot;rat&quot;</span>): add_complex_and_rational,</span><br><span class="line">(<span class="string">&quot;rat&quot;</span>, <span class="string">&quot;com&quot;</span>): add_rational_and_complex&#125;</span><br><span class="line">multipliers = &#123;(<span class="string">&quot;com&quot;</span>, <span class="string">&quot;rat&quot;</span>): mul_complex_and_rational,</span><br><span class="line">(<span class="string">&quot;rat&quot;</span>, <span class="string">&quot;com&quot;</span>): mul_rational_and_complex&#125;</span><br></pre></td></tr></table></figure></p><h3 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h3><p>将一种数据类型和另一种数据类型在符合某种条件时可以实施强制类型转换来简化操作<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Number</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__add__</span>(<span class="params">self, other</span>):</span><br><span class="line">x, y = self.coerce(other)</span><br><span class="line"><span class="keyword">return</span> x.add(y)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__mul__</span>(<span class="params">self, other</span>):</span><br><span class="line">x, y = self.coerce(other)</span><br><span class="line"><span class="keyword">return</span> x.mul(y)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">coerce</span>(<span class="params">self, other</span>):</span><br><span class="line"><span class="keyword">if</span> self.type_tag == other.type_tag:</span><br><span class="line"><span class="keyword">return</span> self, other</span><br><span class="line"><span class="keyword">elif</span> (self.type_tag, other.type_tag) <span class="keyword">in</span> self.coercions:</span><br><span class="line"><span class="keyword">return</span> (self.coerce_to(other.type_tag), other)</span><br><span class="line"><span class="keyword">elif</span> (other.type_tag, self.type_tag) <span class="keyword">in</span> self.coercions:</span><br><span class="line"><span class="keyword">return</span> (self, other.coerce_to(self.type_tag))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">coerce_to</span>(<span class="params">self, other_tag</span>):</span><br><span class="line">coercion_fn = self.coercions[(self.type_tag, other_tag)]</span><br><span class="line"><span class="keyword">return</span> coercion_fn(self)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">rational_to_complex</span>(<span class="params">r</span>):</span><br><span class="line">    <span class="keyword">return</span> ComplexRI(r.numer/r.denom, <span class="number">0</span>)</span><br><span class="line">coercions = &#123;(<span class="string">&#x27;rat&#x27;</span>, <span class="string">&#x27;com&#x27;</span>): rational_to_complex&#125;</span><br><span class="line">        </span><br></pre></td></tr></table></figure></p><h2 id="效率"><a href="#效率" class="headerlink" title="效率"></a>效率</h2><p>可以用记忆化来提前存储值：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">memo</span>(<span class="params">f</span>):</span><br><span class="line">        cache = &#123;&#125;</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">memorized</span>(<span class="params">n</span>):</span><br><span class="line">            <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> cache:</span><br><span class="line">                cache[n] = f(n)</span><br><span class="line">            <span class="keyword">return</span> cache[n]</span><br><span class="line">        <span class="keyword">return</span> memorized</span><br></pre></td></tr></table></figure></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">count</span>(<span class="params">f</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">counted</span>(<span class="params">n</span>):</span><br><span class="line">            counted.call_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> f(n)</span><br><span class="line">        counted.call_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> counted</span><br></pre></td></tr></table></figure><p>这种方式可以构造类似于类的属性值，作为外部变量来使用<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib = count(fib)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib(<span class="number">19</span>)</span><br><span class="line"><span class="number">4181</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib.call_count</span><br><span class="line"><span class="number">10946</span></span><br></pre></td></tr></table></figure></p><p>阶乘的时间复杂度 $\Theta(\log_{2}{n})$ 的优化  </p><script type="math/tex; mode=display">b^{n}=\begin{cases}(b^{\frac{n}{2}})^{2}\qquad\qquad 如果n是偶数\\ b\cdot{b^{n-1}}\qquad\quad 如果n是奇数\end{cases}</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">square</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">fast_exp</span>(<span class="params">b, n</span>):</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> square(fast_exp(b, n // <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> b * fast_exp(b, n - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fast_exp(<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line"><span class="number">1267650600228229401496703205376</span></span><br></pre></td></tr></table></figure><p>python中的<code>if in</code>是有n的时间复杂度的</p><p>feibonaqi数列最接近黄金比例的n-2次方除以根号5</p><h2 id="递归对象"><a href="#递归对象" class="headerlink" title="递归对象"></a>递归对象</h2><p>可以使用类方法的递归来构建诸如<strong>链表、树、集合</strong>的数据结构，以及<strong>add、repr</strong>等方法，和很多操作，其本质和递归函数是一样的，可以用<code>Link.__add__ = extend</code>来直接进行专用方法赋值，具体方法如下：</p><p>链表<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Link</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;一个链表&quot;&quot;&quot;</span></span><br><span class="line">        empty = ()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, first, rest=(<span class="params"></span>)</span>):</span><br><span class="line">            <span class="keyword">assert</span> rest == Link.empty <span class="keyword">or</span> <span class="built_in">isinstance</span>(rest, Link)</span><br><span class="line">            self.first = first</span><br><span class="line">            self.rest = rest</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> self.first</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> self.rest[i-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">len</span>(self.rest)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(s)</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s[<span class="number">1</span>]</span><br><span class="line"><span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">link_expression</span>(<span class="params">s</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个可以计算得到 s 的字符串表达式。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> s.rest <span class="keyword">is</span> Link.empty:</span><br><span class="line">            rest = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rest = <span class="string">&#x27;, &#x27;</span> + link_expression(s.rest)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Link(&#123;0&#125;&#123;1&#125;)&#x27;</span>.<span class="built_in">format</span>(s.first, rest)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>link_expression(s)</span><br><span class="line"><span class="string">&#x27;Link(3, Link(4, Link(5)))&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Link.__repr__ = link_expression</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s_first = Link(s, Link(<span class="number">6</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s_first</span><br><span class="line">Link(Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>))), Link(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(s_first)</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(s_first[<span class="number">0</span>])</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s_first[<span class="number">0</span>][<span class="number">2</span>]</span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">extend_link</span>(<span class="params">s, t</span>):</span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">is</span> Link.empty:</span><br><span class="line">            <span class="keyword">return</span> t</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> Link(s.first, extend_link(s.rest, t))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>extend_link(s, s)</span><br><span class="line">Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>, Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>))))))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Link.__add__ = extend_link</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s + s</span><br><span class="line">Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>, Link(<span class="number">3</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>))))))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">map_link</span>(<span class="params">f, s</span>):</span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">is</span> Link.empty:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> Link(f(s.first), map_link(f, s.rest))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>map_link(square, s)</span><br><span class="line">Link(<span class="number">9</span>, Link(<span class="number">16</span>, Link(<span class="number">25</span>)))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">filter_link</span>(<span class="params">f, s</span>):</span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">is</span> Link.empty:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            filtered = filter_link(f, s.rest)</span><br><span class="line">            <span class="keyword">if</span> f(s.first):</span><br><span class="line">                <span class="keyword">return</span> Link(s.first, filtered)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> filtered</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>odd = <span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>map_link(square, filter_link(odd, s))</span><br><span class="line">Link(<span class="number">9</span>, Link(<span class="number">25</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[square(x) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] <span class="keyword">if</span> odd(x)]</span><br><span class="line">[<span class="number">9</span>, <span class="number">25</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">join_link</span>(<span class="params">s, separator</span>):</span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">is</span> Link.empty:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> s.rest <span class="keyword">is</span> Link.empty:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">str</span>(s.first)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">str</span>(s.first) + separator + join_link(s.rest, separator)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>join_link(s, <span class="string">&quot;, &quot;</span>)</span><br><span class="line"><span class="string">&#x27;3, 4, 5&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">partitions</span>(<span class="params">n, m</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a linked list of partitions of n using parts of up to m.</span></span><br><span class="line"><span class="string">        Each partition is represented as a linked list.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Link(Link.empty) <span class="comment"># A list containing the empty partition</span></span><br><span class="line">        <span class="keyword">elif</span> n &lt; <span class="number">0</span> <span class="keyword">or</span> m == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Link.empty</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            using_m = partitions(n-m, m)</span><br><span class="line">            with_m = map_link(<span class="keyword">lambda</span> s: Link(m, s), using_m)</span><br><span class="line">            without_m = partitions(n, m-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> with_m + without_m</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">print_partitions</span>(<span class="params">n, m</span>):</span><br><span class="line">        lists = partitions(n, m)</span><br><span class="line">        strings = map_link(<span class="keyword">lambda</span> s: join_link(s, <span class="string">&quot; + &quot;</span>), lists)</span><br><span class="line">        <span class="built_in">print</span>(join_link(strings, <span class="string">&quot;\n&quot;</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print_partitions(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line"><span class="number">4</span> + <span class="number">2</span></span><br><span class="line"><span class="number">4</span> + <span class="number">1</span> + <span class="number">1</span></span><br><span class="line"><span class="number">3</span> + <span class="number">3</span></span><br><span class="line"><span class="number">3</span> + <span class="number">2</span> + <span class="number">1</span></span><br><span class="line"><span class="number">3</span> + <span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span></span><br><span class="line"><span class="number">2</span> + <span class="number">2</span> + <span class="number">2</span></span><br><span class="line"><span class="number">2</span> + <span class="number">2</span> + <span class="number">1</span> + <span class="number">1</span></span><br><span class="line"><span class="number">2</span> + <span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span></span><br><span class="line"><span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span> + <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>树：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Tree</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, label, branches=(<span class="params"></span>)</span>):</span><br><span class="line">            self.label = label</span><br><span class="line">            <span class="keyword">for</span> branch <span class="keyword">in</span> branches:</span><br><span class="line">                <span class="keyword">assert</span> <span class="built_in">isinstance</span>(branch, Tree)</span><br><span class="line">            self.branches = branches</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">if</span> self.branches:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;Tree(&#123;0&#125;, &#123;1&#125;)&#x27;</span>.<span class="built_in">format</span>(self.label, <span class="built_in">repr</span>(self.branches))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;Tree(&#123;0&#125;)&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">repr</span>(self.label))</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">is_leaf</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">not</span> self.branches</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">fib_tree</span>(<span class="params">n</span>):</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> Tree(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> n == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> Tree(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left = fib_tree(n-<span class="number">2</span>)</span><br><span class="line">            right = fib_tree(n-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> Tree(left.label + right.label, (left, right))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib_tree(<span class="number">5</span>)</span><br><span class="line">Tree(<span class="number">3</span>, (Tree(<span class="number">1</span>, (Tree(<span class="number">0</span>), Tree(<span class="number">1</span>))), Tree(<span class="number">2</span>, (Tree(<span class="number">1</span>), Tree(<span class="number">1</span>, (Tree(<span class="number">0</span>), Tree(<span class="number">1</span>)))))))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">sum_labels</span>(<span class="params">t</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;对树的 label 求和，可能得到 None。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> t.label + <span class="built_in">sum</span>([sum_labels(b) <span class="keyword">for</span> b <span class="keyword">in</span> t.branches])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sum_labels(fib_tree(<span class="number">5</span>))</span><br><span class="line"><span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fib_tree = memo(fib_tree)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>big_fib_tree = fib_tree(<span class="number">35</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>big_fib_tree.label</span><br><span class="line"><span class="number">5702887</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>big_fib_tree.branches[<span class="number">0</span>] <span class="keyword">is</span> big_fib_tree.branches[<span class="number">1</span>].branches[<span class="number">1</span>]</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sum_labels = memo(sum_labels)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sum_labels(big_fib_tree)</span><br><span class="line"><span class="number">142587180</span></span><br></pre></td></tr></table></figure></p><p>集合：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">3</span> <span class="keyword">in</span> s</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">len</span>(s)</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.union(&#123;<span class="number">1</span>, <span class="number">5</span>&#125;)</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.intersection(&#123;<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>&#125;)</span><br><span class="line">&#123;<span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">empty</span>(<span class="params">s</span>):</span><br><span class="line">        <span class="keyword">return</span> s <span class="keyword">is</span> Link.empty</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">set_contains</span>(<span class="params">s, v</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;当且仅当 set s 包含 v 时返回 True。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> empty(s):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">elif</span> s.first == v:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> set_contains(s.rest, v)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = Link(<span class="number">4</span>, Link(<span class="number">1</span>, Link(<span class="number">5</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>set_contains(s, <span class="number">2</span>)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>set_contains(s, <span class="number">5</span>)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">adjoin_set</span>(<span class="params">s, v</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个包含 s 的所有元素和元素 v 的所有元素的集合。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> set_contains(s, v):</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> Link(v, s)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = adjoin_set(s, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">Link(<span class="number">2</span>, Link(<span class="number">4</span>, Link(<span class="number">1</span>, Link(<span class="number">5</span>))))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">union_set</span>(<span class="params">set1, set2</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回一个集合，包含 set1 和 set2 中的所有元素。&quot;&quot;&quot;</span></span><br><span class="line">        set1_not_set2 = keep_if_link(set1, <span class="keyword">lambda</span> v: <span class="keyword">not</span> set_contains(set2, v))</span><br><span class="line">        <span class="keyword">return</span> extend_link(set1_not_set2, set2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>union_set(t, s)</span><br><span class="line">Link(<span class="number">2</span>, Link(<span class="number">4</span>, Link(<span class="number">1</span>, Link(<span class="number">5</span>))))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">set_contains</span>(<span class="params">s, v</span>):</span><br><span class="line">        <span class="keyword">if</span> empty(s) <span class="keyword">or</span> s.first &gt; v:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">elif</span> s.first == v:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> set_contains(s.rest, v)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>u = Link(<span class="number">1</span>, Link(<span class="number">4</span>, Link(<span class="number">5</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>set_contains(u, <span class="number">0</span>)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>set_contains(u, <span class="number">4</span>)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">intersect_set</span>(<span class="params">set1, set2</span>):</span><br><span class="line">        <span class="keyword">if</span> empty(set1) <span class="keyword">or</span> empty(set2):</span><br><span class="line">            <span class="keyword">return</span> Link.empty</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            e1, e2 = set1.first, set2.first</span><br><span class="line">            <span class="keyword">if</span> e1 == e2:</span><br><span class="line">                <span class="keyword">return</span> Link(e1, intersect_set(set1.rest, set2.rest))</span><br><span class="line">            <span class="keyword">elif</span> e1 &lt; e2:</span><br><span class="line">                <span class="keyword">return</span> intersect_set(set1.rest, set2)</span><br><span class="line">            <span class="keyword">elif</span> e2 &lt; e1:</span><br><span class="line">                <span class="keyword">return</span> intersect_set(set1, set2.rest)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>intersect_set(s, s.rest)</span><br><span class="line">Link(<span class="number">4</span>, Link(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">set_contains</span>(<span class="params">s, v</span>):</span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">elif</span> s.entry == v:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> s.entry &lt; v:</span><br><span class="line">            <span class="keyword">return</span> set_contains(s.right, v)</span><br><span class="line">        <span class="keyword">elif</span> s.entry &gt; v:</span><br><span class="line">            <span class="keyword">return</span> set_contains(s.left, v)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">adjoin_set</span>(<span class="params">s, v</span>):</span><br><span class="line">        <span class="keyword">if</span> s <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> Tree(v)</span><br><span class="line">        <span class="keyword">elif</span> s.entry == v:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        <span class="keyword">elif</span> s.entry &lt; v:</span><br><span class="line">            <span class="keyword">return</span> Tree(s.entry, s.left, adjoin_set(s.right, v))</span><br><span class="line">        <span class="keyword">elif</span> s.entry &gt; v:</span><br><span class="line">            <span class="keyword">return</span> Tree(s.entry, adjoin_set(s.left, v), s.right)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>adjoin_set(adjoin_set(adjoin_set(<span class="literal">None</span>, <span class="number">2</span>), <span class="number">3</span>), <span class="number">1</span>)</span><br><span class="line">Tree(<span class="number">2</span>, Tree(<span class="number">1</span>), Tree(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><p>为了不预先存储值，但是能动态生成所要的数据或者操作，使用惰态计算，给予时延<br>iterator使用next返回下一个值，当没有值时返回StopIteration<br>迭代器用iter构造可迭代对象为迭代器，iter本身也可以被使用iter方法，作用是绑定名称        </p><p>map 函数是惰性的：调用它时并不会执行计算，直到返回的迭代器被 next 调用<br>相反，会创建一个迭代器对象，如果使用 next 查询， 该迭代器对象可以返回结果。我们可以在下面的示例中观察到这一事实，其中对 print 的调用被 延迟，直到从 doubled 迭代器请求相应的元素为止。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">double_and_print</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;***&#x27;</span>, x, <span class="string">&#x27;=&gt;&#x27;</span>, <span class="number">2</span>*x, <span class="string">&#x27;***&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>*x</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="built_in">range</span>(<span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>doubled = <span class="built_in">map</span>(double_and_print, s)  <span class="comment"># double_and_print 未被调用</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">next</span>(doubled)                       <span class="comment"># double_and_print 调用一次</span></span><br><span class="line">*** <span class="number">3</span> =&gt; <span class="number">6</span> ***</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">next</span>(doubled)                       <span class="comment"># double_and_print 再次调用</span></span><br><span class="line">*** <span class="number">4</span> =&gt; <span class="number">8</span> ***</span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(doubled)                       <span class="comment"># double_and_print 再次调用兩次</span></span><br><span class="line">*** <span class="number">5</span> =&gt; <span class="number">10</span> ***                         <span class="comment"># list() 会把剩余的值都计算出来并生成一个列表</span></span><br><span class="line">*** <span class="number">6</span> =&gt; <span class="number">12</span> ***</span><br><span class="line">[<span class="number">10</span>, <span class="number">12</span>]</span><br></pre></td></tr></table></figure><br>filter 函数返回一个迭代器， zip 和 reversed 函数也返回迭代器。  </p><p>for循环是调用了可迭代对象本身的<strong>iter</strong>方法，返回next值给绑定值，然后执行\<suite>主体部分<br>构造for循环：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>items = counts.__iter__()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">             item = items.__next__()</span><br><span class="line">             <span class="built_in">print</span>(item)</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure></suite></p><h3 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h3><p>生成器是一种迭代器，为了控制迭代函数的进度，使用生成器，关键词为yield，当函数内有yield关键词时，默认返回一个生成器，每次next返回下一个yield对应的值</p><h3 id="可迭代接口"><a href="#可迭代接口" class="headerlink" title="可迭代接口"></a>可迭代接口</h3><p>可以使类的<strong>iter</strong>方法返回一个迭代器，但是类本身是不变的<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Letters</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, start=<span class="string">&#x27;a&#x27;</span>, end=<span class="string">&#x27;e&#x27;</span></span>):</span><br><span class="line">            self.start = start</span><br><span class="line">            self.end = end</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> LetterIter(self.start, self.end)</span><br></pre></td></tr></table></figure></p><p>自定义迭代器，next方法，使用next时调用：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">LetterIter</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;依照 ASCII 码值顺序迭代字符的迭代器。&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, start=<span class="string">&#x27;a&#x27;</span>, end=<span class="string">&#x27;e&#x27;</span></span>):</span><br><span class="line">            self.next_letter = start</span><br><span class="line">            self.end = end</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">if</span> self.next_letter == self.end:</span><br><span class="line">                <span class="keyword">raise</span> StopIteration</span><br><span class="line">            letter = self.next_letter</span><br><span class="line">            self.next_letter = <span class="built_in">chr</span>(<span class="built_in">ord</span>(letter)+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> letter</span><br></pre></td></tr></table></figure></p><h3 id="流：惰性递归"><a href="#流：惰性递归" class="headerlink" title="流：惰性递归"></a>流：惰性递归</h3><p>Stream是一种对递归使用惰性的方法，不会提前计算下一个递归结果是什么，除非手动调用它<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = Stream(<span class="number">1</span>, <span class="keyword">lambda</span>: Stream(<span class="number">2</span>+<span class="number">3</span>, <span class="keyword">lambda</span>: Stream.empty))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">class</span> <span class="title class_">Stream</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;A lazily computed recursive list.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, first, compute_rest, empty=<span class="literal">False</span></span>):</span><br><span class="line">            self.first = first</span><br><span class="line">            self._compute_rest = compute_rest</span><br><span class="line">            self.empty = empty</span><br><span class="line">            self._rest = <span class="literal">None</span></span><br><span class="line">            self._computed = <span class="literal">False</span></span><br><span class="line"><span class="meta">        @property</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">rest</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;Return the rest of the stream, computing it if necessary.&quot;&quot;&quot;</span></span><br><span class="line">            <span class="keyword">assert</span> <span class="keyword">not</span> self.empty, <span class="string">&#x27;Empty streams have no rest.&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self._computed:</span><br><span class="line">                self._rest = self._compute_rest()</span><br><span class="line">                self._computed = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">return</span> self._rest</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">if</span> self.empty:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;&lt;empty stream&gt;&#x27;</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;Stream(&#123;0&#125;, &lt;compute_rest&gt;)&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">repr</span>(self.first))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Stream.empty = Stream(<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p><p>可以修改定义map函数或者filter函数，使map功能可以给流的每个递归<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">def</span> <span class="title function_">map_stream</span>(<span class="params">fn, s</span>):</span><br><span class="line">        <span class="keyword">if</span> s.empty:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">compute_rest</span>():</span><br><span class="line">            <span class="keyword">return</span> map_stream(fn, s.rest)</span><br><span class="line">        <span class="keyword">return</span> Stream(fn(s.first), compute_rest)</span><br></pre></td></tr></table></figure></p><h2 id="Scheme"><a href="#Scheme" class="headerlink" title="Scheme"></a>Scheme</h2><p>compiler编译器：编译为底层语言供机器识别<br>interpreter解释器：将一种语言解释为另一种语言，python的解释器就是用c写的，因为解释器是按照每一行来读取输入的，所以解释器是行解释的  </p><p>对一个语言的解释：</p><ul><li>词法解析(lexical analyzer)：使用分词器将语法分为标记(token)，之后传给语法解析器</li><li>语法解析(syntactic analyzer)：对标记使用可递归的数据结构来构架语法树</li><li>计算(calculate)：对语法树进行计算，计算面向的是递归的数据结构</li></ul><h2 id="语法杂记"><a href="#语法杂记" class="headerlink" title="语法杂记"></a>语法杂记</h2><ul><li><p>定义函数参数给出后视作默认值，使用时可以不写</p></li><li><p>文档字符串，三个双引号，使用help(fun)查看</p></li><li><p>元组的加与乘返回结果是组合以后的元组，map函数可将函数应用于数据，reduce可以将函数作用于序列返回单值(归约)，tuple可以将数据类型变为元组</p></li><li><p>python对变量的定义是非严格的，所以想要在函数内使用父帧的变量要进行声明，否则默认为局部帧的新变量<br>声明方法有<code>nonlocal</code>(嵌套函数)和<code>global</code>(全局)</p></li></ul><p>is和is not用于判断两个对像是否相同，这个相同指的是相同的内存地址</p><p>强制类型转换，类型不加括号</p><p>print(‘a’),打印：a</p><p>and运算时，从左到右演算表达式的值。0, ‘’, [], {}, None在布尔表达式环境下为假，其他任何东西都为真，返回作后一个真值或假值<br>or返回最后一个真值或最后一个假值<br>not 10 返回False， not None 返回True   注意0是真值,但是not 0中0为假<br>1/0 or True 为Error，从左到右判断，遇到Error直接Error</p><p>元组元素不可修改</p><p>assert用法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> <span class="built_in">type</span>(sides) == <span class="built_in">int</span> <span class="keyword">and</span> sides &gt;= <span class="number">1</span>, <span class="string">&#x27;Illegal value for sides&#x27;</span> </span><br></pre></td></tr></table></figure></p><p>return A if condition else </p><p>.strip()方法消除字符串首尾空白字符</p><p>字典中的in是判断的key值</p><p>random.sample接受list和指定个数，返回列表   一定要注意函数的返回类型</p><p>列表和字符串[1:]没有的时候会返回空，不会报错</p><p>字典可以多对一</p><p>for 迭代变量会自动修正</p><p>del删除列表元素</p><p>生成迭代器：iter()， 下一个next()，对应类的<strong>iter</strong>和<strong>next</strong>方法，迭代器的一个好处是不会和列表一样占用内存空间</p><p>生成器：是一种返回一个值的迭代器，使用yield产生<br>有next方法，send方法<br>生成器函数的调用会返回一个生成器(就像是类和实例)，生成器也就是迭代器，好处是可以暂存挂起当前状态，来节省状态<br>生成器中的return被调用时会抛出报错，当函数中含有yield关键词的时候，这个函数就已经是一个用来返回生成器的道具了，不是一般的函数  </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="built_in">type</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])     <span class="comment">#推导式</span></span><br><span class="line">Out[<span class="number">1</span>]: <span class="built_in">list</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="built_in">type</span>((i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)))     <span class="comment">#生成器</span></span><br><span class="line">Out[<span class="number">2</span>]: generator</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>常见的使用生成器来实现的斐波那契数列<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/usr/bin/python</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-*- coding: UTF-8 -*-</span></span><br><span class="line"> </span><br><span class="line">def fab(max): </span><br><span class="line">    n, a, b = 0, 0, 1 </span><br><span class="line">    while n &lt; max: </span><br><span class="line">        yield b      # 使用 yield</span><br><span class="line">        # print b </span><br><span class="line">        a, b = b, a + b </span><br><span class="line">        n = n + 1</span><br><span class="line"> </span><br><span class="line">for n in fab(5): </span><br><span class="line">    print n</span><br></pre></td></tr></table></figure></p><p>可以用send方法来发送yield的赋值对象，直接赋值会是None，因为从yield处继续开始迭代，所以不会有值<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">def</span> <span class="title function_">fun</span>():</span><br><span class="line">   ...:     <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">   ...:         <span class="built_in">print</span>(<span class="string">&quot;Start...&quot;</span>)</span><br><span class="line">   ...:         receive =  <span class="keyword">yield</span> i</span><br><span class="line">   ...:         <span class="built_in">print</span>(receive)</span><br><span class="line">   ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: f = fun()</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">next</span>(f)</span><br><span class="line">Start...</span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="built_in">next</span>(f)</span><br><span class="line"><span class="literal">None</span></span><br><span class="line">Start...</span><br><span class="line">Out[<span class="number">4</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">next</span>(f)</span><br><span class="line"><span class="literal">None</span></span><br><span class="line">Start...</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: f.send(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">hello</span><br><span class="line">Start...</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>对iter调用iter，是同一个迭代器<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">t = <span class="built_in">iter</span>(s)</span><br><span class="line">q = <span class="built_in">iter</span>(t)</span><br></pre></td></tr></table></figure></p><p>close方法可以关闭生成器</p><p>无穷：value = float(‘inf’)</p><p>zip函数：返回可迭代对象元素组成的元组的列表<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls = <span class="built_in">zip</span>([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ls</span><br><span class="line">&lt;<span class="built_in">zip</span> <span class="built_in">object</span> at <span class="number">0x7f47f15ef040</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(ls)</span><br><span class="line">[(<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)]</span><br></pre></td></tr></table></figure></p><h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">path_yielder</span>(<span class="params">t, value</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Yields all possible paths from the root of t to a node with the label value</span></span><br><span class="line"><span class="string">    as a list.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; t1 = Tree(1, [Tree(2, [Tree(3), Tree(4, [Tree(6)]), Tree(5)]), Tree(5)])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(t1)</span></span><br><span class="line"><span class="string">    1</span></span><br><span class="line"><span class="string">      2</span></span><br><span class="line"><span class="string">        3</span></span><br><span class="line"><span class="string">        4</span></span><br><span class="line"><span class="string">          6</span></span><br><span class="line"><span class="string">        5</span></span><br><span class="line"><span class="string">      5</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; next(path_yielder(t1, 6))</span></span><br><span class="line"><span class="string">    [1, 2, 4, 6]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; path_to_5 = path_yielder(t1, 5)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; sorted(list(path_to_5))</span></span><br><span class="line"><span class="string">    [[1, 2, 5], [1, 5]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; t2 = Tree(0, [Tree(2, [t1])])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(t2)</span></span><br><span class="line"><span class="string">    0</span></span><br><span class="line"><span class="string">      2</span></span><br><span class="line"><span class="string">        1</span></span><br><span class="line"><span class="string">          2</span></span><br><span class="line"><span class="string">            3</span></span><br><span class="line"><span class="string">            4</span></span><br><span class="line"><span class="string">              6</span></span><br><span class="line"><span class="string">            5</span></span><br><span class="line"><span class="string">          5</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; path_to_2 = path_yielder(t2, 2)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; sorted(list(path_to_2))</span></span><br><span class="line"><span class="string">    [[0, 2], [0, 2, 1, 2]]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## 递归的信仰之越，无关返回类型，只关心现在的判断和下一个关系</span></span><br><span class="line">    <span class="comment">## 生成器遇到return，会停止这一项的yield，但是不会返回值，这题表现为停止一条路径，继续执行下面的yield</span></span><br><span class="line">    <span class="comment">## 因为到叶子节点的时候，path函数是函数，所以返回上一级</span></span><br><span class="line">    <span class="comment">## 抽象思维，先抽象再具象，如果以上来就具象的话会很难完成</span></span><br><span class="line">    <span class="keyword">if</span> t.label == value:</span><br><span class="line">        <span class="keyword">yield</span> [t.label]</span><br><span class="line">    <span class="keyword">elif</span> t.is_leaf():</span><br><span class="line">        <span class="comment">#use return instead of yield! because the road does not exit, so we shouldn&#x27;t yield it</span></span><br><span class="line">        <span class="keyword">return</span> []   </span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> t.branches:</span><br><span class="line">        <span class="keyword">for</span> road <span class="keyword">in</span> path_yielder(b, value):</span><br><span class="line">            <span class="string">&quot;*** YOUR CODE HERE ***&quot;</span></span><br><span class="line">            road.insert(<span class="number">0</span>, t.label)</span><br><span class="line">            <span class="keyword">yield</span> road</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">remove_all</span>(<span class="params">link , value</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Remove all the nodes containing value in link. Assume that the</span></span><br><span class="line"><span class="string">    first element is never removed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; l1 = Link(0, Link(2, Link(2, Link(3, Link(1, Link(2, Link(3)))))))</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(l1)</span></span><br><span class="line"><span class="string">    &lt;0 2 2 3 1 2 3&gt;</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; remove_all(l1, 2)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(l1)</span></span><br><span class="line"><span class="string">    &lt;0 3 1 3&gt;</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; remove_all(l1, 3)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(l1)</span></span><br><span class="line"><span class="string">    &lt;0 1&gt;</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; remove_all(l1, 3)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; print(l1)</span></span><br><span class="line"><span class="string">    &lt;0 1&gt;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;*** YOUR CODE HERE ***&quot;</span></span><br><span class="line">    <span class="comment">#?为什么做起来难度这么大，对于递归要清楚函数的功能，先将函数本身的功能定义好</span></span><br><span class="line">    <span class="keyword">if</span> link <span class="keyword">is</span> Link.empty:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> link.rest <span class="keyword">is</span> Link.empty <span class="keyword">and</span> link.rest.first == value:</span><br><span class="line">        link.rest = link.rest.rest</span><br><span class="line">    remove_all(link.rest, value)</span><br></pre></td></tr></table></figure><p>scheme尾递归：需要新定义函数<br>要存储功能的话就要将存储表作为参数传</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="built_in">define</span></span> (<span class="name">replicate</span> x n)</span><br><span class="line">  <span class="symbol">&#x27;YOUR-CODE-HERE</span></span><br><span class="line">  (<span class="name"><span class="built_in">define</span></span> (<span class="name">repl</span> lst x n)</span><br><span class="line">    (<span class="name"><span class="built_in">if</span></span> (<span class="name"><span class="built_in">=</span></span> n <span class="number">0</span>)</span><br><span class="line">      lst</span><br><span class="line">      (<span class="name">repl</span> (<span class="name"><span class="built_in">cons</span></span> x lst) x (<span class="name"><span class="built_in">-</span></span> n <span class="number">1</span>))))</span><br><span class="line">  (<span class="name">repl</span> nil x n)</span><br><span class="line">  )</span><br></pre></td></tr></table></figure><p>又比如：<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(<span class="name"><span class="built_in">define</span></span> (<span class="name">accumulate</span> combiner start n term)</span><br><span class="line">  <span class="symbol">&#x27;YOUR-CODE-HERE</span></span><br><span class="line">  (<span class="name"><span class="built_in">begin</span></span></span><br><span class="line">    (<span class="name"><span class="built_in">define</span></span> (<span class="name">acc</span> result combiner start n term)</span><br><span class="line">      (<span class="name"><span class="built_in">if</span></span> (<span class="name"><span class="built_in">=</span></span> n <span class="number">0</span>)</span><br><span class="line">        result</span><br><span class="line">        (<span class="name">acc</span> (<span class="name">combiner</span> (<span class="name">term</span> start) result) combiner (<span class="name"><span class="built_in">+</span></span> <span class="number">1</span> start) (<span class="name"><span class="built_in">-</span></span> n <span class="number">1</span>) term)))</span><br><span class="line">    (<span class="name">acc</span> start combiner <span class="number">1</span> n term)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> 知识 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
